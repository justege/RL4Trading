{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "import gym\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvStepReturn, VecEnvWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        #self.batch_norm = nn.BatchNorm1d(hidden_size) ## seems to improve the final performance a lot\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = torch.relu(self.fc1(state)) #self.batch_norm\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, hidden_size)\n",
    "        #self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size+action_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state)) # self.batch_norm\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, dueling=False, device=\"cuda:0\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.dueling = dueling\n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        x = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(x  ))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication –> reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        #action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        #x = torch.cat((x,action),dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n",
    "\n",
    "\n",
    "\n",
    "class DeepActor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepActor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_size = hidden_size+state_size\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        #self.batch_norm = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3)) \n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.fc5.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return torch.tanh(self.fc5(x))\n",
    "\n",
    "\n",
    "class DeepCritic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepCritic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_dim = hidden_size+action_size+state_size\n",
    "        self.fc1 = nn.Linear(state_size+action_size, hidden_size)\n",
    "        #.batch_norm = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(self.input_dim, hidden_size)\n",
    "        self.fc3 = nn.Linear(self.input_dim, hidden_size)\n",
    "        self.fc4 = nn.Linear(self.input_dim, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.fc5.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xu = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.fc1(xu))\n",
    "        x = torch.cat((x, xu), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.cat((x, xu), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.cat((x, xu), dim=1)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n",
    "\n",
    "class DeepIQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, dueling=False, device=\"cuda:0\"):\n",
    "        super(DeepIQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.input_dim = action_size+state_size+layer_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.dueling = dueling\n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.ff_1 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_2 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_3 = nn.Linear(self.input_dim, layer_size)\n",
    "        self.ff_4 = nn.Linear(self.layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])  \n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        xs = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(xs))\n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "        x = torch.cat((x, xs), dim=1)\n",
    "        x = torch.relu(self.ff_2(x))\n",
    "\n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication –> reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        state = input.repeat(num_tau,1).reshape(num_tau,batch_size*self.input_shape).transpose(0,1).reshape(batch_size*num_tau,self.input_shape)\n",
    "        \n",
    "        x = torch.cat((x,action,state),dim=1)\n",
    "        x = torch.relu(self.ff_3(x))\n",
    "\n",
    "        out = self.ff_4(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "TRAINING_DATA_FILE = \"dataprocessing/Yfinance_Data.csv\"\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "TRAINED_MODEL_DIR = f\"trained_models/{now}\"\n",
    "os.makedirs(TRAINED_MODEL_DIR)\n",
    "\n",
    "TESTING_DATA_FILE = \"test.csv\"\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data = data.sort_values(['datadate', 'tic'], ignore_index=True)\n",
    "\n",
    "\n",
    "    # data  = data[final_columns]\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_price(df):\n",
    "    \"\"\"\n",
    "    calcualte adjusted close price, open-high-low price and volume\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    data = data[['Date', 'tic', 'Close', 'Open', 'High', 'Low', 'Volume','datadate']]\n",
    "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_technical_indicator(df):\n",
    "    \"\"\"\n",
    "    calcualte technical indicators\n",
    "    use stockstats package to add technical inidactors\n",
    "    :param data: (df) pandas dataframe\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    stock = Sdf.retype(df.copy())\n",
    "\n",
    "    #print(stock)\n",
    "\n",
    "    unique_ticker = stock.tic.unique()\n",
    "\n",
    "    macd = pd.DataFrame()\n",
    "    rsi = pd.DataFrame()\n",
    "    cci = pd.DataFrame()\n",
    "    dx = pd.DataFrame()\n",
    "\n",
    "    # temp = stock[stock.tic == unique_ticker[0]]['macd']\n",
    "    for i in range(len(unique_ticker)):\n",
    "        ## macd\n",
    "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
    "        temp_macd = pd.DataFrame(temp_macd)\n",
    "        macd = macd.append(temp_macd, ignore_index=True)\n",
    "        ## rsi\n",
    "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
    "        temp_rsi = pd.DataFrame(temp_rsi)\n",
    "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
    "        ## cci\n",
    "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
    "        temp_cci = pd.DataFrame(temp_cci)\n",
    "        cci = cci.append(temp_cci, ignore_index=True)\n",
    "        ## adx\n",
    "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
    "        temp_dx = pd.DataFrame(temp_dx)\n",
    "        dx = dx.append(temp_dx, ignore_index=True)\n",
    "\n",
    "    df['macd'] = macd\n",
    "    df['rsi'] = rsi\n",
    "    df['cci'] = cci\n",
    "    df['adx'] = dx\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"data preprocessing pipeline\"\"\"\n",
    "    start = datetime.datetime(2010, 12, 1)\n",
    "    df = load_dataset(file_name=TRAINING_DATA_FILE)\n",
    "    # get data after 2010\n",
    "    # df = df[df.Date >= start]\n",
    "    # calcualte adjusted price\n",
    "    df_preprocess = calculate_price(df)\n",
    "    # add technical indicators using stockstats\n",
    "    df_final = add_technical_indicator(df_preprocess)\n",
    "    # fill the missing values at the beginning\n",
    "    df_final.fillna(method='bfill', inplace=True)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (1488965179.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [169]\u001b[0;36m\u001b[0m\n\u001b[0;31m    hidden_size,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size,\n",
    "                      action_size,\n",
    "                      n_step,\n",
    "                      per, \n",
    "                      munchausen,\n",
    "                      distributional,\n",
    "                      D2RL,\n",
    "                      random_seed=2,\n",
    "                      hidden_size,\n",
    "                      BUFFER_SIZE = int(1e6),  # replay buffer size\n",
    "                      BATCH_SIZE = 32,        # minibatch size\n",
    "                      GAMMA = 0.99,            # discount factor\n",
    "                      TAU = 1e-3,              # for soft update of target parameters\n",
    "                      LR_ACTOR = 1e-3,         # learning rate of the actor \n",
    "                      LR_CRITIC = 1e-3,        # learning rate of the critic\n",
    "                      WEIGHT_DECAY = 1e-2,        # L2 weight decay\n",
    "                      LEARN_EVERY = 1,\n",
    "                      LEARN_NUMBER = 1,\n",
    "                      EPSILON = 1.0,\n",
    "                      EPSILON_DECAY = 1,\n",
    "                      device = \"cuda\",\n",
    "                      frames = 100000,\n",
    "                      worker=1\n",
    "                      ):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.BUFFER_SIZE = BUFFER_SIZE\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.per = per\n",
    "        self.munchausen = munchausen\n",
    "        self.n_step = n_step\n",
    "        self.distributional = distributional\n",
    "        self.D2RL = D2RL\n",
    "        self.GAMMA = GAMMA\n",
    "        self.TAU = TAU\n",
    "        self.LEARN_EVERY = LEARN_EVERY\n",
    "        self.LEARN_NUMBER = LEARN_NUMBER\n",
    "        self.EPSILON_DECAY = EPSILON_DECAY\n",
    "        self.epsilon = 0.04\n",
    "        self.device = device\n",
    "        self.seed = random.seed(random_seed)\n",
    "        # distributional Values\n",
    "        self.N = 32\n",
    "        self.entropy_coeff = 0.001\n",
    "        # munchausen values\n",
    "        self.entropy_tau = 0.03\n",
    "        self.lo = -1\n",
    "        self.alpha = 0.9\n",
    "        self.last_action = []\n",
    "        self.action = []\n",
    "        self.eta = torch.FloatTensor([.1]).to(device)\n",
    "        \n",
    "        print(\"Using: \", device)\n",
    "        print(\"seed agent: \", self.seed)\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed, hidden_size=hidden_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed, hidden_size=hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        \n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        print(\"Actor: \\n\", self.actor_local)\n",
    "        print(\"\\nCritic: \\n\", self.critic_local)\n",
    "\n",
    "        #self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, n_step=n_step, parallel_env=worker, device=device, seed=random_seed, gamma=GAMMA)\n",
    "        #self.memory = PrioritizedReplay(device, BUFFER_SIZE, self.BATCH_SIZE, gamma=self.GAMMA, n_step=n_step, parallel_env=1)\n",
    "        self.memory = PrioritizedReplay(BUFFER_SIZE, BATCH_SIZE, device=device, seed=random_seed, gamma=GAMMA, n_step=n_step, parallel_env=worker, beta_frames=frames)\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        self.learn = self.learn_distribution\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done, timestamp, writer):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        \n",
    "        print('agent step reward:{}'.format(reward))\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.BATCH_SIZE and timestamp % self.LEARN_EVERY == 0:\n",
    "            for _ in range(self.LEARN_NUMBER):\n",
    "                \n",
    "                experiences = self.memory.sample()\n",
    "                #print(experiences)\n",
    "                losses = self.learn(experiences, self.GAMMA)\n",
    "            writer.add_scalar(\"Critic_loss\", losses[0], timestamp)\n",
    "            writer.add_scalar(\"Actor_loss\", losses[1], timestamp)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        print(state)\n",
    "        \n",
    "        #print('state.shape[0]:{}'.format(state.shape[0]))\n",
    "        #print('self.state_size:{}'.format(self.state_size))\n",
    "\n",
    "        assert state.shape == (state.shape[0],self.state_size), \"shape: {}\".format(state.shape)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "                action = self.actor_local(state).cpu().data.numpy().squeeze(0)\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            self.last_action = action\n",
    "            return action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action \"\"\"\n",
    "        action += self.noise.sample() * self.epsilon\n",
    "        \n",
    "        return action #np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        print('self.seed',self.seed)\n",
    "        self.noise.reset()\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    def learn_distribution(self, experiences, gamma):\n",
    "            \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "            Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "            where:\n",
    "                actor_target(state) -> action\n",
    "                critic_target(state, action) -> Q-value\n",
    "\n",
    "            Params\n",
    "            ======\n",
    "                experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "                gamma (float): discount factor\n",
    "            \"\"\"\n",
    "            states, actions, rewards, next_states, dones, idx, weights = experiences\n",
    "            \n",
    "            #print('states:{}'.format(states.shape))\n",
    "            #print('rewards:{}'.format(rewards))\n",
    "            # ---------------------------- update critic ---------------------------- #\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "\n",
    "            # Get max predicted Q values (for next states) from target model\n",
    "            with torch.no_grad():\n",
    "                next_actions = self.actor_local(next_states)\n",
    "                Q_targets_next, _ = self.critic_target(next_states, next_actions, self.N)\n",
    "                Q_targets_next = Q_targets_next.transpose(1,2)\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets = rewards.unsqueeze(-1) + (self.GAMMA**self.n_step * Q_targets_next.to(self.device) * (1. - dones.unsqueeze(-1)))\n",
    "                \n",
    "            # Get expected Q values from local model\n",
    "            Q_expected, taus = self.critic_local(states, actions, self.N)\n",
    "            assert Q_targets.shape == (self.BATCH_SIZE, 1, self.N)\n",
    "            assert Q_expected.shape == (self.BATCH_SIZE, self.N, 1)\n",
    "    \n",
    "            # Quantile Huber loss\n",
    "            td_error = Q_targets - Q_expected\n",
    "            assert td_error.shape == (self.BATCH_SIZE, self.N, self.N), \"wrong td error shape\"\n",
    "            huber_l = calculate_huber_loss(td_error, 1.0)\n",
    "            quantil_l = abs(taus -(td_error.detach() < 0).float()) * huber_l / 1.0\n",
    "\n",
    "            critic_loss = quantil_l.sum(dim=1).mean(dim=1).mean()\n",
    "            # Minimize the loss\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # ---------------------------- update actor ---------------------------- #\n",
    "            # Compute actor loss\n",
    "            actions_pred = self.actor_local(states)\n",
    "            actor_loss = -self.critic_local.get_qvalues(states, actions_pred).mean()\n",
    "            # Minimize the loss\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # ----------------------- update target networks ----------------------- #\n",
    "            self.soft_update(self.critic_local, self.critic_target)\n",
    "            self.soft_update(self.actor_local, self.actor_target)\n",
    "            \n",
    "            self.epsilon *= self.EPSILON_DECAY\n",
    "            self.noise.reset()\n",
    "\n",
    "            return critic_loss.detach().cpu().numpy(), actor_loss.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "def calculate_huber_loss(td_errors, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate huber loss element-wisely depending on kappa k.\n",
    "    \"\"\"\n",
    "    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n",
    "    assert loss.shape == (td_errors.shape[0], 32, 32), \"huber loss has wrong shape\"\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeepActor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(DeepActor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_size = hidden_size+state_size\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        #self.batch_norm = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3)) \n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.fc5.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.cat((x,state), dim=1)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return torch.tanh(self.fc5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "\n",
    "def weight_init_xavier(layers):\n",
    "    for layer in layers:\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.01)\n",
    "\n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size, seed, N, dueling=False, device=\"cuda:0\"):\n",
    "        super(IQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_shape = state_size\n",
    "        self.action_size = action_size\n",
    "        self.N = N  \n",
    "        self.n_cos = 64\n",
    "        self.layer_size = layer_size\n",
    "        self.pis = torch.FloatTensor([np.pi*i for i in range(1,self.n_cos+1)]).view(1,1,self.n_cos).to(device) # Starting from 0 as in the paper \n",
    "        self.dueling = dueling\n",
    "        self.device = device\n",
    "\n",
    "        # Network Architecture\n",
    "\n",
    "        self.head = nn.Linear(self.action_size+self.input_shape, layer_size) \n",
    "        self.cos_embedding = nn.Linear(self.n_cos, layer_size)\n",
    "        self.ff_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.ff_2 = nn.Linear(layer_size, 1)    \n",
    "        #weight_init([self.head_1, self.ff_1])\n",
    "\n",
    "    def calc_input_layer(self):\n",
    "        x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        x = self.head(x)\n",
    "        return x.flatten().shape[0]\n",
    "        \n",
    "    def calc_cos(self, batch_size, n_tau=32):\n",
    "        \"\"\"\n",
    "        Calculating the cosinus values depending on the number of tau samples\n",
    "        \"\"\"\n",
    "        taus = torch.rand(batch_size, n_tau).unsqueeze(-1).to(self.device) #(batch_size, n_tau, 1)  .to(self.device)\n",
    "        cos = torch.cos(taus*self.pis)\n",
    "\n",
    "        assert cos.shape == (batch_size,n_tau,self.n_cos), \"cos shape is incorrect\"\n",
    "        return cos, taus\n",
    "    \n",
    "    def forward(self, input, action, num_tau=32):\n",
    "        \"\"\"\n",
    "        Quantile Calculation depending on the number of tau\n",
    "        \n",
    "        Return:\n",
    "        quantiles [ shape of (batch_size, num_tau, action_size)]\n",
    "        taus [shape of ((batch_size, num_tau, 1))]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        x = torch.cat((input, action), dim=1)\n",
    "        x = torch.relu(self.head(x  ))\n",
    "        \n",
    "        cos, taus = self.calc_cos(batch_size, num_tau) # cos shape (batch, num_tau, layer_size)\n",
    "        cos = cos.view(batch_size*num_tau, self.n_cos)\n",
    "        cos_x = torch.relu(self.cos_embedding(cos)).view(batch_size, num_tau, self.layer_size) # (batch, n_tau, layer)\n",
    "        \n",
    "        # x has shape (batch, layer_size) for multiplication –> reshape to (batch, 1, layer)\n",
    "        x = (x.unsqueeze(1)*cos_x).view(batch_size*num_tau, self.layer_size)  #batch_size*num_tau, self.cos_layer_out\n",
    "        # Following reshape and transpose is done to bring the action in the same shape as batch*tau:\n",
    "        # first 32 entries are tau for each action -> thats why each action one needs to be repeated 32 times \n",
    "        # x = [[tau1   action = [[a1\n",
    "        #       tau1              a1   \n",
    "        #        ..               ..\n",
    "        #       tau2              a2\n",
    "        #       tau2              a2\n",
    "        #       ..]]              ..]]  \n",
    "        #action = action.repeat(num_tau,1).reshape(num_tau,batch_size*self.action_size).transpose(0,1).reshape(batch_size*num_tau,self.action_size)\n",
    "        #x = torch.cat((x,action),dim=1)\n",
    "        x = torch.relu(self.ff_1(x))\n",
    "\n",
    "        out = self.ff_2(x)\n",
    "        \n",
    "        return out.view(batch_size, num_tau, 1), taus\n",
    "    \n",
    "    def get_qvalues(self, inputs, action):\n",
    "        quantiles, _ = self.forward(inputs, action, self.N)\n",
    "        actions = quantiles.mean(dim=1)\n",
    "        return actions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PrioritizedReplay(object):\n",
    "    \"\"\"\n",
    "    Proportional Prioritization\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity, batch_size, device, seed, gamma=0.99, n_step=1, parallel_env=1, alpha=0.6, beta_start = 0.4, beta_frames=100000):\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.device = device\n",
    "        self.frame = 1 #for beta calculation\n",
    "        self.batch_size = batch_size\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.pos        = 0\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.seed = np.random.seed(seed)\n",
    "        self.parallel_env = parallel_env\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buffer = [deque(maxlen=self.n_step) for i in range(parallel_env)]\n",
    "        self.iter_ = 0\n",
    "        self.memory = deque(maxlen=capacity) \n",
    "        self.gamma = gamma\n",
    "\n",
    "    def calc_multistep_return(self,n_step_buffer):\n",
    "        Return = 0\n",
    "        for idx in range(self.n_step):\n",
    "            Return += self.gamma**idx * n_step_buffer[idx][2]\n",
    "        \n",
    "        return n_step_buffer[0][0], n_step_buffer[0][1], Return, n_step_buffer[-1][3], n_step_buffer[-1][4]\n",
    "\n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        \"\"\"\n",
    "        Linearly increases beta from beta_start to 1 over time from 1 to beta_frames.\n",
    "        \n",
    "        3.4 ANNEALING THE BIAS (Paper: PER)\n",
    "        We therefore exploit the flexibility of annealing the amount of importance-sampling\n",
    "        correction over time, by defining a schedule on the exponent \n",
    "        that reaches 1 only at the end of learning. In practice, we linearly anneal from its initial value 0 to 1\n",
    "        \"\"\"\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if self.iter_ == self.parallel_env:\n",
    "            self.iter_ = 0\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        action = torch.from_numpy(action).unsqueeze(0)\n",
    "\n",
    "        # n_step calc\n",
    "        self.n_step_buffer[self.iter_].append((state, action, reward, next_state, done))\n",
    "        if len(self.n_step_buffer[self.iter_]) == self.n_step:\n",
    "            state, action, reward, next_state, done = self.calc_multistep_return(self.n_step_buffer[self.iter_])\n",
    "            e = self.experience(state, action, reward, next_state, done)\n",
    "            self.memory.append(e)\n",
    "\n",
    "        max_prio = np.array(self.priorities, dtype=float).max() if self.buffer else 1.0 # gives max priority if buffer is not empty else 1\n",
    "        \n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max_prio)\n",
    "        self.iter_ += 1\n",
    "\n",
    "        \n",
    "    def sample(self):\n",
    "        N = len(self.buffer)\n",
    "        prios = np.array(self.priorities, dtype=float)\n",
    "        assert N == len(prios)\n",
    "            \n",
    "        # calc P = p^a/sum(p^a)\n",
    "        probs  = prios ** self.alpha\n",
    "        P = probs/probs.sum()\n",
    "        \n",
    "        #gets the indices depending on the probability p\n",
    "        indices = np.random.choice(N, self.batch_size, p=P) \n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        #print(beta)\n",
    "        self.frame+=1\n",
    "                \n",
    "        #Compute importance-sampling weight\n",
    "        weights  = (N * P[indices])**(-beta)\n",
    "        # normalize weights\n",
    "        weights /= weights.max() \n",
    "        weights  = np.array(weights, dtype=np.float32) \n",
    "        \n",
    "        \n",
    "        CurrentSequence = indices[0]\n",
    "        \n",
    "        if CurrentSequence < 8:\n",
    "            if len(self.memory) < 16:\n",
    "                indices = np.random.choice(N, 8, p=P)\n",
    "                #print('second indices:{}'.format(indices))\n",
    "                SequenceOfSampling=indices\n",
    "            else: \n",
    "                SequenceOfSampling = [CurrentSequence, CurrentSequence+1,CurrentSequence+2,CurrentSequence+3,CurrentSequence+4,CurrentSequence+5,CurrentSequence+6,CurrentSequence+7]\n",
    "        else:\n",
    "            SequenceOfSampling = [CurrentSequence-7,CurrentSequence-6,CurrentSequence-5,CurrentSequence-4,CurrentSequence-3,CurrentSequence-2,CurrentSequence-1,CurrentSequence]\n",
    "        \n",
    "        #print(SequenceOfSampling)\n",
    "        #print(len(self.memory))\n",
    "        experiences = [self.memory[SequenceOfSampling[0]],self.memory[SequenceOfSampling[1]],self.memory[SequenceOfSampling[2]],self.memory[SequenceOfSampling[3]],self.memory[SequenceOfSampling[4]],self.memory[SequenceOfSampling[5]],self.memory[SequenceOfSampling[6]],self.memory[SequenceOfSampling[7]]]\n",
    "        #print(experiences)\n",
    "        \n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*samples) \n",
    "\n",
    "        states      = torch.FloatTensor(np.float32(np.concatenate(states))).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.float32(np.concatenate(next_states))).to(self.device)\n",
    "        actions     = torch.cat(actions).to(self.device)\n",
    "        rewards     = torch.FloatTensor(rewards).to(self.device).unsqueeze(1) \n",
    "        dones       = torch.FloatTensor(dones).to(self.device).unsqueeze(1)\n",
    "        weights    = torch.FloatTensor(weights).unsqueeze(1)\n",
    "        #print(\"s\",states.shape)\n",
    "        #print(\"ns\", next_states.shape)\n",
    "        #print(\"a\", actions.shape)\n",
    "        \n",
    "        #print(\"r:{}\".format(rewards))\n",
    "        #print(\"d\", dones.shape)\n",
    "        #print(\"w\", weights.shape)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "import os\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 10\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE= 1000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.agent_stock_iteration_index = 0\n",
    "        self.penalty = 0\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (7,))\n",
    "        # load data from a pandas dataframe\n",
    "        #print('df: {}'.format(self.df))\n",
    "        #print('day: {}'.format(self.day))\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        \n",
    "        self.terminal = False\n",
    "\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM \n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        self.previous_trades = 0 \n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        action = action\n",
    "        #print('index:{}'.format(index))\n",
    "        #print('selling:{}'.format(action))\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        #print('index:{}'.format(index))\n",
    "        #print('buying:{}'.format(action))\n",
    "        action = action\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        \n",
    "        if available_amount>0:\n",
    "            self.trades+=1\n",
    "    \n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        #print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        #print(actions)\n",
    "        self.actions = actions\n",
    "        if self.terminal:\n",
    "            print(\"Finished\")\n",
    "            print(self.state)\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            #df_total_value.to_csv('results/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('results/account_rewards_train.csv')\n",
    "\n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "\n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "            #print(\"The actions is: {}\".format(self.actions))\n",
    "\n",
    "            #action = np.array([4,4,5])\n",
    "            #actions = np.array([4,0,0,0,0,0,0,0,4,0,4,0,-3,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "            #actions = self.actions * HMAX_NORMALIZE #WHY??\n",
    "            #print(\"actions-index------:{}\".format(actions))\n",
    "            #actions = (actions.astype(int))\n",
    "            print('here:',actions)\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            #print('all-actions:{}'.format(actions))\n",
    "\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions) #TODO: this may not be touched.\n",
    "            #print(\"The actions is: {}\".format(actions))\n",
    "            \n",
    "            \n",
    "\n",
    "            sell_index = argsort_actions[:np.where(actions < 0.5 )[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > -0.5 )[0].shape[0]]\n",
    "            \n",
    "            \n",
    "            for index in sell_index:\n",
    "                # print('take sell action'.format(actions[index]))\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index, actions[index])\n",
    "                \n",
    "            \n",
    "            #print(\"self.day:{}\".format(self.day))\n",
    "            #--print('trades:{}'.format(self.trades))\n",
    "            \n",
    "            \n",
    "            \n",
    "            if self.previous_trades == self.trades:\n",
    "                self.penalty = 10\n",
    "                #self.reward = -1\n",
    "            else: \n",
    "                self.penalty = 0\n",
    "                #self.reward = self.trades\n",
    "                \n",
    "            self.previous_trades = float(self.trades)\n",
    "                \n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                self.data.Close.values.tolist() + \\\n",
    "                list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) \n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            \n",
    "            \n",
    "\n",
    "            self.reward = end_total_asset - begin_total_asset - self.penalty\n",
    "            \n",
    "            print(\"trades:{}\".format(self.trades))\n",
    "            print('previous:{}'.format(self.previous_trades))\n",
    "            print(\"penalty:{}\".format(self.penalty))\n",
    "            print(\"step_reward:{}\".format(self.reward))\n",
    "        \n",
    "            self.rewards_memory.append(self.reward)\n",
    "            #self.reward = self.reward*REWARD_SCALING\n",
    "            print(\"step_reward:{}\".format(self.reward))\n",
    "            \n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.final_asset_value = 0\n",
    "        self.trades = 0\n",
    "        self.previous_trades = 0\n",
    "        self.penalty = 0 \n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        self.agent_stock_iteration_index = 0\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.Close.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM \n",
    "        # iteration += 1 \n",
    "        #print(\"[0]*STOCK_DIM:{}\".format([0]*STOCK_DIM))\n",
    "        #print(\"self.state:{}\".format(len(self.state)))\n",
    "        #print(np.array(self.state))\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def run(env_train,frames=100000, eps_fixed=False, eps_frames=1e6, min_eps=0.001):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per epaisode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "    if eps_fixed:\n",
    "        eps = 0\n",
    "    else:\n",
    "        eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env_train.reset()\n",
    "   # state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0 \n",
    "    \n",
    "    action_high = env_train.action_space.high[0]\n",
    "    action_low = env_train.action_space.low[0]\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    action_size = env_train.action_space.shape[0]\n",
    "    \n",
    "    for frame in range(1, frames+1):\n",
    "        \n",
    "        \n",
    "        \n",
    "        if frame  == 0:\n",
    "            # inital state\n",
    "\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "        #print('initial state:{}'.format(initial))\n",
    "        \n",
    "        if eps_fixed == False:\n",
    "            if frame < eps_frames:\n",
    "                eps = max(eps_start - (frame*(1/eps_frames)), min_eps)\n",
    "            else:\n",
    "                eps = max(min_eps - min_eps*((frame-eps_frames)/(frames-eps_frames)), 0.001)\n",
    "        \n",
    "\n",
    "        action = agent.act(state) #TODO: getting one dimension back.\n",
    "        #\n",
    "        action = np.array([action])\n",
    "\n",
    "        #--print('My Action_V: {}'.format(action_v))\n",
    "        \n",
    "        next_state, reward, done, info = env_train.step(action) #TODO: Wants a list of actions of size a\n",
    "\n",
    "        \n",
    "        #print(\"env_trainNext State: {}\".format(next_state.shape))\n",
    "        \n",
    "        for s, a, r, ns, d in zip(state, action, reward, next_state, done):\n",
    "            agent.step(s, a, r, ns, d, frame, writer)\n",
    "\n",
    "        print('agent seed', agent.seed)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        # linear annealing to the min epsilon value until eps_frames and from there slowly decease epsilon to 0 until the end of training\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "        # evaluation runs\n",
    "        if frame % 1 == 0:\n",
    "            print('My Action: {}'.format(action))\n",
    "            print(\"state: {}\".format(state))\n",
    "            print(\"score: {}\".format(score))\n",
    "            print(\"state: {}\".format(state))\n",
    "            print(\"action:{}, Number:{}\".format(action,frame))\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)              # save most recent score\n",
    "            writer.add_scalar(\"Average100\", np.mean(scores_window), frame*worker)\n",
    "            \n",
    "            print('\\rEpisode {}\\tFrame {} \\tAverage100 Score: {:.2f}'.format(i_episode*worker, frame*worker, np.mean(scores_window)), end=\"\")\n",
    "            #if i_episode % 100 == 0:\n",
    "            #    print('\\rEpisode {}\\tFrame \\tReward: {}\\tAverage100 Score: {:.2f}'.format(i_episode*worker, frame*worker, round(eval_reward,2), np.mean(scores_window)), end=\"\", flush=True)\n",
    "            i_episode +=1 \n",
    "            state = env_train.reset()\n",
    "            score = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "1.0\n",
      "-1.0\n",
      "run seed 4\n",
      "Using:  cpu\n",
      "seed agent:  None\n",
      "Actor: \n",
      " DeepActor(\n",
      "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=135, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=135, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=135, out_features=128, bias=True)\n",
      "  (fc5): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Critic: \n",
      " DeepIQN(\n",
      "  (head): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (ff_1): Linear(in_features=138, out_features=128, bias=True)\n",
      "  (ff_2): Linear(in_features=138, out_features=128, bias=True)\n",
      "  (cos_embedding): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (ff_3): Linear(in_features=138, out_features=128, bias=True)\n",
      "  (ff_4): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [ 0.18709855 -0.44906706 -0.80239016]\n",
      "trades:1\n",
      "previous:1.0\n",
      "penalty:0\n",
      "step_reward:-0.05720137264609093\n",
      "step_reward:-0.05720137264609093\n",
      "agent step reward:-0.05720137432217598\n",
      "agent seed None\n",
      "My Action: [[ 0.18709855 -0.44906706 -0.80239016]]\n",
      "state: [[942.74146    30.572857   40.92       56.18        1.8709855   0.\n",
      "    0.       ]]\n",
      "score: [-0.05720137]\n",
      "state: [[942.74146    30.572857   40.92       56.18        1.8709855   0.\n",
      "    0.       ]]\n",
      "action:[[ 0.18709855 -0.44906706 -0.80239016]], Number:1\n",
      "-------------------------\n",
      "tensor([[942.7415,  30.5729,  40.9200,  56.1800,   1.8710,   0.0000,   0.0000]])\n",
      "here: [ 0.1770779  -0.42275962 -0.7754728 ]\n",
      "trades:2\n",
      "previous:2.0\n",
      "penalty:0\n",
      "step_reward:0.13835551977251725\n",
      "step_reward:0.13835551977251725\n",
      "agent step reward:0.13835552334785461\n",
      "agent seed None\n",
      "My Action: [[ 0.1770779  -0.42275962 -0.7754728 ]]\n",
      "state: [[888.5495     30.625713   40.83       58.02        3.6417646   0.\n",
      "    0.       ]]\n",
      "score: [0.08115415]\n",
      "state: [[888.5495     30.625713   40.83       58.02        3.6417646   0.\n",
      "    0.       ]]\n",
      "action:[[ 0.1770779  -0.42275962 -0.7754728 ]], Number:2\n",
      "-------------------------\n",
      "tensor([[888.5495,  30.6257,  40.8300,  58.0200,   3.6418,   0.0000,   0.0000]])\n",
      "here: [ 0.17428531 -0.38928702 -0.74502325]\n",
      "trades:3\n",
      "previous:3.0\n",
      "penalty:0\n",
      "step_reward:-2.6764541210447987\n",
      "step_reward:-2.6764541210447987\n",
      "agent step reward:-2.6764540672302246\n",
      "agent seed None\n",
      "My Action: [[ 0.17428531 -0.38928702 -0.74502325]]\n",
      "state: [[835.12      30.13857   41.49      59.78       5.384618   0.\n",
      "    0.      ]]\n",
      "score: [-2.5953]\n",
      "state: [[835.12      30.13857   41.49      59.78       5.384618   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.17428531 -0.38928702 -0.74502325]], Number:3\n",
      "-------------------------\n",
      "tensor([[835.1200,  30.1386,  41.4900,  59.7800,   5.3846,   0.0000,   0.0000]])\n",
      "here: [ 0.16549473 -0.3590484  -0.7171686 ]\n",
      "trades:4\n",
      "previous:4.0\n",
      "penalty:0\n",
      "step_reward:-0.4420821042501757\n",
      "step_reward:-0.4420821042501757\n",
      "agent step reward:-0.44208210706710815\n",
      "agent seed None\n",
      "My Action: [[ 0.16549473 -0.3590484  -0.7171686 ]]\n",
      "state: [[785.1924    30.082857  41.98      62.2        7.039565   0.\n",
      "    0.      ]]\n",
      "score: [-3.0373821]\n",
      "state: [[785.1924    30.082857  41.98      62.2        7.039565   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.16549473 -0.3590484  -0.7171686 ]], Number:4\n",
      "-------------------------\n",
      "tensor([[785.1924,  30.0829,  41.9800,  62.2000,   7.0396,   0.0000,   0.0000]])\n",
      "here: [ 0.15705279 -0.33313727 -0.6895655 ]\n",
      "trades:5\n",
      "previous:5.0\n",
      "penalty:0\n",
      "step_reward:1.6747725647842344\n",
      "step_reward:1.6747725647842344\n",
      "agent step reward:1.6747726202011108\n",
      "agent seed None\n",
      "My Action: [[ 0.15705279 -0.33313727 -0.6895655 ]]\n",
      "state: [[737.8992    30.282858  41.95      61.6        8.610092   0.\n",
      "    0.      ]]\n",
      "score: [-1.3626095]\n",
      "state: [[737.8992    30.282858  41.95      61.6        8.610092   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.15705279 -0.33313727 -0.6895655 ]], Number:5\n",
      "-------------------------\n",
      "tensor([[737.8992,  30.2829,  41.9500,  61.6000,   8.6101,   0.0000,   0.0000]])\n",
      "here: [ 0.1535907  -0.30515283 -0.65644515]\n",
      "trades:6\n",
      "previous:6.0\n",
      "penalty:0\n",
      "step_reward:-2.756942885667627\n",
      "step_reward:-2.756942885667627\n",
      "agent step reward:-2.7569429874420166\n",
      "agent seed None\n",
      "My Action: [[ 0.1535907  -0.30515283 -0.65644515]]\n",
      "state: [[691.341     30.015715  41.47      60.87      10.146      0.\n",
      "    0.      ]]\n",
      "score: [-4.1195526]\n",
      "state: [[691.341     30.015715  41.47      60.87      10.146      0.\n",
      "    0.      ]]\n",
      "action:[[ 0.1535907  -0.30515283 -0.65644515]], Number:6\n",
      "-------------------------\n",
      "tensor([[691.3410,  30.0157,  41.4700,  60.8700,  10.1460,   0.0000,   0.0000]])\n",
      "here: [ 0.14881979 -0.2824521  -0.6262497 ]\n",
      "trades:7\n",
      "previous:7.0\n",
      "penalty:0\n",
      "step_reward:-4.0169168464805125\n",
      "step_reward:-4.0169168464805125\n",
      "agent step reward:-4.016916751861572\n",
      "agent seed None\n",
      "My Action: [[ 0.14881979 -0.2824521  -0.6262497 ]]\n",
      "state: [[646.627     29.674286  42.02      60.43      11.634197   0.\n",
      "    0.      ]]\n",
      "score: [-8.136469]\n",
      "state: [[646.627     29.674286  42.02      60.43      11.634197   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.14881979 -0.2824521  -0.6262497 ]], Number:7\n",
      "-------------------------\n",
      "tensor([[646.6270,  29.6743,  42.0200,  60.4300,  11.6342,   0.0000,   0.0000]])\n",
      "here: [ 0.14238857 -0.25668582 -0.59091246]\n",
      "trades:8\n",
      "previous:8.0\n",
      "penalty:0\n",
      "step_reward:5.423487757122075\n",
      "step_reward:5.423487757122075\n",
      "agent step reward:5.423487663269043\n",
      "agent seed None\n",
      "My Action: [[ 0.14238857 -0.25668582 -0.59091246]]\n",
      "state: [[604.332     30.092857  42.15      61.16      13.058084   0.\n",
      "    0.      ]]\n",
      "score: [-2.7129812]\n",
      "state: [[604.332     30.092857  42.15      61.16      13.058084   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.14238857 -0.25668582 -0.59091246]], Number:8\n",
      "-------------------------\n",
      "tensor([[604.3320,  30.0929,  42.1500,  61.1600,  13.0581,   0.0000,   0.0000]])\n",
      "here: [ 0.13901131 -0.23703228 -0.55883735]\n",
      "trades:9\n",
      "previous:9.0\n",
      "penalty:0\n",
      "step_reward:-2.559946624344775\n",
      "step_reward:-2.559946624344775\n",
      "agent step reward:-2.5599465370178223\n",
      "agent seed None\n",
      "My Action: [[ 0.13901131 -0.23703228 -0.55883735]]\n",
      "state: [[562.4577    29.918571  42.68      61.56      14.448196   0.\n",
      "    0.      ]]\n",
      "score: [-5.2729278]\n",
      "state: [[562.4577    29.918571  42.68      61.56      14.448196   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.13901131 -0.23703228 -0.55883735]], Number:9\n",
      "-------------------------\n",
      "tensor([[562.4577,  29.9186,  42.6800,  61.5600,  14.4482,   0.0000,   0.0000]])\n",
      "here: [ 0.13400392 -0.21431594 -0.5290409 ]\n",
      "trades:10\n",
      "previous:10.0\n",
      "penalty:0\n",
      "step_reward:-7.9342098326960695\n",
      "step_reward:-7.9342098326960695\n",
      "agent step reward:-7.934209823608398\n",
      "agent seed None\n",
      "My Action: [[ 0.13400392 -0.21431594 -0.5290409 ]]\n",
      "state: [[522.3255    29.418571  42.39      60.82      15.788236   0.\n",
      "    0.      ]]\n",
      "score: [-13.207138]\n",
      "state: [[522.3255    29.418571  42.39      60.82      15.788236   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.13400392 -0.21431594 -0.5290409 ]], Number:10\n",
      "-------------------------\n",
      "tensor([[522.3255,  29.4186,  42.3900,  60.8200,  15.7882,   0.0000,   0.0000]])\n",
      "here: [ 0.12697093 -0.19540326 -0.4913197 ]\n",
      "trades:11\n",
      "previous:11.0\n",
      "penalty:0\n",
      "step_reward:22.162343795155493\n",
      "step_reward:22.162343795155493\n",
      "agent step reward:22.162343978881836\n",
      "agent seed None\n",
      "My Action: [[ 0.12697093 -0.19540326 -0.4913197 ]]\n",
      "state: [[484.93515   30.72      42.96      60.65      17.057945   0.\n",
      "    0.      ]]\n",
      "score: [8.955206]\n",
      "state: [[484.93515   30.72      42.96      60.65      17.057945   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.12697093 -0.19540326 -0.4913197 ]], Number:11\n",
      "-------------------------\n",
      "tensor([[484.9352,  30.7200,  42.9600,  60.6500,  17.0579,   0.0000,   0.0000]])\n",
      "here: [ 0.12160497 -0.17471574 -0.4601119 ]\n",
      "trades:12\n",
      "previous:12.0\n",
      "penalty:0\n",
      "step_reward:-8.691398783862496\n",
      "step_reward:-8.691398783862496\n",
      "agent step reward:-8.691398620605469\n",
      "agent seed None\n",
      "My Action: [[ 0.12160497 -0.17471574 -0.4601119 ]]\n",
      "state: [[447.54074   30.24643   42.98      60.2       18.273994   0.\n",
      "    0.      ]]\n",
      "score: [0.2638073]\n",
      "state: [[447.54074   30.24643   42.98      60.2       18.273994   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.12160497 -0.17471574 -0.4601119 ]], Number:12\n",
      "-------------------------\n",
      "tensor([[447.5407,  30.2464,  42.9800,  60.2000,  18.2740,   0.0000,   0.0000]])\n",
      "here: [ 0.11854183 -0.15259022 -0.42206147]\n",
      "trades:13\n",
      "previous:13.0\n",
      "penalty:0\n",
      "step_reward:-10.190888185137965\n",
      "step_reward:-10.190888185137965\n",
      "agent step reward:-10.190888404846191\n",
      "agent seed None\n",
      "My Action: [[ 0.11854183 -0.15259022 -0.42206147]]\n",
      "state: [[411.6502    29.724571  42.16      59.2       19.459414   0.\n",
      "    0.      ]]\n",
      "score: [-9.927081]\n",
      "state: [[411.6502    29.724571  42.16      59.2       19.459414   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.11854183 -0.15259022 -0.42206147]], Number:13\n",
      "-------------------------\n",
      "tensor([[411.6502,  29.7246,  42.1600,  59.2000,  19.4594,   0.0000,   0.0000]])\n",
      "here: [ 0.1145518  -0.13163477 -0.38855758]\n",
      "trades:14\n",
      "previous:14.0\n",
      "penalty:0\n",
      "step_reward:-30.41749242289245\n",
      "step_reward:-30.41749242289245\n",
      "agent step reward:-30.417491912841797\n",
      "agent seed None\n",
      "My Action: [[ 0.1145518  -0.13163477 -0.38855758]]\n",
      "state: [[377.56613  28.25     38.59     57.77     20.60493   0.        0.     ]]\n",
      "score: [-40.344574]\n",
      "state: [[377.56613  28.25     38.59     57.77     20.60493   0.        0.     ]]\n",
      "action:[[ 0.1145518  -0.13163477 -0.38855758]], Number:14\n",
      "-------------------------\n",
      "tensor([[377.5661,  28.2500,  38.5900,  57.7700,  20.6049,   0.0000,   0.0000]])\n",
      "here: [ 0.1095165  -0.10919268 -0.35792223]\n",
      "trades:15\n",
      "previous:15.0\n",
      "penalty:0\n",
      "step_reward:16.476634627725502\n",
      "step_reward:16.476634627725502\n",
      "agent step reward:16.476634979248047\n",
      "agent seed None\n",
      "My Action: [[ 0.1095165  -0.10919268 -0.35792223]]\n",
      "state: [[346.59677   29.010714  37.79      57.78      21.700096   0.\n",
      "    0.      ]]\n",
      "score: [-23.867939]\n",
      "state: [[346.59677   29.010714  37.79      57.78      21.700096   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.1095165  -0.10919268 -0.35792223]], Number:15\n",
      "-------------------------\n",
      "tensor([[346.5968,  29.0107,  37.7900,  57.7800,  21.7001,   0.0000,   0.0000]])\n",
      "here: [ 0.10062785 -0.08817132 -0.33001167]\n",
      "trades:16\n",
      "previous:16.0\n",
      "penalty:0\n",
      "step_reward:9.26420171212817\n",
      "step_reward:9.26420171212817\n",
      "agent step reward:9.264202117919922\n",
      "agent seed None\n",
      "My Action: [[ 0.10062785 -0.08817132 -0.33001167]]\n",
      "state: [[317.37473   29.42      38.1       57.71      22.706375   0.\n",
      "    0.      ]]\n",
      "score: [-14.603737]\n",
      "state: [[317.37473   29.42      38.1       57.71      22.706375   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.10062785 -0.08817132 -0.33001167]], Number:16\n",
      "-------------------------\n",
      "tensor([[317.3747,  29.4200,  38.1000,  57.7100,  22.7064,   0.0000,   0.0000]])\n",
      "here: [ 0.09056354 -0.07154166 -0.30041376]\n",
      "trades:17\n",
      "previous:17.0\n",
      "penalty:0\n",
      "step_reward:6.530748734803865\n",
      "step_reward:6.530748734803865\n",
      "agent step reward:6.5307488441467285\n",
      "agent seed None\n",
      "My Action: [[ 0.09056354 -0.07154166 -0.30041376]]\n",
      "state: [[290.70428   29.697714  38.67      61.93      23.61201    0.\n",
      "    0.      ]]\n",
      "score: [-8.0729885]\n",
      "state: [[290.70428   29.697714  38.67      61.93      23.61201    0.\n",
      "    0.      ]]\n",
      "action:[[ 0.09056354 -0.07154166 -0.30041376]], Number:17\n",
      "-------------------------\n",
      "tensor([[290.7043,  29.6977,  38.6700,  61.9300,  23.6120,   0.0000,   0.0000]])\n",
      "here: [ 0.0844118  -0.05397965 -0.27120262]\n",
      "trades:18\n",
      "previous:18.0\n",
      "penalty:0\n",
      "step_reward:-30.05020590761228\n",
      "step_reward:-30.05020590761228\n",
      "agent step reward:-30.05020523071289\n",
      "agent seed None\n",
      "My Action: [[ 0.0844118  -0.05397965 -0.27120262]]\n",
      "state: [[265.61084   28.47      37.43      62.56      24.456127   0.\n",
      "    0.      ]]\n",
      "score: [-38.12319]\n",
      "state: [[265.61084   28.47      37.43      62.56      24.456127   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.0844118  -0.05397965 -0.27120262]], Number:18\n",
      "-------------------------\n",
      "tensor([[265.6108,  28.4700,  37.4300,  62.5600,  24.4561,   0.0000,   0.0000]])\n",
      "here: [ 0.08079015 -0.04077092 -0.24441588]\n",
      "trades:20\n",
      "previous:20.0\n",
      "penalty:0\n",
      "step_reward:-26.184819124470096\n",
      "step_reward:-26.184819124470096\n",
      "agent step reward:-26.184818267822266\n",
      "agent seed None\n",
      "My Action: [[ 0.08079015 -0.04077092 -0.24441588]]\n",
      "state: [[257.8627     27.43757    37.66       60.6        25.264029   -0.4077092\n",
      "    0.       ]]\n",
      "score: [-64.308014]\n",
      "state: [[257.8627     27.43757    37.66       60.6        25.264029   -0.4077092\n",
      "    0.       ]]\n",
      "action:[[ 0.08079015 -0.04077092 -0.24441588]], Number:19\n",
      "-------------------------\n",
      "tensor([[257.8627,  27.4376,  37.6600,  60.6000,  25.2640,  -0.4077,   0.0000]])\n",
      "Finished\n",
      "[257.8627078378702, 27.43757143, 37.66, 60.6, 25.264029264450073, -0.4077092111110687, 0]\n",
      "end_total_asset:935.6919865003865\n",
      "Sharpe:  -3.7967239574212757\n",
      "agent step reward:-26.184818267822266\n",
      "agent seed None\n",
      "My Action: [[ 0.07624788 -0.03668565 -0.236892  ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.49283]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[ 0.07624788 -0.03668565 -0.236892  ]], Number:20\n",
      "-------------------------\n",
      "\r",
      "Episode 1\tFrame 20 \tAverage100 Score: -90.49tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [ 0.21196678 -0.4227054  -0.78678215]\n",
      "trades:1\n",
      "previous:1.0\n",
      "penalty:0\n",
      "step_reward:-0.06480429986993386\n",
      "step_reward:-0.06480429986993386\n",
      "agent step reward:-0.06480430066585541\n",
      "agent seed None\n",
      "My Action: [[ 0.21196678 -0.4227054  -0.78678215]]\n",
      "state: [[935.1309     30.572857   40.92       56.18        2.1196678   0.\n",
      "    0.       ]]\n",
      "score: [-0.0648043]\n",
      "state: [[935.1309     30.572857   40.92       56.18        2.1196678   0.\n",
      "    0.       ]]\n",
      "action:[[ 0.21196678 -0.4227054  -0.78678215]], Number:21\n",
      "-------------------------\n",
      "tensor([[935.1309,  30.5729,  40.9200,  56.1800,   2.1197,   0.0000,   0.0000]])\n",
      "here: [ 0.19998199 -0.39015317 -0.7612053 ]\n",
      "trades:2\n",
      "previous:2.0\n",
      "penalty:0\n",
      "step_reward:0.15660416894513673\n",
      "step_reward:0.15660416894513673\n",
      "agent step reward:0.15660417079925537\n",
      "agent seed None\n",
      "My Action: [[ 0.19998199 -0.39015317 -0.7612053 ]]\n",
      "state: [[873.92957   30.625713  40.83      58.02       4.119488   0.\n",
      "    0.      ]]\n",
      "score: [0.09179987]\n",
      "state: [[873.92957   30.625713  40.83      58.02       4.119488   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.19998199 -0.39015317 -0.7612053 ]], Number:22\n",
      "-------------------------\n",
      "tensor([[873.9296,  30.6257,  40.8300,  58.0200,   4.1195,   0.0000,   0.0000]])\n",
      "here: [ 0.18533123 -0.36218852 -0.72855496]\n",
      "trades:3\n",
      "previous:3.0\n",
      "penalty:0\n",
      "step_reward:-2.9663658357064833\n",
      "step_reward:-2.9663658357064833\n",
      "agent step reward:-2.9663658142089844\n",
      "agent seed None\n",
      "My Action: [[ 0.18533123 -0.36218852 -0.72855496]]\n",
      "state: [[817.1138   30.13857  41.49     59.78      5.9728    0.        0.     ]]\n",
      "score: [-2.8745658]\n",
      "state: [[817.1138   30.13857  41.49     59.78      5.9728    0.        0.     ]]\n",
      "action:[[ 0.18533123 -0.36218852 -0.72855496]], Number:23\n",
      "-------------------------\n",
      "tensor([[817.1138,  30.1386,  41.4900,  59.7800,   5.9728,   0.0000,   0.0000]])\n",
      "here: [ 0.1770526  -0.33555174 -0.69913167]\n",
      "trades:4\n",
      "previous:4.0\n",
      "penalty:0\n",
      "step_reward:-0.48477503334368066\n",
      "step_reward:-0.48477503334368066\n",
      "agent step reward:-0.48477503657341003\n",
      "agent seed None\n",
      "My Action: [[ 0.1770526  -0.33555174 -0.69913167]]\n",
      "state: [[763.6993    30.082857  41.98      62.2        7.743326   0.\n",
      "    0.      ]]\n",
      "score: [-3.359341]\n",
      "state: [[763.6993    30.082857  41.98      62.2        7.743326   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.1770526  -0.33555174 -0.69913167]], Number:24\n",
      "-------------------------\n",
      "tensor([[763.6993,  30.0829,  41.9800,  62.2000,   7.7433,   0.0000,   0.0000]])\n",
      "here: [ 0.16574553 -0.30710188 -0.671246  ]\n",
      "trades:5\n",
      "previous:5.0\n",
      "penalty:0\n",
      "step_reward:1.8302952427509354\n",
      "step_reward:1.8302952427509354\n",
      "agent step reward:1.830295205116272\n",
      "agent seed None\n",
      "My Action: [[ 0.16574553 -0.30710188 -0.671246  ]]\n",
      "state: [[713.78845   30.282858  41.95      61.6        9.400782   0.\n",
      "    0.      ]]\n",
      "score: [-1.5290457]\n",
      "state: [[713.78845   30.282858  41.95      61.6        9.400782   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.16574553 -0.30710188 -0.671246  ]], Number:25\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[713.7885,  30.2829,  41.9500,  61.6000,   9.4008,   0.0000,   0.0000]])\n",
      "here: [ 0.15799057 -0.2794054  -0.6351183 ]\n",
      "trades:6\n",
      "previous:6.0\n",
      "penalty:0\n",
      "step_reward:-2.981256054297546\n",
      "step_reward:-2.981256054297546\n",
      "agent step reward:-2.9812560081481934\n",
      "agent seed None\n",
      "My Action: [[ 0.15799057 -0.2794054  -0.6351183 ]]\n",
      "state: [[665.89655   30.015715  41.47      60.87      10.980687   0.\n",
      "    0.      ]]\n",
      "score: [-4.5103016]\n",
      "state: [[665.89655   30.015715  41.47      60.87      10.980687   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.15799057 -0.2794054  -0.6351183 ]], Number:26\n",
      "-------------------------\n",
      "tensor([[665.8965,  30.0157,  41.4700,  60.8700,  10.9807,   0.0000,   0.0000]])\n",
      "here: [ 0.15109135 -0.25173962 -0.6061452 ]\n",
      "trades:7\n",
      "previous:7.0\n",
      "penalty:0\n",
      "step_reward:-4.310340532206169\n",
      "step_reward:-4.310340532206169\n",
      "agent step reward:-4.310340404510498\n",
      "agent seed None\n",
      "My Action: [[ 0.15109135 -0.25173962 -0.6061452 ]]\n",
      "state: [[620.50006   29.674286  42.02      60.43      12.4916     0.\n",
      "    0.      ]]\n",
      "score: [-8.820642]\n",
      "state: [[620.50006   29.674286  42.02      60.43      12.4916     0.\n",
      "    0.      ]]\n",
      "action:[[ 0.15109135 -0.25173962 -0.6061452 ]], Number:27\n",
      "-------------------------\n",
      "tensor([[620.5001,  29.6743,  42.0200,  60.4300,  12.4916,   0.0000,   0.0000]])\n",
      "here: [ 0.14126211 -0.22660808 -0.5704866 ]\n",
      "trades:8\n",
      "previous:8.0\n",
      "penalty:0\n",
      "step_reward:5.77799136750582\n",
      "step_reward:5.77799136750582\n",
      "agent step reward:5.77799129486084\n",
      "agent seed None\n",
      "My Action: [[ 0.14126211 -0.22660808 -0.5704866 ]]\n",
      "state: [[578.5396    30.092857  42.15      61.16      13.904222   0.\n",
      "    0.      ]]\n",
      "score: [-3.0426512]\n",
      "state: [[578.5396    30.092857  42.15      61.16      13.904222   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.14126211 -0.22660808 -0.5704866 ]], Number:28\n",
      "-------------------------\n",
      "tensor([[578.5396,  30.0929,  42.1500,  61.1600,  13.9042,   0.0000,   0.0000]])\n",
      "here: [ 0.13388665 -0.20280093 -0.5378728 ]\n",
      "trades:9\n",
      "previous:9.0\n",
      "penalty:0\n",
      "step_reward:-2.696942733366541\n",
      "step_reward:-2.696942733366541\n",
      "agent step reward:-2.6969428062438965\n",
      "agent seed None\n",
      "My Action: [[ 0.13388665 -0.20280093 -0.5378728 ]]\n",
      "state: [[538.209     29.918571  42.68      61.56      15.243088   0.\n",
      "    0.      ]]\n",
      "score: [-5.739594]\n",
      "state: [[538.209     29.918571  42.68      61.56      15.243088   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.13388665 -0.20280093 -0.5378728 ]], Number:29\n",
      "-------------------------\n",
      "tensor([[538.2090,  29.9186,  42.6800,  61.5600,  15.2431,   0.0000,   0.0000]])\n",
      "here: [ 0.13036916 -0.18535616 -0.5076907 ]\n",
      "trades:10\n",
      "previous:10.0\n",
      "penalty:0\n",
      "step_reward:-8.312394407307238\n",
      "step_reward:-8.312394407307238\n",
      "agent step reward:-8.312394142150879\n",
      "agent seed None\n",
      "My Action: [[ 0.13036916 -0.18535616 -0.5076907 ]]\n",
      "state: [[499.1654    29.418571  42.39      60.82      16.54678    0.\n",
      "    0.      ]]\n",
      "score: [-14.051989]\n",
      "state: [[499.1654    29.418571  42.39      60.82      16.54678    0.\n",
      "    0.      ]]\n",
      "action:[[ 0.13036916 -0.18535616 -0.5076907 ]], Number:30\n",
      "-------------------------\n",
      "tensor([[499.1654,  29.4186,  42.3900,  60.8200,  16.5468,   0.0000,   0.0000]])\n",
      "here: [ 0.12291979 -0.168653   -0.4744383 ]\n",
      "trades:11\n",
      "previous:11.0\n",
      "penalty:0\n",
      "step_reward:23.09800372315931\n",
      "step_reward:23.09800372315931\n",
      "agent step reward:23.098003387451172\n",
      "agent seed None\n",
      "My Action: [[ 0.12291979 -0.168653   -0.4744383 ]]\n",
      "state: [[462.968     30.72      42.96      60.65      17.775978   0.\n",
      "    0.      ]]\n",
      "score: [9.046015]\n",
      "state: [[462.968     30.72      42.96      60.65      17.775978   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.12291979 -0.168653   -0.4744383 ]], Number:31\n",
      "-------------------------\n",
      "tensor([[462.9680,  30.7200,  42.9600,  60.6500,  17.7760,   0.0000,   0.0000]])\n",
      "here: [ 0.1188098  -0.15230632 -0.44510254]\n",
      "trades:12\n",
      "previous:12.0\n",
      "penalty:0\n",
      "step_reward:-9.017342756907965\n",
      "step_reward:-9.017342756907965\n",
      "agent step reward:-9.017342567443848\n",
      "agent seed None\n",
      "My Action: [[ 0.1188098  -0.15230632 -0.44510254]]\n",
      "state: [[426.4331    30.24643   42.98      60.2       18.964075   0.\n",
      "    0.      ]]\n",
      "score: [0.02867222]\n",
      "state: [[426.4331    30.24643   42.98      60.2       18.964075   0.\n",
      "    0.      ]]\n",
      "action:[[ 0.1188098  -0.15230632 -0.44510254]], Number:32\n",
      "-------------------------\n",
      "tensor([[426.4331,  30.2464,  42.9800,  60.2000,  18.9641,   0.0000,   0.0000]])\n",
      "here: [ 0.10884548 -0.13534781 -0.4070863 ]\n",
      "trades:13\n",
      "previous:13.0\n",
      "penalty:0\n",
      "step_reward:-10.497478040079955\n",
      "step_reward:-10.497478040079955\n",
      "agent step reward:-10.497478485107422\n",
      "agent seed None\n",
      "My Action: [[ 0.10884548 -0.13534781 -0.4070863 ]]\n",
      "state: [[393.47833   29.724571  42.16      59.2       20.05253    0.\n",
      "    0.      ]]\n",
      "score: [-10.468806]\n",
      "state: [[393.47833   29.724571  42.16      59.2       20.05253    0.\n",
      "    0.      ]]\n",
      "action:[[ 0.10884548 -0.13534781 -0.4070863 ]], Number:33\n",
      "-------------------------\n",
      "tensor([[393.4783,  29.7246,  42.1600,  59.2000,  20.0525,   0.0000,   0.0000]])\n",
      "here: [-0.99641466 -0.9971741  -0.99876285]\n",
      "trades:14\n",
      "previous:14.0\n",
      "penalty:0\n",
      "step_reward:-15.172222505027094\n",
      "step_reward:-15.172222505027094\n",
      "agent step reward:-15.172222137451172\n",
      "agent seed None\n",
      "My Action: [[-0.99641466 -0.9971741  -0.99876285]]\n",
      "state: [[689.3621    28.25      38.59      57.77      10.088384   0.\n",
      "    0.      ]]\n",
      "score: [-25.64103]\n",
      "state: [[689.3621    28.25      38.59      57.77      10.088384   0.\n",
      "    0.      ]]\n",
      "action:[[-0.99641466 -0.9971741  -0.99876285]], Number:34\n",
      "-------------------------\n",
      "tensor([[689.3621,  28.2500,  38.5900,  57.7700,  10.0884,   0.0000,   0.0000]])\n",
      "here: [-0.992146  -0.9966084 -0.9983367]\n",
      "trades:15\n",
      "previous:15.0\n",
      "penalty:0\n",
      "step_reward:-0.15330004935583474\n",
      "step_reward:-0.15330004935583474\n",
      "agent step reward:-0.15330004692077637\n",
      "agent seed None\n",
      "My Action: [[-0.992146  -0.9966084 -0.9983367]]\n",
      "state: [[9.6936310e+02 2.9010714e+01 3.7790001e+01 5.7779999e+01 1.6692364e-01\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "score: [-25.794329]\n",
      "state: [[9.6936310e+02 2.9010714e+01 3.7790001e+01 5.7779999e+01 1.6692364e-01\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "action:[[-0.992146  -0.9966084 -0.9983367]], Number:35\n",
      "-------------------------\n",
      "tensor([[9.6936e+02, 2.9011e+01, 3.7790e+01, 5.7780e+01, 1.6692e-01, 0.0000e+00,\n",
      "         0.0000e+00]])\n",
      "here: [-0.9995253  -0.99955785 -0.9986506 ]\n",
      "trades:16\n",
      "previous:16.0\n",
      "penalty:0\n",
      "step_reward:-0.004842574090957896\n",
      "step_reward:-0.004842574090957896\n",
      "agent step reward:-0.004842574242502451\n",
      "agent seed None\n",
      "My Action: [[-0.9995253  -0.99955785 -0.9986506 ]]\n",
      "state: [[974.2008  29.42    38.1     57.71     0.       0.       0.    ]]\n",
      "score: [-25.799171]\n",
      "state: [[974.2008  29.42    38.1     57.71     0.       0.       0.    ]]\n",
      "action:[[-0.9995253  -0.99955785 -0.9986506 ]], Number:36\n",
      "-------------------------\n",
      "tensor([[974.2008,  29.4200,  38.1000,  57.7100,   0.0000,   0.0000,   0.0000]])\n",
      "here: [-0.9945854 -0.9988029 -0.9996729]\n",
      "trades:16\n",
      "previous:16.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945854 -0.9988029 -0.9996729]]\n",
      "state: [[974.2008    29.697714  38.67      61.93       0.         0.\n",
      "    0.      ]]\n",
      "score: [-35.79917]\n",
      "state: [[974.2008    29.697714  38.67      61.93       0.         0.\n",
      "    0.      ]]\n",
      "action:[[-0.9945854 -0.9988029 -0.9996729]], Number:37\n",
      "-------------------------\n",
      "tensor([[974.2008,  29.6977,  38.6700,  61.9300,   0.0000,   0.0000,   0.0000]])\n",
      "here: [-0.9960747  -0.99800754 -0.9920189 ]\n",
      "trades:16\n",
      "previous:16.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9960747  -0.99800754 -0.9920189 ]]\n",
      "state: [[974.2008  28.47    37.43    62.56     0.       0.       0.    ]]\n",
      "score: [-45.79917]\n",
      "state: [[974.2008  28.47    37.43    62.56     0.       0.       0.    ]]\n",
      "action:[[-0.9960747  -0.99800754 -0.9920189 ]], Number:38\n",
      "-------------------------\n",
      "tensor([[974.2008,  28.4700,  37.4300,  62.5600,   0.0000,   0.0000,   0.0000]])\n",
      "here: [-0.9990218  -0.99576604 -0.99380964]\n",
      "trades:16\n",
      "previous:16.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h3/d5kvsb7d4rl8jq8scv_520vw0000gn/T/ipykernel_1882/1294247666.py:114: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  dones       = torch.FloatTensor(dones).to(self.device).unsqueeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9990218  -0.99576604 -0.99380964]]\n",
      "state: [[974.2008   27.43757  37.66     60.6       0.        0.        0.     ]]\n",
      "score: [-55.79917]\n",
      "state: [[974.2008   27.43757  37.66     60.6       0.        0.        0.     ]]\n",
      "action:[[-0.9990218  -0.99576604 -0.99380964]], Number:39\n",
      "-------------------------\n",
      "tensor([[974.2008,  27.4376,  37.6600,  60.6000,   0.0000,   0.0000,   0.0000]])\n",
      "Finished\n",
      "[974.2008296808018, 27.43757143, 37.66, 60.6, 0.0, 0, 0]\n",
      "end_total_asset:974.2008296808018\n",
      "Sharpe:  -2.7450655355088283\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99672544 -0.99209875 -0.9961779 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-65.79917]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99672544 -0.99209875 -0.9961779 ]], Number:40\n",
      "-------------------------\n",
      "Episode 2\tFrame 40 \tAverage100 Score: -78.15tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9980651 -0.996715  -0.9997051]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9980651 -0.996715  -0.9997051]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9980651 -0.996715  -0.9997051]], Number:41\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99663025 -0.9980113  -0.9928856 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99663025 -0.9980113  -0.9928856 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99663025 -0.9980113  -0.9928856 ]], Number:42\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99335164 -0.9960114  -0.9997468 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99335164 -0.9960114  -0.9997468 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99335164 -0.9960114  -0.9997468 ]], Number:43\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99796486 -0.9980609  -0.9983355 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99796486 -0.9980609  -0.9983355 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99796486 -0.9980609  -0.9983355 ]], Number:44\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99814826 -0.99304235 -0.9988664 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99814826 -0.99304235 -0.9988664 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99814826 -0.99304235 -0.9988664 ]], Number:45\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995898  -0.9925757  -0.99547726]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995898  -0.9925757  -0.99547726]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9995898  -0.9925757  -0.99547726]], Number:46\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99207544 -0.9967763  -0.99279237]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99207544 -0.9967763  -0.99279237]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99207544 -0.9967763  -0.99279237]], Number:47\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947682  -0.99367315 -0.9940422 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947682  -0.99367315 -0.9940422 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947682  -0.99367315 -0.9940422 ]], Number:48\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9960457  -0.99925673 -0.99831265]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9960457  -0.99925673 -0.99831265]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9960457  -0.99925673 -0.99831265]], Number:49\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99300957 -0.9928019  -0.99260336]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99300957 -0.9928019  -0.99260336]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99300957 -0.9928019  -0.99260336]], Number:50\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973073  -0.9947447  -0.99360394]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973073  -0.9947447  -0.99360394]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9973073  -0.9947447  -0.99360394]], Number:51\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99486005 -0.9934814  -0.9957758 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99486005 -0.9934814  -0.9957758 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99486005 -0.9934814  -0.9957758 ]], Number:52\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947621  -0.9945123  -0.99785364]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947621  -0.9945123  -0.99785364]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947621  -0.9945123  -0.99785364]], Number:53\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926176  -0.99234974 -0.99940497]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926176  -0.99234974 -0.99940497]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9926176  -0.99234974 -0.99940497]], Number:54\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922313  -0.9923058  -0.99465317]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922313  -0.9923058  -0.99465317]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922313  -0.9923058  -0.99465317]], Number:55\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996436 -0.9928082 -0.9989789]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996436 -0.9928082 -0.9989789]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9996436 -0.9928082 -0.9989789]], Number:56\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922517  -0.99466246 -0.9995161 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922517  -0.99466246 -0.9995161 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922517  -0.99466246 -0.9995161 ]], Number:57\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9986619  -0.99491847 -0.9954463 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9986619  -0.99491847 -0.9954463 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9986619  -0.99491847 -0.9954463 ]], Number:58\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99402803 -0.9925802  -0.9982517 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99402803 -0.9925802  -0.9982517 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99402803 -0.9925802  -0.9982517 ]], Number:59\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99997383 -0.9926211  -0.9998951 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99997383 -0.9926211  -0.9998951 ]], Number:60\n",
      "-------------------------\n",
      "\r",
      "Episode 3\tFrame 60 \tAverage100 Score: -118.76tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929886  -0.9990729  -0.99352103]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929886  -0.9990729  -0.99352103]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929886  -0.9990729  -0.99352103]], Number:61\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99373627 -0.99297696 -0.99559516]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99373627 -0.99297696 -0.99559516]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99373627 -0.99297696 -0.99559516]], Number:62\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99297035 -0.9983866  -0.99462813]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99297035 -0.9983866  -0.99462813]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99297035 -0.9983866  -0.99462813]], Number:63\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99735487 -0.992866   -0.9938114 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99735487 -0.992866   -0.9938114 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99735487 -0.992866   -0.9938114 ]], Number:64\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962279  -0.99578875 -0.9997889 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962279  -0.99578875 -0.9997889 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962279  -0.99578875 -0.9997889 ]], Number:65\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99972653 -0.9952441  -0.99608934]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99972653 -0.9952441  -0.99608934]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99972653 -0.9952441  -0.99608934]], Number:66\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930822 -0.995135  -0.9988899]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930822 -0.995135  -0.9988899]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930822 -0.995135  -0.9988899]], Number:67\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99709946 -0.99385935 -0.9958161 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99709946 -0.99385935 -0.9958161 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99709946 -0.99385935 -0.9958161 ]], Number:68\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999156  -0.9932985  -0.99337953]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h3/d5kvsb7d4rl8jq8scv_520vw0000gn/T/ipykernel_1882/2830659553.py:128: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9999156  -0.9932985  -0.99337953]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9999156  -0.9932985  -0.99337953]], Number:69\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993189  -0.995653   -0.99695075]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993189  -0.995653   -0.99695075]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993189  -0.995653   -0.99695075]], Number:70\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9937009 -0.9975107 -0.9981304]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937009 -0.9975107 -0.9981304]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9937009 -0.9975107 -0.9981304]], Number:71\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961068  -0.99226975 -0.999239  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961068  -0.99226975 -0.999239  ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9961068  -0.99226975 -0.999239  ]], Number:72\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990844  -0.9950323  -0.99291724]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990844  -0.9950323  -0.99291724]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9990844  -0.9950323  -0.99291724]], Number:73\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959002  -0.9965284  -0.99313724]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959002  -0.9965284  -0.99313724]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9959002  -0.9965284  -0.99313724]], Number:74\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9937873  -0.99946463 -0.9929494 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937873  -0.99946463 -0.9929494 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9937873  -0.99946463 -0.9929494 ]], Number:75\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984332  -0.99758154 -0.9933085 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984332  -0.99758154 -0.9933085 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9984332  -0.99758154 -0.9933085 ]], Number:76\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966203 -0.9936132 -0.998661 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966203 -0.9936132 -0.998661 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966203 -0.9936132 -0.998661 ]], Number:77\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930057 -0.9985892 -0.9988055]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930057 -0.9985892 -0.9988055]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9930057 -0.9985892 -0.9988055]], Number:78\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99604595 -0.9972913  -0.9956651 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99604595 -0.9972913  -0.9956651 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99604595 -0.9972913  -0.9956651 ]], Number:79\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927674 -0.9943159 -0.9999555]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927674 -0.9943159 -0.9999555]], Number:80\n",
      "-------------------------\n",
      "Episode 4\tFrame 80 \tAverage100 Score: -139.07tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975055  -0.9956404  -0.99610794]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975055  -0.9956404  -0.99610794]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975055  -0.9956404  -0.99610794]], Number:81\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99427533 -0.996126   -0.99939454]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99427533 -0.996126   -0.99939454]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99427533 -0.996126   -0.99939454]], Number:82\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99803644 -0.99321944 -0.99714565]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99803644 -0.99321944 -0.99714565]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99803644 -0.99321944 -0.99714565]], Number:83\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938666 -0.9921136 -0.9949864]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938666 -0.9921136 -0.9949864]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938666 -0.9921136 -0.9949864]], Number:84\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945861  -0.99512374 -0.9974938 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945861  -0.99512374 -0.9974938 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945861  -0.99512374 -0.9974938 ]], Number:85\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99269766 -0.99626374 -0.99270874]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99269766 -0.99626374 -0.99270874]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99269766 -0.99626374 -0.99270874]], Number:86\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975548 -0.9930598 -0.9937052]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9975548 -0.9930598 -0.9937052]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975548 -0.9930598 -0.9937052]], Number:87\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995096   -0.9964635  -0.99887466]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995096   -0.9964635  -0.99887466]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995096   -0.9964635  -0.99887466]], Number:88\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99383163 -0.99710256 -0.9947033 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99383163 -0.99710256 -0.9947033 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99383163 -0.99710256 -0.9947033 ]], Number:89\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.998934  -0.9993395 -0.9988485]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998934  -0.9993395 -0.9988485]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.998934  -0.9993395 -0.9988485]], Number:90\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935278  -0.99857867 -0.9927847 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9935278  -0.99857867 -0.9927847 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9935278  -0.99857867 -0.9927847 ]], Number:91\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970241  -0.99539214 -0.9971965 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970241  -0.99539214 -0.9971965 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9970241  -0.99539214 -0.9971965 ]], Number:92\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950334  -0.99925226 -0.9967796 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950334  -0.99925226 -0.9967796 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950334  -0.99925226 -0.9967796 ]], Number:93\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925105 -0.9985626 -0.994766 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925105 -0.9985626 -0.994766 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9925105 -0.9985626 -0.994766 ]], Number:94\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99738663 -0.99759537 -0.9998146 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99738663 -0.99759537 -0.9998146 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99738663 -0.99759537 -0.9998146 ]], Number:95\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998392  -0.9924049  -0.99336183]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9998392  -0.9924049  -0.99336183]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9998392  -0.9924049  -0.99336183]], Number:96\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935912  -0.993542   -0.99237335]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9935912  -0.993542   -0.99237335]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9935912  -0.993542   -0.99237335]], Number:97\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99873227 -0.99532664 -0.9960381 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99873227 -0.99532664 -0.9960381 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99873227 -0.99532664 -0.9960381 ]], Number:98\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954091 -0.9924967 -0.993918 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954091 -0.9924967 -0.993918 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9954091 -0.9924967 -0.993918 ]], Number:99\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99225223 -0.9990654  -0.9947876 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99225223 -0.9990654  -0.9947876 ]], Number:100\n",
      "-------------------------\n",
      "Episode 5\tFrame 100 \tAverage100 Score: -151.26tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99459684 -0.9940384  -0.99505705]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99459684 -0.9940384  -0.99505705]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99459684 -0.9940384  -0.99505705]], Number:101\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933499  -0.9975771  -0.99257743]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933499  -0.9975771  -0.99257743]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9933499  -0.9975771  -0.99257743]], Number:102\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9967511  -0.9952077  -0.99282503]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9967511  -0.9952077  -0.99282503]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9967511  -0.9952077  -0.99282503]], Number:103\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943713 -0.9975226 -0.998157 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943713 -0.9975226 -0.998157 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9943713 -0.9975226 -0.998157 ]], Number:104\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99738705 -0.99498564 -0.9920284 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99738705 -0.99498564 -0.9920284 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99738705 -0.99498564 -0.9920284 ]], Number:105\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99280787 -0.9967983  -0.9967947 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99280787 -0.9967983  -0.9967947 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99280787 -0.9967983  -0.9967947 ]], Number:106\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99346006 -0.99772984 -0.9967075 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99346006 -0.99772984 -0.9967075 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99346006 -0.99772984 -0.9967075 ]], Number:107\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99989456 -0.99852884 -0.9956784 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99989456 -0.99852884 -0.9956784 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99989456 -0.99852884 -0.9956784 ]], Number:108\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99445367 -0.9950819  -0.9970856 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99445367 -0.9950819  -0.9970856 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99445367 -0.9950819  -0.9970856 ]], Number:109\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99239147 -0.9950142  -0.9987516 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99239147 -0.9950142  -0.9987516 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99239147 -0.9950142  -0.9987516 ]], Number:110\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99945825 -0.9922097  -0.99209744]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99945825 -0.9922097  -0.99209744]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99945825 -0.9922097  -0.99209744]], Number:111\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926403 -0.9951696 -0.9975021]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926403 -0.9951696 -0.9975021]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9926403 -0.9951696 -0.9975021]], Number:112\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992693 -0.9979368 -0.9982227]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992693 -0.9979368 -0.9982227]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992693 -0.9979368 -0.9982227]], Number:113\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99257404 -0.9928595  -0.9937766 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99257404 -0.9928595  -0.9937766 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99257404 -0.9928595  -0.9937766 ]], Number:114\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988102 -0.9980932 -0.9976063]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988102 -0.9980932 -0.9976063]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988102 -0.9980932 -0.9976063]], Number:115\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99241656 -0.99869347 -0.9936765 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99241656 -0.99869347 -0.9936765 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99241656 -0.99869347 -0.9936765 ]], Number:116\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945544  -0.99562293 -0.99232566]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945544  -0.99562293 -0.99232566]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945544  -0.99562293 -0.99232566]], Number:117\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9979013  -0.9958051  -0.99873984]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9979013  -0.9958051  -0.99873984]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9979013  -0.9958051  -0.99873984]], Number:118\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992259 -0.999746  -0.9974679]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992259 -0.999746  -0.9974679]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9992259 -0.999746  -0.9974679]], Number:119\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990258  -0.99950993 -0.99205965]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9990258  -0.99950993 -0.99205965]], Number:120\n",
      "-------------------------\n",
      "Episode 6\tFrame 120 \tAverage100 Score: -159.38tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976875  -0.99287814 -0.9943841 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976875  -0.99287814 -0.9943841 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976875  -0.99287814 -0.9943841 ]], Number:121\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941494  -0.99475855 -0.99237907]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941494  -0.99475855 -0.99237907]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941494  -0.99475855 -0.99237907]], Number:122\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99297214 -0.9942445  -0.99552035]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99297214 -0.9942445  -0.99552035]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99297214 -0.9942445  -0.99552035]], Number:123\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944499  -0.99421036 -0.99558115]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944499  -0.99421036 -0.99558115]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9944499  -0.99421036 -0.99558115]], Number:124\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959796  -0.99876636 -0.9932453 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959796  -0.99876636 -0.9932453 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9959796  -0.99876636 -0.9932453 ]], Number:125\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961265  -0.9994576  -0.99865574]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9961265  -0.9994576  -0.99865574]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9961265  -0.9994576  -0.99865574]], Number:126\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99300176 -0.99795145 -0.9968694 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99300176 -0.99795145 -0.9968694 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99300176 -0.99795145 -0.9968694 ]], Number:127\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945429  -0.99310726 -0.9973726 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945429  -0.99310726 -0.9973726 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945429  -0.99310726 -0.9973726 ]], Number:128\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99690557 -0.9966153  -0.9997756 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99690557 -0.9966153  -0.9997756 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99690557 -0.9966153  -0.9997756 ]], Number:129\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929868 -0.999848  -0.9923192]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929868 -0.999848  -0.9923192]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929868 -0.999848  -0.9923192]], Number:130\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987799  -0.99874663 -0.99321127]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987799  -0.99874663 -0.99321127]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9987799  -0.99874663 -0.99321127]], Number:131\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934128  -0.99814385 -0.9955718 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934128  -0.99814385 -0.9955718 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9934128  -0.99814385 -0.9955718 ]], Number:132\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961864  -0.99425167 -0.99851924]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961864  -0.99425167 -0.99851924]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9961864  -0.99425167 -0.99851924]], Number:133\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933961  -0.9920285  -0.99434835]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933961  -0.9920285  -0.99434835]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9933961  -0.9920285  -0.99434835]], Number:134\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926302  -0.992503   -0.99696434]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926302  -0.992503   -0.99696434]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9926302  -0.992503   -0.99696434]], Number:135\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932199  -0.99332994 -0.9953022 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932199  -0.99332994 -0.9953022 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9932199  -0.99332994 -0.9953022 ]], Number:136\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991427 -0.995046  -0.9927061]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991427 -0.995046  -0.9927061]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991427 -0.995046  -0.9927061]], Number:137\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99755883 -0.9948252  -0.9928227 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99755883 -0.9948252  -0.9928227 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99755883 -0.9948252  -0.9928227 ]], Number:138\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951992 -0.9997022 -0.9949444]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951992 -0.9997022 -0.9949444]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9951992 -0.9997022 -0.9949444]], Number:139\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99795467 -0.9931363  -0.9947044 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99795467 -0.9931363  -0.9947044 ]], Number:140\n",
      "-------------------------\n",
      "Episode 7\tFrame 140 \tAverage100 Score: -165.18tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975409  -0.99283487 -0.9949996 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975409  -0.99283487 -0.9949996 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975409  -0.99283487 -0.9949996 ]], Number:141\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99728686 -0.9933266  -0.99286705]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99728686 -0.9933266  -0.99286705]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99728686 -0.9933266  -0.99286705]], Number:142\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928544  -0.99293387 -0.9947333 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928544  -0.99293387 -0.9947333 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9928544  -0.99293387 -0.9947333 ]], Number:143\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99441016 -0.99516255 -0.99578327]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99441016 -0.99516255 -0.99578327]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99441016 -0.99516255 -0.99578327]], Number:144\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921005 -0.9971755 -0.9993479]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921005 -0.9971755 -0.9993479]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921005 -0.9971755 -0.9993479]], Number:145\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942919  -0.9960153  -0.99563485]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9942919  -0.9960153  -0.99563485]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9942919  -0.9960153  -0.99563485]], Number:146\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995217   -0.998001   -0.99839324]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995217   -0.998001   -0.99839324]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995217   -0.998001   -0.99839324]], Number:147\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994292 -0.9937473 -0.9927323]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994292 -0.9937473 -0.9927323]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994292 -0.9937473 -0.9927323]], Number:148\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944263  -0.99906576 -0.9921719 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944263  -0.99906576 -0.9921719 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9944263  -0.99906576 -0.9921719 ]], Number:149\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933865 -0.9959256 -0.9999928]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933865 -0.9959256 -0.9999928]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9933865 -0.9959256 -0.9999928]], Number:150\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932491  -0.9950131  -0.99502194]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932491  -0.9950131  -0.99502194]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9932491  -0.9950131  -0.99502194]], Number:151\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99985385 -0.9941358  -0.99972564]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99985385 -0.9941358  -0.99972564]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99985385 -0.9941358  -0.99972564]], Number:152\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99617535 -0.9988411  -0.99712133]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99617535 -0.9988411  -0.99712133]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99617535 -0.9988411  -0.99712133]], Number:153\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928787  -0.9940129  -0.99347264]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928787  -0.9940129  -0.99347264]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9928787  -0.9940129  -0.99347264]], Number:154\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997615   -0.99689233 -0.99516153]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997615   -0.99689233 -0.99516153]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.997615   -0.99689233 -0.99516153]], Number:155\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99972767 -0.9967016  -0.9922009 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99972767 -0.9967016  -0.9922009 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99972767 -0.9967016  -0.9922009 ]], Number:156\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99396765 -0.99309814 -0.99760985]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99396765 -0.99309814 -0.99760985]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99396765 -0.99309814 -0.99760985]], Number:157\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944741 -0.9936893 -0.9943635]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944741 -0.9936893 -0.9943635]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9944741 -0.9936893 -0.9943635]], Number:158\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965008 -0.998601  -0.9998564]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965008 -0.998601  -0.9998564]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9965008 -0.998601  -0.9998564]], Number:159\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99290395 -0.9925453  -0.9977924 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99290395 -0.9925453  -0.9977924 ]], Number:160\n",
      "-------------------------\n",
      "Episode 8\tFrame 160 \tAverage100 Score: -169.54tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939537  -0.9967431  -0.99497986]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939537  -0.9967431  -0.99497986]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939537  -0.9967431  -0.99497986]], Number:161\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99326473 -0.9974303  -0.99503905]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99326473 -0.9974303  -0.99503905]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99326473 -0.9974303  -0.99503905]], Number:162\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9979521  -0.99591106 -0.9997593 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9979521  -0.99591106 -0.9997593 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9979521  -0.99591106 -0.9997593 ]], Number:163\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9978914  -0.99770284 -0.99268395]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978914  -0.99770284 -0.99268395]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9978914  -0.99770284 -0.99268395]], Number:164\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99894077 -0.9939338  -0.9993061 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99894077 -0.9939338  -0.9993061 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99894077 -0.9939338  -0.9993061 ]], Number:165\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99210924 -0.99867207 -0.999263  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99210924 -0.99867207 -0.999263  ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99210924 -0.99867207 -0.999263  ]], Number:166\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0.0000]])\n",
      "here: [-0.99830717 -0.9925355  -0.99464995]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99830717 -0.9925355  -0.99464995]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99830717 -0.9925355  -0.99464995]], Number:167\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928756 -0.9960061 -0.9990878]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928756 -0.9960061 -0.9990878]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9928756 -0.9960061 -0.9990878]], Number:168\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997276  -0.996355  -0.9920775]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997276  -0.996355  -0.9920775]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.997276  -0.996355  -0.9920775]], Number:169\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99866825 -0.9980467  -0.993254  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99866825 -0.9980467  -0.993254  ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99866825 -0.9980467  -0.993254  ]], Number:170\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999088   -0.99222887 -0.99762714]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999088   -0.99222887 -0.99762714]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.999088   -0.99222887 -0.99762714]], Number:171\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995462   -0.9947149  -0.99274385]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995462   -0.9947149  -0.99274385]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.995462   -0.9947149  -0.99274385]], Number:172\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993943  -0.9932257  -0.99856585]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993943  -0.9932257  -0.99856585]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993943  -0.9932257  -0.99856585]], Number:173\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99425375 -0.99978226 -0.9938667 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99425375 -0.99978226 -0.9938667 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99425375 -0.99978226 -0.9938667 ]], Number:174\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99855137 -0.99835074 -0.99972117]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99855137 -0.99835074 -0.99972117]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99855137 -0.99835074 -0.99972117]], Number:175\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99740463 -0.9973749  -0.9921366 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99740463 -0.9973749  -0.9921366 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99740463 -0.9973749  -0.9921366 ]], Number:176\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99514556 -0.99708575 -0.99207914]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99514556 -0.99708575 -0.99207914]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99514556 -0.99708575 -0.99207914]], Number:177\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.998625  -0.9982802 -0.9923513]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998625  -0.9982802 -0.9923513]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.998625  -0.9982802 -0.9923513]], Number:178\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923885  -0.99452704 -0.99217093]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923885  -0.99452704 -0.99217093]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9923885  -0.99452704 -0.99217093]], Number:179\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995401  -0.9927715  -0.99436545]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9995401  -0.9927715  -0.99436545]], Number:180\n",
      "-------------------------\n",
      "Episode 9\tFrame 180 \tAverage100 Score: -172.92tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946554 -0.993255  -0.9991661]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946554 -0.993255  -0.9991661]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9946554 -0.993255  -0.9991661]], Number:181\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984034 -0.9988976 -0.9961698]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984034 -0.9988976 -0.9961698]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9984034 -0.9988976 -0.9961698]], Number:182\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99563074 -0.995675   -0.99710244]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99563074 -0.995675   -0.99710244]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99563074 -0.995675   -0.99710244]], Number:183\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940503 -0.9932926 -0.9939319]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940503 -0.9932926 -0.9939319]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9940503 -0.9932926 -0.9939319]], Number:184\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99969697 -0.9987767  -0.998246  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99969697 -0.9987767  -0.998246  ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99969697 -0.9987767  -0.998246  ]], Number:185\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99809384 -0.9954094  -0.99844366]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99809384 -0.9954094  -0.99844366]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99809384 -0.9954094  -0.99844366]], Number:186\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950026 -0.9972474 -0.9970906]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950026 -0.9972474 -0.9970906]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950026 -0.9972474 -0.9970906]], Number:187\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99433535 -0.9924451  -0.9984749 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99433535 -0.9924451  -0.9984749 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99433535 -0.9924451  -0.9984749 ]], Number:188\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99721414 -0.9921475  -0.9982991 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99721414 -0.9921475  -0.9982991 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99721414 -0.9921475  -0.9982991 ]], Number:189\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99978435 -0.9984709  -0.9933967 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99978435 -0.9984709  -0.9933967 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99978435 -0.9984709  -0.9933967 ]], Number:190\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941497  -0.99249256 -0.99602133]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941497  -0.99249256 -0.99602133]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9941497  -0.99249256 -0.99602133]], Number:191\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99966913 -0.99755013 -0.99424636]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99966913 -0.99755013 -0.99424636]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99966913 -0.99755013 -0.99424636]], Number:192\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968574  -0.9988833  -0.99699444]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968574  -0.9988833  -0.99699444]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968574  -0.9988833  -0.99699444]], Number:193\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99629   -0.9971526 -0.9964287]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99629   -0.9971526 -0.9964287]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99629   -0.9971526 -0.9964287]], Number:194\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99890965 -0.9997619  -0.99373037]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99890965 -0.9997619  -0.99373037]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99890965 -0.9997619  -0.99373037]], Number:195\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99414206 -0.99665266 -0.9990808 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99414206 -0.99665266 -0.9990808 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99414206 -0.99665266 -0.9990808 ]], Number:196\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976357  -0.99553365 -0.99284273]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976357  -0.99553365 -0.99284273]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976357  -0.99553365 -0.99284273]], Number:197\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99614686 -0.99221003 -0.9957841 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99614686 -0.99221003 -0.9957841 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99614686 -0.99221003 -0.9957841 ]], Number:198\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985512  -0.99507946 -0.99519926]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985512  -0.99507946 -0.99519926]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9985512  -0.99507946 -0.99519926]], Number:199\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99520624 -0.99420285 -0.999817  ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99520624 -0.99420285 -0.999817  ]], Number:200\n",
      "-------------------------\n",
      "Episode 10\tFrame 200 \tAverage100 Score: -175.63tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99650437 -0.9935975  -0.99889404]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99650437 -0.9935975  -0.99889404]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99650437 -0.9935975  -0.99889404]], Number:201\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996585  -0.9985363  -0.99771553]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996585  -0.9985363  -0.99771553]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996585  -0.9985363  -0.99771553]], Number:202\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99651885 -0.997485   -0.9949248 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99651885 -0.997485   -0.9949248 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99651885 -0.997485   -0.9949248 ]], Number:203\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9986392 -0.9972358 -0.994594 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9986392 -0.9972358 -0.994594 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9986392 -0.9972358 -0.994594 ]], Number:204\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956452  -0.9923303  -0.99251044]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956452  -0.9923303  -0.99251044]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9956452  -0.9923303  -0.99251044]], Number:205\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99793595 -0.9973194  -0.995699  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99793595 -0.9973194  -0.995699  ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99793595 -0.9973194  -0.995699  ]], Number:206\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956474 -0.9970734 -0.9930986]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9956474 -0.9970734 -0.9930986]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9956474 -0.9970734 -0.9930986]], Number:207\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984246 -0.9962306 -0.9990446]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984246 -0.9962306 -0.9990446]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9984246 -0.9962306 -0.9990446]], Number:208\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929353 -0.9952346 -0.992098 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929353 -0.9952346 -0.992098 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929353 -0.9952346 -0.992098 ]], Number:209\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988539  -0.99431115 -0.9952249 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988539  -0.99431115 -0.9952249 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988539  -0.99431115 -0.9952249 ]], Number:210\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995885  -0.9934397  -0.99300647]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995885  -0.9934397  -0.99300647]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9995885  -0.9934397  -0.99300647]], Number:211\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99935573 -0.9924664  -0.9937203 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99935573 -0.9924664  -0.9937203 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99935573 -0.9924664  -0.9937203 ]], Number:212\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99516356 -0.99564946 -0.9978948 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99516356 -0.99564946 -0.9978948 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99516356 -0.99564946 -0.9978948 ]], Number:213\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99954176 -0.9961963  -0.99307245]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99954176 -0.9961963  -0.99307245]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99954176 -0.9961963  -0.99307245]], Number:214\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99832714 -0.9962201  -0.99774003]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99832714 -0.9962201  -0.99774003]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99832714 -0.9962201  -0.99774003]], Number:215\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944724 -0.9923229 -0.9928956]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944724 -0.9923229 -0.9928956]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9944724 -0.9923229 -0.9928956]], Number:216\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99769723 -0.9966642  -0.9999788 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99769723 -0.9966642  -0.9999788 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99769723 -0.9966642  -0.9999788 ]], Number:217\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957651  -0.9931716  -0.99324167]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957651  -0.9931716  -0.99324167]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9957651  -0.9931716  -0.99324167]], Number:218\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994236  -0.996533   -0.99935967]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994236  -0.996533   -0.99935967]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9994236  -0.996533   -0.99935967]], Number:219\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966043 -0.9926972 -0.9977318]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966043 -0.9926972 -0.9977318]], Number:220\n",
      "-------------------------\n",
      "Episode 11\tFrame 220 \tAverage100 Score: -177.84tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932342 -0.9923576 -0.9934529]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932342 -0.9923576 -0.9934529]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932342 -0.9923576 -0.9934529]], Number:221\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9983844 -0.9926542 -0.9926172]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983844 -0.9926542 -0.9926172]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9983844 -0.9926542 -0.9926172]], Number:222\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9920113  -0.99991524 -0.99997115]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920113  -0.99991524 -0.99997115]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9920113  -0.99991524 -0.99997115]], Number:223\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976685 -0.9958586 -0.9966075]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976685 -0.9958586 -0.9966075]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976685 -0.9958586 -0.9966075]], Number:224\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992601  -0.99487597 -0.99465805]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992601  -0.99487597 -0.99465805]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992601  -0.99487597 -0.99465805]], Number:225\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949572 -0.995915  -0.9956057]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949572 -0.995915  -0.9956057]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9949572 -0.995915  -0.9956057]], Number:226\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944817  -0.99948436 -0.99651   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944817  -0.99948436 -0.99651   ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9944817  -0.99948436 -0.99651   ]], Number:227\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99891603 -0.99916786 -0.993904  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99891603 -0.99916786 -0.993904  ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99891603 -0.99916786 -0.993904  ]], Number:228\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974054  -0.99728507 -0.9977833 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974054  -0.99728507 -0.9977833 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974054  -0.99728507 -0.9977833 ]], Number:229\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995302  -0.9990545 -0.9948708]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995302  -0.9990545 -0.9948708]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995302  -0.9990545 -0.9948708]], Number:230\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994368 -0.9923584 -0.9987436]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994368 -0.9923584 -0.9987436]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9994368 -0.9923584 -0.9987436]], Number:231\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938587  -0.995638   -0.99660057]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938587  -0.995638   -0.99660057]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9938587  -0.995638   -0.99660057]], Number:232\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99756616 -0.9971256  -0.993929  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99756616 -0.9971256  -0.993929  ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99756616 -0.9971256  -0.993929  ]], Number:233\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995802   -0.99570894 -0.9980194 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995802   -0.99570894 -0.9980194 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.995802   -0.99570894 -0.9980194 ]], Number:234\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950012  -0.99863523 -0.99614036]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950012  -0.99863523 -0.99614036]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950012  -0.99863523 -0.99614036]], Number:235\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99478847 -0.99564636 -0.9953552 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99478847 -0.99564636 -0.9953552 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99478847 -0.99564636 -0.9953552 ]], Number:236\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954387  -0.99844897 -0.9923386 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954387  -0.99844897 -0.9923386 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954387  -0.99844897 -0.9923386 ]], Number:237\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99452937 -0.99419504 -0.9923597 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99452937 -0.99419504 -0.9923597 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99452937 -0.99419504 -0.9923597 ]], Number:238\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963866 -0.9973383 -0.9975677]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963866 -0.9973383 -0.9975677]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9963866 -0.9973383 -0.9975677]], Number:239\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99746984 -0.99759877 -0.9974712 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99746984 -0.99759877 -0.9974712 ]], Number:240\n",
      "-------------------------\n",
      "Episode 12\tFrame 240 \tAverage100 Score: -179.69tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9983274 -0.9961281 -0.9976635]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983274 -0.9961281 -0.9976635]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9983274 -0.9961281 -0.9976635]], Number:241\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99705225 -0.9946912  -0.9974784 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99705225 -0.9946912  -0.9974784 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99705225 -0.9946912  -0.9974784 ]], Number:242\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993093   -0.99362    -0.99737185]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993093   -0.99362    -0.99737185]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.993093   -0.99362    -0.99737185]], Number:243\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933246  -0.998588   -0.99923337]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933246  -0.998588   -0.99923337]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9933246  -0.998588   -0.99923337]], Number:244\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99204123 -0.99973655 -0.99995506]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99204123 -0.99973655 -0.99995506]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99204123 -0.99973655 -0.99995506]], Number:245\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930632  -0.9999791  -0.99475884]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930632  -0.9999791  -0.99475884]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930632  -0.9999791  -0.99475884]], Number:246\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99555343 -0.99821717 -0.9980359 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99555343 -0.99821717 -0.9980359 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99555343 -0.99821717 -0.9980359 ]], Number:247\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922626  -0.99685234 -0.99806565]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922626  -0.99685234 -0.99806565]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922626  -0.99685234 -0.99806565]], Number:248\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964683  -0.99414265 -0.9976748 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964683  -0.99414265 -0.9976748 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9964683  -0.99414265 -0.9976748 ]], Number:249\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99928766 -0.9936743  -0.999231  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99928766 -0.9936743  -0.999231  ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99928766 -0.9936743  -0.999231  ]], Number:250\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995568  -0.99996233 -0.99205214]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995568  -0.99996233 -0.99205214]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9995568  -0.99996233 -0.99205214]], Number:251\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965266  -0.9941018  -0.99553823]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965266  -0.9941018  -0.99553823]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9965266  -0.9941018  -0.99553823]], Number:252\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922012  -0.9972546  -0.99720454]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922012  -0.9972546  -0.99720454]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922012  -0.9972546  -0.99720454]], Number:253\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99454266 -0.99999547 -0.9970912 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99454266 -0.99999547 -0.9970912 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99454266 -0.99999547 -0.9970912 ]], Number:254\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939255  -0.99541754 -0.9960806 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939255  -0.99541754 -0.9960806 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939255  -0.99541754 -0.9960806 ]], Number:255\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99250525 -0.9965378  -0.994153  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99250525 -0.9965378  -0.994153  ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99250525 -0.9965378  -0.994153  ]], Number:256\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968606  -0.9993086  -0.99689937]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968606  -0.9993086  -0.99689937]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968606  -0.9993086  -0.99689937]], Number:257\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9983142  -0.99978167 -0.9977122 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983142  -0.99978167 -0.9977122 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9983142  -0.99978167 -0.9977122 ]], Number:258\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989904  -0.9963474  -0.99409807]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989904  -0.9963474  -0.99409807]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9989904  -0.9963474  -0.99409807]], Number:259\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99393624 -0.99293226 -0.9974582 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99393624 -0.99293226 -0.9974582 ]], Number:260\n",
      "-------------------------\n",
      "Episode 13\tFrame 260 \tAverage100 Score: -181.25tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99464387 -0.9928868  -0.99683285]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99464387 -0.9928868  -0.99683285]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99464387 -0.9928868  -0.99683285]], Number:261\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99269056 -0.9991767  -0.99263346]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99269056 -0.9991767  -0.99263346]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99269056 -0.9991767  -0.99263346]], Number:262\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99295104 -0.99216    -0.9933537 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99295104 -0.99216    -0.9933537 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99295104 -0.99216    -0.9933537 ]], Number:263\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99431354 -0.9965193  -0.9942604 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99431354 -0.9965193  -0.9942604 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99431354 -0.9965193  -0.9942604 ]], Number:264\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929717 -0.9953933 -0.9950261]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929717 -0.9953933 -0.9950261]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929717 -0.9953933 -0.9950261]], Number:265\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926053 -0.9977227 -0.9973183]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926053 -0.9977227 -0.9973183]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9926053 -0.9977227 -0.9973183]], Number:266\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997157 -0.9929775 -0.9949641]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997157 -0.9929775 -0.9949641]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997157 -0.9929775 -0.9949641]], Number:267\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946179  -0.9980956  -0.99852365]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946179  -0.9980956  -0.99852365]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9946179  -0.9980956  -0.99852365]], Number:268\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99617034 -0.99298364 -0.9971625 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99617034 -0.99298364 -0.9971625 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99617034 -0.99298364 -0.9971625 ]], Number:269\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966244 -0.9966569 -0.9932444]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966244 -0.9966569 -0.9932444]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966244 -0.9966569 -0.9932444]], Number:270\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99900866 -0.9930751  -0.999906  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99900866 -0.9930751  -0.999906  ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99900866 -0.9930751  -0.999906  ]], Number:271\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994407   -0.99389154 -0.9962434 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994407   -0.99389154 -0.9962434 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.994407   -0.99389154 -0.9962434 ]], Number:272\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99406546 -0.9977562  -0.99887127]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99406546 -0.9977562  -0.99887127]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99406546 -0.9977562  -0.99887127]], Number:273\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9971208  -0.9978792  -0.99426454]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9971208  -0.9978792  -0.99426454]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9971208  -0.9978792  -0.99426454]], Number:274\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954867  -0.9947142  -0.99219376]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954867  -0.9947142  -0.99219376]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954867  -0.9947142  -0.99219376]], Number:275\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938141 -0.9936673 -0.9922401]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938141 -0.9936673 -0.9922401]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9938141 -0.9936673 -0.9922401]], Number:276\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99256986 -0.9940671  -0.99721736]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99256986 -0.9940671  -0.99721736]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99256986 -0.9940671  -0.99721736]], Number:277\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959334  -0.99395466 -0.9937064 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959334  -0.99395466 -0.9937064 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9959334  -0.99395466 -0.9937064 ]], Number:278\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992801  -0.9940611 -0.9946492]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992801  -0.9940611 -0.9946492]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.992801  -0.9940611 -0.9946492]], Number:279\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9979653  -0.9950275  -0.99850446]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9979653  -0.9950275  -0.99850446]], Number:280\n",
      "-------------------------\n",
      "Episode 14\tFrame 280 \tAverage100 Score: -182.59tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948242  -0.9985917  -0.99985874]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948242  -0.9985917  -0.99985874]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948242  -0.9985917  -0.99985874]], Number:281\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99235696 -0.9930411  -0.99756134]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99235696 -0.9930411  -0.99756134]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99235696 -0.9930411  -0.99756134]], Number:282\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99964064 -0.9934508  -0.9938294 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99964064 -0.9934508  -0.9938294 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99964064 -0.9934508  -0.9938294 ]], Number:283\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987615 -0.9934632 -0.9932923]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987615 -0.9934632 -0.9932923]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9987615 -0.9934632 -0.9932923]], Number:284\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99540514 -0.99780905 -0.99954844]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99540514 -0.99780905 -0.99954844]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99540514 -0.99780905 -0.99954844]], Number:285\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994927  -0.9964718  -0.99422824]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994927  -0.9964718  -0.99422824]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994927  -0.9964718  -0.99422824]], Number:286\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948941  -0.99820167 -0.9936151 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948941  -0.99820167 -0.9936151 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948941  -0.99820167 -0.9936151 ]], Number:287\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999495   -0.99508774 -0.9986076 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999495   -0.99508774 -0.9986076 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999495   -0.99508774 -0.9986076 ]], Number:288\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99995655 -0.99845123 -0.9936337 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99995655 -0.99845123 -0.9936337 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99995655 -0.99845123 -0.9936337 ]], Number:289\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982418  -0.9998876  -0.99613935]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982418  -0.9998876  -0.99613935]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9982418  -0.9998876  -0.99613935]], Number:290\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972079  -0.99540836 -0.9953817 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972079  -0.99540836 -0.9953817 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9972079  -0.99540836 -0.9953817 ]], Number:291\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99433744 -0.9926764  -0.9972932 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99433744 -0.9926764  -0.9972932 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99433744 -0.9926764  -0.9972932 ]], Number:292\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99492    -0.99905145 -0.9979901 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99492    -0.99905145 -0.9979901 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99492    -0.99905145 -0.9979901 ]], Number:293\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99923116 -0.9931031  -0.9972987 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99923116 -0.9931031  -0.9972987 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99923116 -0.9931031  -0.9972987 ]], Number:294\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943155 -0.9929818 -0.9969228]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943155 -0.9929818 -0.9969228]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9943155 -0.9929818 -0.9969228]], Number:295\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.998345   -0.99801546 -0.9960842 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998345   -0.99801546 -0.9960842 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.998345   -0.99801546 -0.9960842 ]], Number:296\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99690115 -0.99943924 -0.9931962 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99690115 -0.99943924 -0.9931962 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99690115 -0.99943924 -0.9931962 ]], Number:297\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99488497 -0.99302346 -0.9932372 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99488497 -0.99302346 -0.9932372 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99488497 -0.99302346 -0.9932372 ]], Number:298\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941962  -0.9934078  -0.99436927]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941962  -0.9934078  -0.99436927]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9941962  -0.9934078  -0.99436927]], Number:299\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965758 -0.99345   -0.9926569]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9965758 -0.99345   -0.9926569]], Number:300\n",
      "-------------------------\n",
      "Episode 15\tFrame 300 \tAverage100 Score: -183.75tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99913985 -0.9988456  -0.9986413 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99913985 -0.9988456  -0.9986413 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99913985 -0.9988456  -0.9986413 ]], Number:301\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963541 -0.9930852 -0.9957865]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963541 -0.9930852 -0.9957865]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9963541 -0.9930852 -0.9957865]], Number:302\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99315816 -0.99706846 -0.99547553]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99315816 -0.99706846 -0.99547553]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99315816 -0.99706846 -0.99547553]], Number:303\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9980239 -0.9948791 -0.997839 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9980239 -0.9948791 -0.997839 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9980239 -0.9948791 -0.997839 ]], Number:304\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999406  -0.99681383 -0.9954909 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999406  -0.99681383 -0.9954909 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9999406  -0.99681383 -0.9954909 ]], Number:305\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929508  -0.99770904 -0.9925863 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929508  -0.99770904 -0.9925863 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929508  -0.99770904 -0.9925863 ]], Number:306\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99350697 -0.9987398  -0.9937707 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99350697 -0.9987398  -0.9937707 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99350697 -0.9987398  -0.9937707 ]], Number:307\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925166  -0.9923036  -0.99446464]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925166  -0.9923036  -0.99446464]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9925166  -0.9923036  -0.99446464]], Number:308\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968769  -0.99564976 -0.99479383]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968769  -0.99564976 -0.99479383]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968769  -0.99564976 -0.99479383]], Number:309\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950953  -0.99848723 -0.992923  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950953  -0.99848723 -0.992923  ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950953  -0.99848723 -0.992923  ]], Number:310\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956741  -0.9994296  -0.99755013]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9956741  -0.9994296  -0.99755013]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9956741  -0.9994296  -0.99755013]], Number:311\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99935514 -0.9963133  -0.9992481 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99935514 -0.9963133  -0.9992481 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99935514 -0.9963133  -0.9992481 ]], Number:312\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957624  -0.99777067 -0.9933826 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957624  -0.99777067 -0.9933826 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9957624  -0.99777067 -0.9933826 ]], Number:313\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99384975 -0.99849904 -0.9935081 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99384975 -0.99849904 -0.9935081 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99384975 -0.99849904 -0.9935081 ]], Number:314\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99534345 -0.9978412  -0.9927106 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99534345 -0.9978412  -0.9927106 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99534345 -0.9978412  -0.9927106 ]], Number:315\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999494  -0.99680823 -0.99521023]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999494  -0.99680823 -0.99521023]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9999494  -0.99680823 -0.99521023]], Number:316\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9986989 -0.9960685 -0.992926 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9986989 -0.9960685 -0.992926 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9986989 -0.9960685 -0.992926 ]], Number:317\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99725145 -0.99433076 -0.9984772 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99725145 -0.99433076 -0.9984772 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99725145 -0.99433076 -0.9984772 ]], Number:318\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992196   -0.99228746 -0.99868053]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992196   -0.99228746 -0.99868053]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.992196   -0.99228746 -0.99868053]], Number:319\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938834 -0.9973296 -0.9997449]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938834 -0.9973296 -0.9997449]], Number:320\n",
      "-------------------------\n",
      "Episode 16\tFrame 320 \tAverage100 Score: -184.77tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9983126  -0.99416953 -0.9962784 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983126  -0.99416953 -0.9962784 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9983126  -0.99416953 -0.9962784 ]], Number:321\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99576354 -0.9989768  -0.9986264 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99576354 -0.9989768  -0.9986264 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99576354 -0.9989768  -0.9986264 ]], Number:322\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970784  -0.99589044 -0.9934366 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970784  -0.99589044 -0.9934366 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9970784  -0.99589044 -0.9934366 ]], Number:323\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99640596 -0.9973766  -0.99269104]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99640596 -0.9973766  -0.99269104]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99640596 -0.9973766  -0.99269104]], Number:324\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99447733 -0.99395263 -0.9935554 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99447733 -0.99395263 -0.9935554 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99447733 -0.99395263 -0.9935554 ]], Number:325\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961231 -0.9979189 -0.999104 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961231 -0.9979189 -0.999104 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9961231 -0.9979189 -0.999104 ]], Number:326\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947404  -0.99401176 -0.9927823 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947404  -0.99401176 -0.9927823 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947404  -0.99401176 -0.9927823 ]], Number:327\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99264544 -0.995971   -0.99608886]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99264544 -0.995971   -0.99608886]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99264544 -0.995971   -0.99608886]], Number:328\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99228084 -0.99931383 -0.9970019 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99228084 -0.99931383 -0.9970019 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99228084 -0.99931383 -0.9970019 ]], Number:329\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99563617 -0.9951869  -0.99511147]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99563617 -0.9951869  -0.99511147]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99563617 -0.9951869  -0.99511147]], Number:330\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987331  -0.99593425 -0.99859273]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987331  -0.99593425 -0.99859273]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9987331  -0.99593425 -0.99859273]], Number:331\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969286  -0.99856776 -0.9974419 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969286  -0.99856776 -0.9974419 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9969286  -0.99856776 -0.9974419 ]], Number:332\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99498475 -0.99680907 -0.9986564 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99498475 -0.99680907 -0.9986564 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99498475 -0.99680907 -0.9986564 ]], Number:333\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99624974 -0.99464047 -0.9975899 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99624974 -0.99464047 -0.9975899 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99624974 -0.99464047 -0.9975899 ]], Number:334\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9978241 -0.9965107 -0.9944348]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978241 -0.9965107 -0.9944348]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9978241 -0.9965107 -0.9944348]], Number:335\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99970895 -0.99461645 -0.99956006]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99970895 -0.99461645 -0.99956006]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99970895 -0.99461645 -0.99956006]], Number:336\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99288094 -0.9984706  -0.9934052 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99288094 -0.9984706  -0.9934052 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99288094 -0.9984706  -0.9934052 ]], Number:337\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997284  -0.9960535  -0.99794513]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997284  -0.9960535  -0.99794513]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9997284  -0.9960535  -0.99794513]], Number:338\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99379194 -0.9938319  -0.998986  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99379194 -0.9938319  -0.998986  ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99379194 -0.9938319  -0.998986  ]], Number:339\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995877  -0.99415755 -0.99664795]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9995877  -0.99415755 -0.99664795]], Number:340\n",
      "-------------------------\n",
      "Episode 17\tFrame 340 \tAverage100 Score: -185.66tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970179 -0.9972126 -0.9970653]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970179 -0.9972126 -0.9970653]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9970179 -0.9972126 -0.9970653]], Number:341\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953876 -0.9968943 -0.9989691]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953876 -0.9968943 -0.9989691]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9953876 -0.9968943 -0.9989691]], Number:342\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962284 -0.9920921 -0.9938665]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962284 -0.9920921 -0.9938665]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9962284 -0.9920921 -0.9938665]], Number:343\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99425334 -0.99257904 -0.996443  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99425334 -0.99257904 -0.996443  ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99425334 -0.99257904 -0.996443  ]], Number:344\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9971017 -0.9936573 -0.9991779]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9971017 -0.9936573 -0.9991779]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9971017 -0.9936573 -0.9991779]], Number:345\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938676 -0.9988152 -0.9950135]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938676 -0.9988152 -0.9950135]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938676 -0.9988152 -0.9950135]], Number:346\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99588925 -0.99382514 -0.9994972 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99588925 -0.99382514 -0.9994972 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99588925 -0.99382514 -0.9994972 ]], Number:347\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946276  -0.99338365 -0.99968946]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946276  -0.99338365 -0.99968946]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9946276  -0.99338365 -0.99968946]], Number:348\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945311 -0.9977602 -0.9940454]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945311 -0.9977602 -0.9940454]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945311 -0.9977602 -0.9940454]], Number:349\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921706  -0.99997824 -0.99674577]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921706  -0.99997824 -0.99674577]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921706  -0.99997824 -0.99674577]], Number:350\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here: [-0.9960053  -0.99215007 -0.99260694]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9960053  -0.99215007 -0.99260694]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9960053  -0.99215007 -0.99260694]], Number:351\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99977    -0.9989084  -0.99673885]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99977    -0.9989084  -0.99673885]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99977    -0.9989084  -0.99673885]], Number:352\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935676  -0.9936825  -0.99555427]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9935676  -0.9936825  -0.99555427]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9935676  -0.9936825  -0.99555427]], Number:353\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992629   -0.9934379  -0.99297726]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992629   -0.9934379  -0.99297726]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.992629   -0.9934379  -0.99297726]], Number:354\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949254  -0.9979258  -0.99412465]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949254  -0.9979258  -0.99412465]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9949254  -0.9979258  -0.99412465]], Number:355\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994269   -0.99257725 -0.9986317 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994269   -0.99257725 -0.9986317 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.994269   -0.99257725 -0.9986317 ]], Number:356\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99982285 -0.99727064 -0.9979531 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99982285 -0.99727064 -0.9979531 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99982285 -0.99727064 -0.9979531 ]], Number:357\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969035  -0.9995223  -0.99511564]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969035  -0.9995223  -0.99511564]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9969035  -0.9995223  -0.99511564]], Number:358\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965812 -0.9978792 -0.9991879]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965812 -0.9978792 -0.9991879]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9965812 -0.9978792 -0.9991879]], Number:359\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99540716 -0.99920505 -0.99311656]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99540716 -0.99920505 -0.99311656]], Number:360\n",
      "-------------------------\n",
      "Episode 18\tFrame 360 \tAverage100 Score: -186.46tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977847  -0.99281627 -0.99900883]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977847  -0.99281627 -0.99900883]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9977847  -0.99281627 -0.99900883]], Number:361\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968835  -0.99377286 -0.9958278 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968835  -0.99377286 -0.9958278 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968835  -0.99377286 -0.9958278 ]], Number:362\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964177  -0.99295205 -0.9986877 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964177  -0.99295205 -0.9986877 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9964177  -0.99295205 -0.9986877 ]], Number:363\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99319047 -0.9958822  -0.99771535]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99319047 -0.9958822  -0.99771535]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99319047 -0.9958822  -0.99771535]], Number:364\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99720055 -0.9977722  -0.9949397 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99720055 -0.9977722  -0.9949397 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99720055 -0.9977722  -0.9949397 ]], Number:365\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99647397 -0.99585533 -0.9948061 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99647397 -0.99585533 -0.9948061 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99647397 -0.99585533 -0.9948061 ]], Number:366\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99312377 -0.99441355 -0.9976341 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99312377 -0.99441355 -0.9976341 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99312377 -0.99441355 -0.9976341 ]], Number:367\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946746  -0.99446195 -0.99273485]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946746  -0.99446195 -0.99273485]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9946746  -0.99446195 -0.99273485]], Number:368\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99691045 -0.9957426  -0.99394745]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99691045 -0.9957426  -0.99394745]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99691045 -0.9957426  -0.99394745]], Number:369\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973233 -0.9973907 -0.9950323]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973233 -0.9973907 -0.9950323]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9973233 -0.9973907 -0.9950323]], Number:370\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970115 -0.9932213 -0.9972226]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970115 -0.9932213 -0.9972226]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9970115 -0.9932213 -0.9972226]], Number:371\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982382  -0.99512124 -0.9952129 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982382  -0.99512124 -0.9952129 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9982382  -0.99512124 -0.9952129 ]], Number:372\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99207705 -0.9924607  -0.99662817]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99207705 -0.9924607  -0.99662817]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99207705 -0.9924607  -0.99662817]], Number:373\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964111 -0.9947857 -0.9959624]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964111 -0.9947857 -0.9959624]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9964111 -0.9947857 -0.9959624]], Number:374\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9952991 -0.9967896 -0.9931201]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952991 -0.9967896 -0.9931201]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9952991 -0.9967896 -0.9931201]], Number:375\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928625  -0.99367625 -0.9940064 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928625  -0.99367625 -0.9940064 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9928625  -0.99367625 -0.9940064 ]], Number:376\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928973  -0.9979539  -0.99246573]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928973  -0.9979539  -0.99246573]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9928973  -0.9979539  -0.99246573]], Number:377\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99838597 -0.99919033 -0.9960407 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99838597 -0.99919033 -0.9960407 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99838597 -0.99919033 -0.9960407 ]], Number:378\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987354 -0.9979522 -0.9951536]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987354 -0.9979522 -0.9951536]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9987354 -0.9979522 -0.9951536]], Number:379\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99881124 -0.9968178  -0.9970672 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99881124 -0.9968178  -0.9970672 ]], Number:380\n",
      "-------------------------\n",
      "Episode 19\tFrame 380 \tAverage100 Score: -187.17tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924539  -0.99937195 -0.9959214 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924539  -0.99937195 -0.9959214 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9924539  -0.99937195 -0.9959214 ]], Number:381\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962553  -0.9987035  -0.99722767]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962553  -0.9987035  -0.99722767]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962553  -0.9987035  -0.99722767]], Number:382\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970935 -0.997135  -0.9955443]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970935 -0.997135  -0.9955443]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9970935 -0.997135  -0.9955443]], Number:383\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955482  -0.99627095 -0.9964768 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955482  -0.99627095 -0.9964768 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955482  -0.99627095 -0.9964768 ]], Number:384\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951259  -0.99612194 -0.9989963 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951259  -0.99612194 -0.9989963 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9951259  -0.99612194 -0.9989963 ]], Number:385\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947506  -0.9967283  -0.99317753]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947506  -0.9967283  -0.99317753]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947506  -0.9967283  -0.99317753]], Number:386\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989426  -0.9956332  -0.99760264]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989426  -0.9956332  -0.99760264]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9989426  -0.9956332  -0.99760264]], Number:387\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949397  -0.99345607 -0.994804  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949397  -0.99345607 -0.994804  ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9949397  -0.99345607 -0.994804  ]], Number:388\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954262  -0.99254805 -0.9981907 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954262  -0.99254805 -0.9981907 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954262  -0.99254805 -0.9981907 ]], Number:389\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992035  -0.9964888 -0.9949334]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992035  -0.9964888 -0.9949334]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.992035  -0.9964888 -0.9949334]], Number:390\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99896157 -0.99975914 -0.9987269 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99896157 -0.99975914 -0.9987269 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99896157 -0.99975914 -0.9987269 ]], Number:391\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956543 -0.9980781 -0.9936429]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956543 -0.9980781 -0.9936429]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9956543 -0.9980781 -0.9936429]], Number:392\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99876106 -0.9946968  -0.99279916]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99876106 -0.9946968  -0.99279916]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99876106 -0.9946968  -0.99279916]], Number:393\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99445665 -0.99261767 -0.99340975]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99445665 -0.99261767 -0.99340975]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99445665 -0.99261767 -0.99340975]], Number:394\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993138   -0.99968153 -0.99591607]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993138   -0.99968153 -0.99591607]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993138   -0.99968153 -0.99591607]], Number:395\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997607   -0.9964906  -0.99417174]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997607   -0.9964906  -0.99417174]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.997607   -0.9964906  -0.99417174]], Number:396\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941517 -0.9983669 -0.9986324]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941517 -0.9983669 -0.9986324]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941517 -0.9983669 -0.9986324]], Number:397\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994404  -0.99408305 -0.99341875]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994404  -0.99408305 -0.99341875]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9994404  -0.99408305 -0.99341875]], Number:398\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99926335 -0.9989072  -0.99889815]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99926335 -0.9989072  -0.99889815]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99926335 -0.9989072  -0.99889815]], Number:399\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920781 -0.9994291 -0.9953939]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9920781 -0.9994291 -0.9953939]], Number:400\n",
      "-------------------------\n",
      "Episode 20\tFrame 400 \tAverage100 Score: -187.81tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929896  -0.99863553 -0.9945366 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929896  -0.99863553 -0.9945366 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929896  -0.99863553 -0.9945366 ]], Number:401\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972856  -0.99486977 -0.9962535 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972856  -0.99486977 -0.9962535 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9972856  -0.99486977 -0.9962535 ]], Number:402\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99880666 -0.99282223 -0.9959979 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99880666 -0.99282223 -0.9959979 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99880666 -0.99282223 -0.9959979 ]], Number:403\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99214363 -0.9952378  -0.992324  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99214363 -0.9952378  -0.992324  ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99214363 -0.9952378  -0.992324  ]], Number:404\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947627 -0.9995387 -0.9999632]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947627 -0.9995387 -0.9999632]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947627 -0.9995387 -0.9999632]], Number:405\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99206305 -0.99467164 -0.9936687 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99206305 -0.99467164 -0.9936687 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99206305 -0.99467164 -0.9936687 ]], Number:406\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932944 -0.9971126 -0.994358 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932944 -0.9971126 -0.994358 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932944 -0.9971126 -0.994358 ]], Number:407\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99482113 -0.99951845 -0.99568886]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99482113 -0.99951845 -0.99568886]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99482113 -0.99951845 -0.99568886]], Number:408\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925611  -0.9955491  -0.99370545]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925611  -0.9955491  -0.99370545]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9925611  -0.9955491  -0.99370545]], Number:409\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994635  -0.99667746 -0.9959533 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994635  -0.99667746 -0.9959533 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994635  -0.99667746 -0.9959533 ]], Number:410\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99555856 -0.99323136 -0.99607736]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99555856 -0.99323136 -0.99607736]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99555856 -0.99323136 -0.99607736]], Number:411\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99546957 -0.9958534  -0.9946057 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99546957 -0.9958534  -0.9946057 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99546957 -0.9958534  -0.9946057 ]], Number:412\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953624  -0.99683625 -0.9977152 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953624  -0.99683625 -0.9977152 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9953624  -0.99683625 -0.9977152 ]], Number:413\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930898  -0.9971647  -0.99429625]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930898  -0.9971647  -0.99429625]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9930898  -0.9971647  -0.99429625]], Number:414\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997828   -0.99210024 -0.99827033]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997828   -0.99210024 -0.99827033]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.997828   -0.99210024 -0.99827033]], Number:415\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99664015 -0.99705    -0.99297976]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99664015 -0.99705    -0.99297976]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99664015 -0.99705    -0.99297976]], Number:416\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941462 -0.9979009 -0.9980517]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941462 -0.9979009 -0.9980517]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941462 -0.9979009 -0.9980517]], Number:417\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99405366 -0.9964276  -0.99855924]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99405366 -0.9964276  -0.99855924]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99405366 -0.9964276  -0.99855924]], Number:418\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990689 -0.9942315 -0.9947114]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990689 -0.9942315 -0.9947114]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9990689 -0.9942315 -0.9947114]], Number:419\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9935499 -0.9993846 -0.9961574]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9935499 -0.9993846 -0.9961574]], Number:420\n",
      "-------------------------\n",
      "Episode 21\tFrame 420 \tAverage100 Score: -188.39tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988424  -0.99543875 -0.9981632 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988424  -0.99543875 -0.9981632 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988424  -0.99543875 -0.9981632 ]], Number:421\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9981597  -0.99551266 -0.99868685]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9981597  -0.99551266 -0.99868685]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9981597  -0.99551266 -0.99868685]], Number:422\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99364567 -0.99866664 -0.99786633]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99364567 -0.99866664 -0.99786633]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99364567 -0.99866664 -0.99786633]], Number:423\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939599  -0.9978898  -0.99761194]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939599  -0.9978898  -0.99761194]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939599  -0.9978898  -0.99761194]], Number:424\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99373245 -0.9954962  -0.9935181 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99373245 -0.9954962  -0.9935181 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99373245 -0.9954962  -0.9935181 ]], Number:425\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930502 -0.9922947 -0.9993084]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930502 -0.9922947 -0.9993084]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930502 -0.9922947 -0.9993084]], Number:426\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968573  -0.99636585 -0.9977075 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968573  -0.99636585 -0.9977075 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968573  -0.99636585 -0.9977075 ]], Number:427\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99306107 -0.9968458  -0.99257815]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99306107 -0.9968458  -0.99257815]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99306107 -0.9968458  -0.99257815]], Number:428\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994369 -0.9964463 -0.9949738]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994369 -0.9964463 -0.9949738]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994369 -0.9964463 -0.9949738]], Number:429\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99370885 -0.99613744 -0.9928048 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99370885 -0.99613744 -0.9928048 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99370885 -0.99613744 -0.9928048 ]], Number:430\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99762374 -0.99715495 -0.9984595 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99762374 -0.99715495 -0.9984595 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99762374 -0.99715495 -0.9984595 ]], Number:431\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99848604 -0.996111   -0.99414974]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99848604 -0.996111   -0.99414974]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99848604 -0.996111   -0.99414974]], Number:432\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99845994 -0.996095   -0.9965357 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99845994 -0.996095   -0.9965357 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99845994 -0.996095   -0.9965357 ]], Number:433\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99213916 -0.9960583  -0.9935274 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99213916 -0.9960583  -0.9935274 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99213916 -0.9960583  -0.9935274 ]], Number:434\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9981242 -0.9928409 -0.999811 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9981242 -0.9928409 -0.999811 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9981242 -0.9928409 -0.999811 ]], Number:435\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987872  -0.9949235  -0.99575704]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987872  -0.9949235  -0.99575704]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9987872  -0.9949235  -0.99575704]], Number:436\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957833  -0.99502057 -0.9972171 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957833  -0.99502057 -0.9972171 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9957833  -0.99502057 -0.9972171 ]], Number:437\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99470377 -0.996826   -0.99531883]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99470377 -0.996826   -0.99531883]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99470377 -0.996826   -0.99531883]], Number:438\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99428654 -0.99752337 -0.9939106 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99428654 -0.99752337 -0.9939106 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99428654 -0.99752337 -0.9939106 ]], Number:439\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993401   -0.99208933 -0.99949974]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993401   -0.99208933 -0.99949974]], Number:440\n",
      "-------------------------\n",
      "Episode 22\tFrame 440 \tAverage100 Score: -188.92tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965905 -0.9986879 -0.9986556]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965905 -0.9986879 -0.9986556]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9965905 -0.9986879 -0.9986556]], Number:441\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962193  -0.99377644 -0.9989974 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962193  -0.99377644 -0.9989974 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962193  -0.99377644 -0.9989974 ]], Number:442\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936393  -0.99324894 -0.9922601 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936393  -0.99324894 -0.9922601 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9936393  -0.99324894 -0.9922601 ]], Number:443\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939464  -0.99832374 -0.9942597 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939464  -0.99832374 -0.9942597 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939464  -0.99832374 -0.9942597 ]], Number:444\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99540657 -0.9978143  -0.99367005]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99540657 -0.9978143  -0.99367005]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99540657 -0.9978143  -0.99367005]], Number:445\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973474  -0.99327856 -0.9929562 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973474  -0.99327856 -0.9929562 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9973474  -0.99327856 -0.9929562 ]], Number:446\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99682564 -0.9989194  -0.9934735 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99682564 -0.9989194  -0.9934735 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99682564 -0.9989194  -0.9934735 ]], Number:447\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987428 -0.9990382 -0.992987 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987428 -0.9990382 -0.992987 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9987428 -0.9990382 -0.992987 ]], Number:448\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99810433 -0.99791515 -0.99686456]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99810433 -0.99791515 -0.99686456]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99810433 -0.99791515 -0.99686456]], Number:449\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994216  -0.9984416 -0.9974708]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994216  -0.9984416 -0.9974708]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.994216  -0.9984416 -0.9974708]], Number:450\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99946487 -0.99794656 -0.99402803]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99946487 -0.99794656 -0.99402803]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99946487 -0.99794656 -0.99402803]], Number:451\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948881  -0.9952023  -0.99281394]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948881  -0.9952023  -0.99281394]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9948881  -0.9952023  -0.99281394]], Number:452\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997543 -0.9977492 -0.9949211]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997543 -0.9977492 -0.9949211]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997543 -0.9977492 -0.9949211]], Number:453\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9971243  -0.99218595 -0.99821275]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9971243  -0.99218595 -0.99821275]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9971243  -0.99218595 -0.99821275]], Number:454\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923216  -0.9941399  -0.99458444]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923216  -0.9941399  -0.99458444]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923216  -0.9941399  -0.99458444]], Number:455\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99246734 -0.9965306  -0.9973028 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99246734 -0.9965306  -0.9973028 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99246734 -0.9965306  -0.9973028 ]], Number:456\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99650973 -0.99411726 -0.99343014]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99650973 -0.99411726 -0.99343014]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99650973 -0.99411726 -0.99343014]], Number:457\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949527  -0.99366474 -0.9946739 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949527  -0.99366474 -0.9946739 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9949527  -0.99366474 -0.9946739 ]], Number:458\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9937789  -0.99897456 -0.99336016]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937789  -0.99897456 -0.99336016]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9937789  -0.99897456 -0.99336016]], Number:459\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99573195 -0.99738014 -0.998965  ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99573195 -0.99738014 -0.998965  ]], Number:460\n",
      "-------------------------\n",
      "Episode 23\tFrame 460 \tAverage100 Score: -189.40tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951484  -0.9982422  -0.99710727]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951484  -0.9982422  -0.99710727]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9951484  -0.9982422  -0.99710727]], Number:461\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956307  -0.9983272  -0.99814445]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956307  -0.9983272  -0.99814445]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9956307  -0.9983272  -0.99814445]], Number:462\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993905  -0.99721074 -0.9933803 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993905  -0.99721074 -0.9933803 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9993905  -0.99721074 -0.9933803 ]], Number:463\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9980457  -0.99786395 -0.99892175]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9980457  -0.99786395 -0.99892175]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9980457  -0.99786395 -0.99892175]], Number:464\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951462 -0.9940719 -0.9985011]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951462 -0.9940719 -0.9985011]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9951462 -0.9940719 -0.9985011]], Number:465\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934341  -0.99991715 -0.999064  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934341  -0.99991715 -0.999064  ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934341  -0.99991715 -0.999064  ]], Number:466\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99924767 -0.993706   -0.999338  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99924767 -0.993706   -0.999338  ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99924767 -0.993706   -0.999338  ]], Number:467\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941537  -0.99832886 -0.99761313]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941537  -0.99832886 -0.99761313]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941537  -0.99832886 -0.99761313]], Number:468\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992022  -0.9935114  -0.99703383]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992022  -0.9935114  -0.99703383]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992022  -0.9935114  -0.99703383]], Number:469\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925603 -0.9944495 -0.9954975]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925603 -0.9944495 -0.9954975]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9925603 -0.9944495 -0.9954975]], Number:470\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963131  -0.995499   -0.99535996]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963131  -0.995499   -0.99535996]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9963131  -0.995499   -0.99535996]], Number:471\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99224436 -0.9982395  -0.99857205]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99224436 -0.9982395  -0.99857205]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99224436 -0.9982395  -0.99857205]], Number:472\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9920125  -0.9944961  -0.99494714]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920125  -0.9944961  -0.99494714]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9920125  -0.9944961  -0.99494714]], Number:473\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99624753 -0.999895   -0.998214  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99624753 -0.999895   -0.998214  ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99624753 -0.999895   -0.998214  ]], Number:474\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99358225 -0.9978296  -0.99516207]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99358225 -0.9978296  -0.99516207]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99358225 -0.9978296  -0.99516207]], Number:475\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9960936 -0.9986203 -0.9995509]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9960936 -0.9986203 -0.9995509]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9960936 -0.9986203 -0.9995509]], Number:476\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99947464 -0.9942966  -0.99809533]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99947464 -0.9942966  -0.99809533]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99947464 -0.9942966  -0.99809533]], Number:477\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959119 -0.9953811 -0.9939178]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959119 -0.9953811 -0.9939178]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9959119 -0.9953811 -0.9939178]], Number:478\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99872166 -0.9965846  -0.99395597]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99872166 -0.9965846  -0.99395597]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99872166 -0.9965846  -0.99395597]], Number:479\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99468195 -0.99620205 -0.99487853]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99468195 -0.99620205 -0.99487853]], Number:480\n",
      "-------------------------\n",
      "Episode 24\tFrame 480 \tAverage100 Score: -189.85tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99370563 -0.9929858  -0.9957614 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99370563 -0.9929858  -0.9957614 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99370563 -0.9929858  -0.9957614 ]], Number:481\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99919116 -0.99896276 -0.994674  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99919116 -0.99896276 -0.994674  ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99919116 -0.99896276 -0.994674  ]], Number:482\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945603 -0.9964375 -0.9944237]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945603 -0.9964375 -0.9944237]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9945603 -0.9964375 -0.9944237]], Number:483\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982591  -0.99684846 -0.9969553 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982591  -0.99684846 -0.9969553 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9982591  -0.99684846 -0.9969553 ]], Number:484\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940049  -0.99227375 -0.9977997 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940049  -0.99227375 -0.9977997 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9940049  -0.99227375 -0.9977997 ]], Number:485\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999204   -0.9970455  -0.99307126]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999204   -0.9970455  -0.99307126]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999204   -0.9970455  -0.99307126]], Number:486\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9920415 -0.9995404 -0.9973102]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920415 -0.9995404 -0.9973102]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9920415 -0.9995404 -0.9973102]], Number:487\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99619025 -0.9928449  -0.99544996]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99619025 -0.9928449  -0.99544996]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99619025 -0.9928449  -0.99544996]], Number:488\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965759  -0.99221116 -0.9941305 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965759  -0.99221116 -0.9941305 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9965759  -0.99221116 -0.9941305 ]], Number:489\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99281853 -0.9935203  -0.9921371 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99281853 -0.9935203  -0.9921371 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99281853 -0.9935203  -0.9921371 ]], Number:490\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945309  -0.99375474 -0.9959679 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945309  -0.99375474 -0.9959679 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9945309  -0.99375474 -0.9959679 ]], Number:491\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945726 -0.9924987 -0.9924165]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945726 -0.9924987 -0.9924165]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9945726 -0.9924987 -0.9924165]], Number:492\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927965 -0.9945942 -0.9960609]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927965 -0.9945942 -0.9960609]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927965 -0.9945942 -0.9960609]], Number:493\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.996661   -0.9968685  -0.99238163]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.996661   -0.9968685  -0.99238163]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.996661   -0.9968685  -0.99238163]], Number:494\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99467   -0.9936099 -0.9946663]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99467   -0.9936099 -0.9946663]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99467   -0.9936099 -0.9946663]], Number:495\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991944  -0.99641573 -0.9972005 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991944  -0.99641573 -0.9972005 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9991944  -0.99641573 -0.9972005 ]], Number:496\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99225277 -0.9944977  -0.9970722 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99225277 -0.9944977  -0.9970722 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99225277 -0.9944977  -0.9970722 ]], Number:497\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959005  -0.9928585  -0.99577546]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959005  -0.9928585  -0.99577546]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9959005  -0.9928585  -0.99577546]], Number:498\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99434876 -0.99710274 -0.99658823]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99434876 -0.99710274 -0.99658823]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99434876 -0.99710274 -0.99658823]], Number:499\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938681 -0.9984075 -0.994594 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938681 -0.9984075 -0.994594 ]], Number:500\n",
      "-------------------------\n",
      "Episode 25\tFrame 500 \tAverage100 Score: -190.25tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99327093 -0.99203855 -0.99577016]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99327093 -0.99203855 -0.99577016]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99327093 -0.99203855 -0.99577016]], Number:501\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950886  -0.9920483  -0.99365234]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950886  -0.9920483  -0.99365234]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950886  -0.9920483  -0.99365234]], Number:502\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930469 -0.9957942 -0.998941 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930469 -0.9957942 -0.998941 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9930469 -0.9957942 -0.998941 ]], Number:503\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933836  -0.9992442  -0.99429774]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933836  -0.9992442  -0.99429774]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9933836  -0.9992442  -0.99429774]], Number:504\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976674  -0.99353456 -0.99486876]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976674  -0.99353456 -0.99486876]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976674  -0.99353456 -0.99486876]], Number:505\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959128  -0.996164   -0.99761057]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959128  -0.996164   -0.99761057]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9959128  -0.996164   -0.99761057]], Number:506\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99200696 -0.9955494  -0.9923231 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99200696 -0.9955494  -0.9923231 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99200696 -0.9955494  -0.9923231 ]], Number:507\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99904376 -0.99980235 -0.9985984 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99904376 -0.99980235 -0.9985984 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99904376 -0.99980235 -0.9985984 ]], Number:508\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974433  -0.99211633 -0.9989747 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974433  -0.99211633 -0.9989747 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974433  -0.99211633 -0.9989747 ]], Number:509\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here: [-0.9958285 -0.9931502 -0.9975649]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9958285 -0.9931502 -0.9975649]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9958285 -0.9931502 -0.9975649]], Number:510\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976704  -0.99830014 -0.9995659 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976704  -0.99830014 -0.9995659 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9976704  -0.99830014 -0.9995659 ]], Number:511\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953436  -0.99550515 -0.9982715 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953436  -0.99550515 -0.9982715 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9953436  -0.99550515 -0.9982715 ]], Number:512\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995511   -0.99598527 -0.9944419 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995511   -0.99598527 -0.9944419 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995511   -0.99598527 -0.9944419 ]], Number:513\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9980493 -0.9981821 -0.9929895]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9980493 -0.9981821 -0.9929895]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9980493 -0.9981821 -0.9929895]], Number:514\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945047 -0.993939  -0.9972518]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945047 -0.993939  -0.9972518]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945047 -0.993939  -0.9972518]], Number:515\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953355  -0.9969652  -0.99455845]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953355  -0.9969652  -0.99455845]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9953355  -0.9969652  -0.99455845]], Number:516\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923515 -0.9960987 -0.9963541]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923515 -0.9960987 -0.9963541]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923515 -0.9960987 -0.9963541]], Number:517\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943908  -0.99603146 -0.99706066]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943908  -0.99603146 -0.99706066]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9943908  -0.99603146 -0.99706066]], Number:518\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998217  -0.99344873 -0.9959237 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9998217  -0.99344873 -0.9959237 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9998217  -0.99344873 -0.9959237 ]], Number:519\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932181  -0.9972502  -0.99292696]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932181  -0.9972502  -0.99292696]], Number:520\n",
      "-------------------------\n",
      "Episode 26\tFrame 520 \tAverage100 Score: -190.63tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99354243 -0.9981327  -0.9937723 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99354243 -0.9981327  -0.9937723 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99354243 -0.9981327  -0.9937723 ]], Number:521\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996616  -0.99484295 -0.99375916]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996616  -0.99484295 -0.99375916]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996616  -0.99484295 -0.99375916]], Number:522\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99422926 -0.99767315 -0.9971846 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99422926 -0.99767315 -0.9971846 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99422926 -0.99767315 -0.9971846 ]], Number:523\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985444  -0.9928235  -0.99526864]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985444  -0.9928235  -0.99526864]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985444  -0.9928235  -0.99526864]], Number:524\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993747  -0.9960791 -0.9965394]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993747  -0.9960791 -0.9965394]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993747  -0.9960791 -0.9965394]], Number:525\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924631 -0.9925137 -0.994064 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924631 -0.9925137 -0.994064 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9924631 -0.9925137 -0.994064 ]], Number:526\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947966  -0.99288005 -0.9974154 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947966  -0.99288005 -0.9974154 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947966  -0.99288005 -0.9974154 ]], Number:527\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99744195 -0.9937016  -0.9946031 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99744195 -0.9937016  -0.9946031 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99744195 -0.9937016  -0.9946031 ]], Number:528\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938821  -0.99428207 -0.99678934]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938821  -0.99428207 -0.99678934]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938821  -0.99428207 -0.99678934]], Number:529\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990638 -0.9938138 -0.9930388]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990638 -0.9938138 -0.9930388]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9990638 -0.9938138 -0.9930388]], Number:530\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99574995 -0.99745464 -0.9932338 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99574995 -0.99745464 -0.9932338 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99574995 -0.99745464 -0.9932338 ]], Number:531\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984902 -0.9984549 -0.9953837]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984902 -0.9984549 -0.9953837]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9984902 -0.9984549 -0.9953837]], Number:532\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961659 -0.9939219 -0.9998795]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961659 -0.9939219 -0.9998795]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9961659 -0.9939219 -0.9998795]], Number:533\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99254394 -0.9977556  -0.9960169 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99254394 -0.9977556  -0.9960169 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99254394 -0.9977556  -0.9960169 ]], Number:534\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997887 -0.9969993 -0.9959499]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997887 -0.9969993 -0.9959499]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997887 -0.9969993 -0.9959499]], Number:535\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924307 -0.9984687 -0.9989769]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924307 -0.9984687 -0.9989769]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9924307 -0.9984687 -0.9989769]], Number:536\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930767  -0.9926301  -0.99602115]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930767  -0.9926301  -0.99602115]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930767  -0.9926301  -0.99602115]], Number:537\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99586916 -0.9929545  -0.9963101 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99586916 -0.9929545  -0.9963101 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99586916 -0.9929545  -0.9963101 ]], Number:538\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99639606 -0.99653685 -0.99805886]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99639606 -0.99653685 -0.99805886]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99639606 -0.99653685 -0.99805886]], Number:539\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978029  -0.99706656 -0.9994733 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9978029  -0.99706656 -0.9994733 ]], Number:540\n",
      "-------------------------\n",
      "Episode 27\tFrame 540 \tAverage100 Score: -190.97tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962934  -0.993762   -0.99303174]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962934  -0.993762   -0.99303174]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962934  -0.993762   -0.99303174]], Number:541\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99899244 -0.99708617 -0.9930403 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99899244 -0.99708617 -0.9930403 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99899244 -0.99708617 -0.9930403 ]], Number:542\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99235135 -0.99747014 -0.99857527]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99235135 -0.99747014 -0.99857527]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99235135 -0.99747014 -0.99857527]], Number:543\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99328023 -0.99763596 -0.9947427 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99328023 -0.99763596 -0.9947427 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99328023 -0.99763596 -0.9947427 ]], Number:544\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925994  -0.99798524 -0.9976369 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925994  -0.99798524 -0.9976369 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9925994  -0.99798524 -0.9976369 ]], Number:545\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954441  -0.9993243  -0.99595034]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954441  -0.9993243  -0.99595034]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954441  -0.9993243  -0.99595034]], Number:546\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99539036 -0.99654406 -0.99407387]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99539036 -0.99654406 -0.99407387]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99539036 -0.99654406 -0.99407387]], Number:547\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946665 -0.9953534 -0.9934722]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946665 -0.9953534 -0.9934722]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9946665 -0.9953534 -0.9934722]], Number:548\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.998856  -0.9985731 -0.9977317]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998856  -0.9985731 -0.9977317]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.998856  -0.9985731 -0.9977317]], Number:549\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99859905 -0.99281937 -0.9931952 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99859905 -0.99281937 -0.9931952 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99859905 -0.99281937 -0.9931952 ]], Number:550\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928383  -0.99686944 -0.9931922 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928383  -0.99686944 -0.9931922 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9928383  -0.99686944 -0.9931922 ]], Number:551\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99252516 -0.9938532  -0.9932392 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99252516 -0.9938532  -0.9932392 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99252516 -0.9938532  -0.9932392 ]], Number:552\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945172  -0.999901   -0.99737597]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945172  -0.999901   -0.99737597]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945172  -0.999901   -0.99737597]], Number:553\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9979336  -0.9957611  -0.99549675]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9979336  -0.9957611  -0.99549675]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9979336  -0.9957611  -0.99549675]], Number:554\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99265957 -0.9979213  -0.99218804]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99265957 -0.9979213  -0.99218804]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99265957 -0.9979213  -0.99218804]], Number:555\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.998782   -0.9937193  -0.99747103]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998782   -0.9937193  -0.99747103]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.998782   -0.9937193  -0.99747103]], Number:556\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954979 -0.9950614 -0.9996098]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954979 -0.9950614 -0.9996098]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954979 -0.9950614 -0.9996098]], Number:557\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953722  -0.99472064 -0.9979198 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953722  -0.99472064 -0.9979198 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9953722  -0.99472064 -0.9979198 ]], Number:558\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99925214 -0.99516124 -0.9979189 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99925214 -0.99516124 -0.9979189 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99925214 -0.99516124 -0.9979189 ]], Number:559\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99858725 -0.99807256 -0.9954282 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99858725 -0.99807256 -0.9954282 ]], Number:560\n",
      "-------------------------\n",
      "Episode 28\tFrame 560 \tAverage100 Score: -191.30tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962491  -0.99693245 -0.9990584 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962491  -0.99693245 -0.9990584 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962491  -0.99693245 -0.9990584 ]], Number:561\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982458  -0.99508965 -0.9967992 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982458  -0.99508965 -0.9967992 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9982458  -0.99508965 -0.9967992 ]], Number:562\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985362  -0.9934596  -0.99562526]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985362  -0.9934596  -0.99562526]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9985362  -0.9934596  -0.99562526]], Number:563\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972393 -0.9922279 -0.9957   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972393 -0.9922279 -0.9957   ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9972393 -0.9922279 -0.9957   ]], Number:564\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987799  -0.99616855 -0.99788064]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987799  -0.99616855 -0.99788064]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9987799  -0.99616855 -0.99788064]], Number:565\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932357  -0.99376553 -0.9993978 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932357  -0.99376553 -0.9993978 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932357  -0.99376553 -0.9993978 ]], Number:566\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989025 -0.9946133 -0.9959742]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989025 -0.9946133 -0.9959742]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9989025 -0.9946133 -0.9959742]], Number:567\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975046 -0.9960948 -0.99889  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975046 -0.9960948 -0.99889  ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975046 -0.9960948 -0.99889  ]], Number:568\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962502  -0.99546164 -0.99252254]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962502  -0.99546164 -0.99252254]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962502  -0.99546164 -0.99252254]], Number:569\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99510056 -0.99538994 -0.99969304]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99510056 -0.99538994 -0.99969304]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99510056 -0.99538994 -0.99969304]], Number:570\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959628 -0.9954991 -0.9946856]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959628 -0.9954991 -0.9946856]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9959628 -0.9954991 -0.9946856]], Number:571\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99749386 -0.9924568  -0.99432313]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99749386 -0.9924568  -0.99432313]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99749386 -0.9924568  -0.99432313]], Number:572\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942256  -0.9921793  -0.99234843]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942256  -0.9921793  -0.99234843]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9942256  -0.9921793  -0.99234843]], Number:573\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99896467 -0.99281377 -0.9957558 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99896467 -0.99281377 -0.9957558 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99896467 -0.99281377 -0.9957558 ]], Number:574\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951676  -0.99983245 -0.99946475]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951676  -0.99983245 -0.99946475]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9951676  -0.99983245 -0.99946475]], Number:575\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99363065 -0.99706537 -0.99212974]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99363065 -0.99706537 -0.99212974]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99363065 -0.99706537 -0.99212974]], Number:576\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924316 -0.9956745 -0.9952858]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924316 -0.9956745 -0.9952858]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9924316 -0.9956745 -0.9952858]], Number:577\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99523896 -0.99568766 -0.9935098 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99523896 -0.99568766 -0.9935098 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99523896 -0.99568766 -0.9935098 ]], Number:578\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99211204 -0.99732405 -0.9966738 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99211204 -0.99732405 -0.9966738 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99211204 -0.99732405 -0.9966738 ]], Number:579\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99686354 -0.99937236 -0.9987318 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99686354 -0.99937236 -0.9987318 ]], Number:580\n",
      "-------------------------\n",
      "Episode 29\tFrame 580 \tAverage100 Score: -191.60tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985483  -0.99404746 -0.9985542 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985483  -0.99404746 -0.9985542 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985483  -0.99404746 -0.9985542 ]], Number:581\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99523187 -0.992188   -0.9967328 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99523187 -0.992188   -0.9967328 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99523187 -0.992188   -0.9967328 ]], Number:582\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9920055 -0.9979985 -0.9971545]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920055 -0.9979985 -0.9971545]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9920055 -0.9979985 -0.9971545]], Number:583\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99885696 -0.99877125 -0.99674374]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99885696 -0.99877125 -0.99674374]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99885696 -0.99877125 -0.99674374]], Number:584\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938049  -0.9947624  -0.99821585]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938049  -0.9947624  -0.99821585]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938049  -0.9947624  -0.99821585]], Number:585\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994791  -0.9993937  -0.99735373]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994791  -0.9993937  -0.99735373]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994791  -0.9993937  -0.99735373]], Number:586\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956024 -0.9999935 -0.9991592]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956024 -0.9999935 -0.9991592]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9956024 -0.9999935 -0.9991592]], Number:587\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99467003 -0.9981337  -0.9996457 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99467003 -0.9981337  -0.9996457 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99467003 -0.9981337  -0.9996457 ]], Number:588\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993514   -0.9993987  -0.99405134]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993514   -0.9993987  -0.99405134]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993514   -0.9993987  -0.99405134]], Number:589\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995784 -0.9931084 -0.9962191]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995784 -0.9931084 -0.9962191]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9995784 -0.9931084 -0.9962191]], Number:590\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972988 -0.9983016 -0.9934366]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972988 -0.9983016 -0.9934366]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9972988 -0.9983016 -0.9934366]], Number:591\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99666584 -0.9955402  -0.99743354]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99666584 -0.9955402  -0.99743354]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99666584 -0.9955402  -0.99743354]], Number:592\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985654  -0.9975302  -0.99794483]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985654  -0.9975302  -0.99794483]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985654  -0.9975302  -0.99794483]], Number:593\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99407023 -0.9934119  -0.9937992 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99407023 -0.9934119  -0.9937992 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99407023 -0.9934119  -0.9937992 ]], Number:594\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99764174 -0.997293   -0.99244905]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99764174 -0.997293   -0.99244905]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99764174 -0.997293   -0.99244905]], Number:595\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963063 -0.995367  -0.9995852]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963063 -0.995367  -0.9995852]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9963063 -0.995367  -0.9995852]], Number:596\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993789   -0.99645406 -0.9982621 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993789   -0.99645406 -0.9982621 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993789   -0.99645406 -0.9982621 ]], Number:597\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961986 -0.9983927 -0.9971816]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961986 -0.9983927 -0.9971816]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9961986 -0.9983927 -0.9971816]], Number:598\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99711084 -0.99651146 -0.99728733]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99711084 -0.99651146 -0.99728733]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99711084 -0.99651146 -0.99728733]], Number:599\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9986515  -0.9933395  -0.99575496]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9986515  -0.9933395  -0.99575496]], Number:600\n",
      "-------------------------\n",
      "Episode 30\tFrame 600 \tAverage100 Score: -191.88tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99697787 -0.9949667  -0.99920225]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99697787 -0.9949667  -0.99920225]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99697787 -0.9949667  -0.99920225]], Number:601\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976383  -0.99714994 -0.99614096]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976383  -0.99714994 -0.99614096]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976383  -0.99714994 -0.99614096]], Number:602\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930976  -0.9964382  -0.99673426]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9930976  -0.9964382  -0.99673426]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9930976  -0.9964382  -0.99673426]], Number:603\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951844  -0.99380857 -0.9974454 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951844  -0.99380857 -0.9974454 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9951844  -0.99380857 -0.9974454 ]], Number:604\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945683  -0.99709815 -0.9998536 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945683  -0.99709815 -0.9998536 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945683  -0.99709815 -0.9998536 ]], Number:605\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99940914 -0.9995963  -0.9968611 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99940914 -0.9995963  -0.9968611 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99940914 -0.9995963  -0.9968611 ]], Number:606\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982233  -0.9954995  -0.99411464]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982233  -0.9954995  -0.99411464]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9982233  -0.9954995  -0.99411464]], Number:607\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963918 -0.9941632 -0.9975742]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963918 -0.9941632 -0.9975742]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9963918 -0.9941632 -0.9975742]], Number:608\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984879  -0.99340445 -0.9955559 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984879  -0.99340445 -0.9955559 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9984879  -0.99340445 -0.9955559 ]], Number:609\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99826026 -0.9971321  -0.99292696]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99826026 -0.9971321  -0.99292696]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99826026 -0.9971321  -0.99292696]], Number:610\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995106 -0.9931712 -0.999086 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995106 -0.9931712 -0.999086 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9995106 -0.9931712 -0.999086 ]], Number:611\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99297935 -0.99572426 -0.9990056 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99297935 -0.99572426 -0.9990056 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99297935 -0.99572426 -0.9990056 ]], Number:612\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977968 -0.9968498 -0.9951786]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977968 -0.9968498 -0.9951786]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9977968 -0.9968498 -0.9951786]], Number:613\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99368405 -0.99325055 -0.9982403 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99368405 -0.99325055 -0.9982403 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99368405 -0.99325055 -0.9982403 ]], Number:614\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99535745 -0.99381375 -0.99847394]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99535745 -0.99381375 -0.99847394]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99535745 -0.99381375 -0.99847394]], Number:615\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99352926 -0.9980645  -0.99526453]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99352926 -0.9980645  -0.99526453]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99352926 -0.9980645  -0.99526453]], Number:616\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9960101  -0.99572027 -0.9931789 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9960101  -0.99572027 -0.9931789 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9960101  -0.99572027 -0.9931789 ]], Number:617\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988917 -0.9979167 -0.9922809]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988917 -0.9979167 -0.9922809]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9988917 -0.9979167 -0.9922809]], Number:618\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99496937 -0.992791   -0.9981653 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99496937 -0.992791   -0.9981653 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99496937 -0.992791   -0.9981653 ]], Number:619\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974386  -0.99533683 -0.9937798 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974386  -0.99533683 -0.9937798 ]], Number:620\n",
      "-------------------------\n",
      "Episode 31\tFrame 620 \tAverage100 Score: -192.14tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959133  -0.9994006  -0.99644375]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959133  -0.9994006  -0.99644375]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9959133  -0.9994006  -0.99644375]], Number:621\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938496  -0.9929479  -0.99928474]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938496  -0.9929479  -0.99928474]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938496  -0.9929479  -0.99928474]], Number:622\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99686366 -0.9927118  -0.9941269 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99686366 -0.9927118  -0.9941269 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99686366 -0.9927118  -0.9941269 ]], Number:623\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99506474 -0.9986679  -0.9960447 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99506474 -0.9986679  -0.9960447 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99506474 -0.9986679  -0.9960447 ]], Number:624\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966545  -0.9959856  -0.99604976]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966545  -0.9959856  -0.99604976]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966545  -0.9959856  -0.99604976]], Number:625\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99927473 -0.9935043  -0.9964795 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99927473 -0.9935043  -0.9964795 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99927473 -0.9935043  -0.9964795 ]], Number:626\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99848455 -0.99506557 -0.99741286]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99848455 -0.99506557 -0.99741286]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99848455 -0.99506557 -0.99741286]], Number:627\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99898225 -0.99534154 -0.99984044]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99898225 -0.99534154 -0.99984044]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99898225 -0.99534154 -0.99984044]], Number:628\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99266195 -0.9954405  -0.9978781 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99266195 -0.9954405  -0.9978781 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99266195 -0.9954405  -0.9978781 ]], Number:629\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925044  -0.9953497  -0.99988765]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925044  -0.9953497  -0.99988765]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9925044  -0.9953497  -0.99988765]], Number:630\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942148  -0.9995743  -0.99778163]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942148  -0.9995743  -0.99778163]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9942148  -0.9995743  -0.99778163]], Number:631\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954438  -0.99472    -0.99236906]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954438  -0.99472    -0.99236906]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9954438  -0.99472    -0.99236906]], Number:632\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934575 -0.9985712 -0.9956091]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934575 -0.9985712 -0.9956091]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934575 -0.9985712 -0.9956091]], Number:633\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991818  -0.9943714  -0.99658585]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991818  -0.9943714  -0.99658585]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9991818  -0.9943714  -0.99658585]], Number:634\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99477345 -0.99945784 -0.9960486 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99477345 -0.99945784 -0.9960486 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99477345 -0.99945784 -0.9960486 ]], Number:635\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965098  -0.9996559  -0.99472183]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965098  -0.9996559  -0.99472183]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9965098  -0.9996559  -0.99472183]], Number:636\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99216306 -0.99402374 -0.9922893 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99216306 -0.99402374 -0.9922893 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99216306 -0.99402374 -0.9922893 ]], Number:637\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999227   -0.99488324 -0.99533826]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999227   -0.99488324 -0.99533826]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.999227   -0.99488324 -0.99533826]], Number:638\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99638563 -0.99358195 -0.9967016 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99638563 -0.99358195 -0.9967016 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99638563 -0.99358195 -0.9967016 ]], Number:639\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99930197 -0.9949307  -0.99318624]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99930197 -0.9949307  -0.99318624]], Number:640\n",
      "-------------------------\n",
      "Episode 32\tFrame 640 \tAverage100 Score: -192.38tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970379  -0.99497217 -0.9988129 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970379  -0.99497217 -0.9988129 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9970379  -0.99497217 -0.9988129 ]], Number:641\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99218357 -0.99217325 -0.99987733]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99218357 -0.99217325 -0.99987733]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99218357 -0.99217325 -0.99987733]], Number:642\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948573 -0.9925688 -0.9958917]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948573 -0.9925688 -0.9958917]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9948573 -0.9925688 -0.9958917]], Number:643\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993571  -0.9941688  -0.99599284]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993571  -0.9941688  -0.99599284]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993571  -0.9941688  -0.99599284]], Number:644\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972661  -0.99953365 -0.99822325]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972661  -0.99953365 -0.99822325]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9972661  -0.99953365 -0.99822325]], Number:645\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99346256 -0.99623585 -0.9957402 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99346256 -0.99623585 -0.9957402 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99346256 -0.99623585 -0.9957402 ]], Number:646\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948593  -0.9969537  -0.99391866]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948593  -0.9969537  -0.99391866]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948593  -0.9969537  -0.99391866]], Number:647\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99421036 -0.9998426  -0.99488246]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99421036 -0.9998426  -0.99488246]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99421036 -0.9998426  -0.99488246]], Number:648\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997842  -0.9959561  -0.99260026]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997842  -0.9959561  -0.99260026]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997842  -0.9959561  -0.99260026]], Number:649\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99337155 -0.99757224 -0.9925388 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99337155 -0.99757224 -0.9925388 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99337155 -0.99757224 -0.9925388 ]], Number:650\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9986127 -0.99438   -0.9938811]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9986127 -0.99438   -0.9938811]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9986127 -0.99438   -0.9938811]], Number:651\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996851  -0.99793017 -0.99570304]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996851  -0.99793017 -0.99570304]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9996851  -0.99793017 -0.99570304]], Number:652\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99203044 -0.9986753  -0.99630684]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99203044 -0.9986753  -0.99630684]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99203044 -0.9986753  -0.99630684]], Number:653\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99283546 -0.99862194 -0.99332386]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99283546 -0.99862194 -0.99332386]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99283546 -0.99862194 -0.99332386]], Number:654\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940539  -0.9936346  -0.99726087]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940539  -0.9936346  -0.99726087]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9940539  -0.9936346  -0.99726087]], Number:655\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951674  -0.99882084 -0.997749  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951674  -0.99882084 -0.997749  ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9951674  -0.99882084 -0.997749  ]], Number:656\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99352366 -0.9990947  -0.9956591 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99352366 -0.9990947  -0.9956591 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99352366 -0.9990947  -0.9956591 ]], Number:657\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993244  -0.9935451  -0.99486655]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9993244  -0.9935451  -0.99486655]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9993244  -0.9935451  -0.99486655]], Number:658\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990247  -0.9929404  -0.99478847]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990247  -0.9929404  -0.99478847]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9990247  -0.9929404  -0.99478847]], Number:659\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994848  -0.9993828 -0.9933875]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.994848  -0.9993828 -0.9933875]], Number:660\n",
      "-------------------------\n",
      "Episode 33\tFrame 660 \tAverage100 Score: -192.61tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99440706 -0.99692583 -0.9993889 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99440706 -0.99692583 -0.9993889 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99440706 -0.99692583 -0.9993889 ]], Number:661\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99382186 -0.9982368  -0.9936905 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99382186 -0.9982368  -0.9936905 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99382186 -0.9982368  -0.9936905 ]], Number:662\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953241  -0.99702895 -0.99641   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953241  -0.99702895 -0.99641   ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9953241  -0.99702895 -0.99641   ]], Number:663\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997745   -0.9959317  -0.99292505]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997745   -0.9959317  -0.99292505]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.997745   -0.9959317  -0.99292505]], Number:664\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99549145 -0.9968983  -0.99446553]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99549145 -0.9968983  -0.99446553]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99549145 -0.9968983  -0.99446553]], Number:665\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99221027 -0.9985273  -0.9955184 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99221027 -0.9985273  -0.9955184 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99221027 -0.9985273  -0.9955184 ]], Number:666\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99289274 -0.99557585 -0.99811035]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99289274 -0.99557585 -0.99811035]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99289274 -0.99557585 -0.99811035]], Number:667\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99850917 -0.9943527  -0.99541795]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99850917 -0.9943527  -0.99541795]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99850917 -0.9943527  -0.99541795]], Number:668\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993742 -0.9991533 -0.9975464]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993742 -0.9991533 -0.9975464]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993742 -0.9991533 -0.9975464]], Number:669\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99559116 -0.9964847  -0.9976497 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99559116 -0.9964847  -0.9976497 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99559116 -0.9964847  -0.9976497 ]], Number:670\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99803746 -0.9952765  -0.9937074 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99803746 -0.9952765  -0.9937074 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99803746 -0.9952765  -0.9937074 ]], Number:671\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99700063 -0.99632376 -0.99441355]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99700063 -0.99632376 -0.99441355]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99700063 -0.99632376 -0.99441355]], Number:672\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997079  -0.9938083 -0.999985 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997079  -0.9938083 -0.999985 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.997079  -0.9938083 -0.999985 ]], Number:673\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9978901 -0.9953853 -0.9990869]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978901 -0.9953853 -0.9990869]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9978901 -0.9953853 -0.9990869]], Number:674\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99967456 -0.99505943 -0.99551195]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99967456 -0.99505943 -0.99551195]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99967456 -0.99505943 -0.99551195]], Number:675\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947897  -0.9932104  -0.99742323]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9947897  -0.9932104  -0.99742323]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9947897  -0.9932104  -0.99742323]], Number:676\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927457 -0.9931275 -0.9923796]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927457 -0.9931275 -0.9923796]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927457 -0.9931275 -0.9923796]], Number:677\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975623  -0.99948776 -0.9937688 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975623  -0.99948776 -0.9937688 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9975623  -0.99948776 -0.9937688 ]], Number:678\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99846554 -0.9995375  -0.9942143 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99846554 -0.9995375  -0.9942143 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99846554 -0.9995375  -0.9942143 ]], Number:679\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944247  -0.99463344 -0.9991279 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9944247  -0.99463344 -0.9991279 ]], Number:680\n",
      "-------------------------\n",
      "Episode 34\tFrame 680 \tAverage100 Score: -192.83tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99428666 -0.998045   -0.99601877]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99428666 -0.998045   -0.99601877]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99428666 -0.998045   -0.99601877]], Number:681\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945894  -0.9971921  -0.99713767]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945894  -0.9971921  -0.99713767]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945894  -0.9971921  -0.99713767]], Number:682\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933348  -0.99476814 -0.99334353]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933348  -0.99476814 -0.99334353]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9933348  -0.99476814 -0.99334353]], Number:683\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995944  -0.99378633 -0.9985    ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995944  -0.99378633 -0.9985    ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9995944  -0.99378633 -0.9985    ]], Number:684\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974147  -0.99480486 -0.9936915 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974147  -0.99480486 -0.9936915 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974147  -0.99480486 -0.9936915 ]], Number:685\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99488676 -0.99595076 -0.995704  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99488676 -0.99595076 -0.995704  ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99488676 -0.99595076 -0.995704  ]], Number:686\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951687 -0.9982989 -0.9970304]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951687 -0.9982989 -0.9970304]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9951687 -0.9982989 -0.9970304]], Number:687\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965457 -0.9991444 -0.9965522]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965457 -0.9991444 -0.9965522]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9965457 -0.9991444 -0.9965522]], Number:688\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99447054 -0.9999349  -0.9972837 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99447054 -0.9999349  -0.9972837 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99447054 -0.9999349  -0.9972837 ]], Number:689\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992543  -0.9927563 -0.9989963]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992543  -0.9927563 -0.9989963]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.992543  -0.9927563 -0.9989963]], Number:690\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99272776 -0.99265814 -0.99494994]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99272776 -0.99265814 -0.99494994]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99272776 -0.99265814 -0.99494994]], Number:691\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99830276 -0.9962596  -0.99882394]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99830276 -0.9962596  -0.99882394]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99830276 -0.9962596  -0.99882394]], Number:692\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99560875 -0.99920577 -0.9968649 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99560875 -0.99920577 -0.9968649 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99560875 -0.99920577 -0.9968649 ]], Number:693\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99887073 -0.99457186 -0.99720454]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99887073 -0.99457186 -0.99720454]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99887073 -0.99457186 -0.99720454]], Number:694\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932097  -0.99338126 -0.99875206]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932097  -0.99338126 -0.99875206]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932097  -0.99338126 -0.99875206]], Number:695\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99415344 -0.9941173  -0.99783695]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99415344 -0.9941173  -0.99783695]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99415344 -0.9941173  -0.99783695]], Number:696\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933908  -0.99951905 -0.9974396 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933908  -0.99951905 -0.9974396 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9933908  -0.99951905 -0.9974396 ]], Number:697\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944791  -0.99355435 -0.9924364 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944791  -0.99355435 -0.9924364 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9944791  -0.99355435 -0.9924364 ]], Number:698\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955328 -0.9941566 -0.9924047]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955328 -0.9941566 -0.9924047]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9955328 -0.9941566 -0.9924047]], Number:699\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925235  -0.9973868  -0.99647075]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9925235  -0.9973868  -0.99647075]], Number:700\n",
      "-------------------------\n",
      "Episode 35\tFrame 700 \tAverage100 Score: -193.04tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938264  -0.99582225 -0.9993543 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938264  -0.99582225 -0.9993543 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938264  -0.99582225 -0.9993543 ]], Number:701\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99272585 -0.99689925 -0.99874246]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99272585 -0.99689925 -0.99874246]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99272585 -0.99689925 -0.99874246]], Number:702\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973298 -0.999778  -0.9926908]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973298 -0.999778  -0.9926908]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9973298 -0.999778  -0.9926908]], Number:703\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99888206 -0.99999756 -0.9923373 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99888206 -0.99999756 -0.9923373 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99888206 -0.99999756 -0.9923373 ]], Number:704\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975377  -0.99501294 -0.9970598 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975377  -0.99501294 -0.9970598 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975377  -0.99501294 -0.9970598 ]], Number:705\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99728554 -0.9926008  -0.9941452 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99728554 -0.9926008  -0.9941452 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99728554 -0.9926008  -0.9941452 ]], Number:706\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99731165 -0.9933857  -0.9938352 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99731165 -0.9933857  -0.9938352 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99731165 -0.9933857  -0.9938352 ]], Number:707\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99306494 -0.996625   -0.99657327]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99306494 -0.996625   -0.99657327]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99306494 -0.996625   -0.99657327]], Number:708\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927942  -0.9963011  -0.99709374]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927942  -0.9963011  -0.99709374]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927942  -0.9963011  -0.99709374]], Number:709\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950487 -0.994422  -0.9958402]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950487 -0.994422  -0.9958402]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950487 -0.994422  -0.9958402]], Number:710\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99939483 -0.99207747 -0.99854714]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99939483 -0.99207747 -0.99854714]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99939483 -0.99207747 -0.99854714]], Number:711\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995325  -0.9973788 -0.9995964]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.995325  -0.9973788 -0.9995964]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.995325  -0.9973788 -0.9995964]], Number:712\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99813336 -0.99671245 -0.9996344 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99813336 -0.99671245 -0.9996344 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99813336 -0.99671245 -0.9996344 ]], Number:713\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99924016 -0.9958657  -0.9967713 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99924016 -0.9958657  -0.9967713 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99924016 -0.9958657  -0.9967713 ]], Number:714\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995649 -0.9974792 -0.9981989]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995649 -0.9974792 -0.9981989]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9995649 -0.9974792 -0.9981989]], Number:715\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99313986 -0.9925943  -0.99298567]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99313986 -0.9925943  -0.99298567]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99313986 -0.9925943  -0.99298567]], Number:716\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928338  -0.9961728  -0.99821055]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928338  -0.9961728  -0.99821055]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9928338  -0.9961728  -0.99821055]], Number:717\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99495214 -0.9982129  -0.99262077]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99495214 -0.9982129  -0.99262077]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99495214 -0.9982129  -0.99262077]], Number:718\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982472  -0.9953364  -0.99507666]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982472  -0.9953364  -0.99507666]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9982472  -0.9953364  -0.99507666]], Number:719\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992044  -0.99414855 -0.9961709 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992044  -0.99414855 -0.9961709 ]], Number:720\n",
      "-------------------------\n",
      "Episode 36\tFrame 720 \tAverage100 Score: -193.23tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944263 -0.995664  -0.9937575]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944263 -0.995664  -0.9937575]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9944263 -0.995664  -0.9937575]], Number:721\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991161 -0.9982338 -0.9945583]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991161 -0.9982338 -0.9945583]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991161 -0.9982338 -0.9945583]], Number:722\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99925864 -0.9967815  -0.9943131 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99925864 -0.9967815  -0.9943131 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99925864 -0.9967815  -0.9943131 ]], Number:723\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988846  -0.99677885 -0.9943996 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988846  -0.99677885 -0.9943996 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988846  -0.99677885 -0.9943996 ]], Number:724\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9983521  -0.9975284  -0.99587375]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983521  -0.9975284  -0.99587375]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9983521  -0.9975284  -0.99587375]], Number:725\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99317807 -0.99583745 -0.9931873 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99317807 -0.99583745 -0.9931873 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99317807 -0.99583745 -0.9931873 ]], Number:726\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.996174  -0.9991225 -0.9928431]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.996174  -0.9991225 -0.9928431]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.996174  -0.9991225 -0.9928431]], Number:727\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965029  -0.9989371  -0.99546933]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9965029  -0.9989371  -0.99546933]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9965029  -0.9989371  -0.99546933]], Number:728\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964982 -0.9999798 -0.9975747]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964982 -0.9999798 -0.9975747]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9964982 -0.9999798 -0.9975747]], Number:729\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989691  -0.99682003 -0.9974331 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989691  -0.99682003 -0.9974331 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9989691  -0.99682003 -0.9974331 ]], Number:730\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992432   -0.9996304  -0.99761873]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992432   -0.9996304  -0.99761873]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.992432   -0.9996304  -0.99761873]], Number:731\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997837 -0.9969585 -0.9971804]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997837 -0.9969585 -0.9971804]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9997837 -0.9969585 -0.9971804]], Number:732\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99742734 -0.9984991  -0.9974911 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99742734 -0.9984991  -0.9974911 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99742734 -0.9984991  -0.9974911 ]], Number:733\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933235  -0.99277145 -0.9957856 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933235  -0.99277145 -0.9957856 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9933235  -0.99277145 -0.9957856 ]], Number:734\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953988 -0.9980237 -0.9923878]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953988 -0.9980237 -0.9923878]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9953988 -0.9980237 -0.9923878]], Number:735\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99982214 -0.9959554  -0.99578637]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99982214 -0.9959554  -0.99578637]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99982214 -0.9959554  -0.99578637]], Number:736\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99592906 -0.99256635 -0.99524206]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99592906 -0.99256635 -0.99524206]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99592906 -0.99256635 -0.99524206]], Number:737\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99906963 -0.99913627 -0.9952524 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99906963 -0.99913627 -0.9952524 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99906963 -0.99913627 -0.9952524 ]], Number:738\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99986666 -0.9969217  -0.9937688 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99986666 -0.9969217  -0.9937688 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99986666 -0.9969217  -0.9937688 ]], Number:739\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962782 -0.9920448 -0.9962545]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962782 -0.9920448 -0.9962545]], Number:740\n",
      "-------------------------\n",
      "Episode 37\tFrame 740 \tAverage100 Score: -193.41tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972756  -0.9992902  -0.99639726]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972756  -0.9992902  -0.99639726]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9972756  -0.9992902  -0.99639726]], Number:741\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.996644  -0.9942652 -0.9953274]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.996644  -0.9942652 -0.9953274]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.996644  -0.9942652 -0.9953274]], Number:742\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99303347 -0.99619937 -0.99663234]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99303347 -0.99619937 -0.99663234]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99303347 -0.99619937 -0.99663234]], Number:743\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999154   -0.99403614 -0.99742126]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.999154   -0.99403614 -0.99742126]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999154   -0.99403614 -0.99742126]], Number:744\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99628663 -0.99471104 -0.9956296 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99628663 -0.99471104 -0.9956296 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99628663 -0.99471104 -0.9956296 ]], Number:745\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99852777 -0.9946555  -0.9987989 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99852777 -0.9946555  -0.9987989 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99852777 -0.9946555  -0.9987989 ]], Number:746\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99272513 -0.99202985 -0.9921145 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99272513 -0.99202985 -0.9921145 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99272513 -0.99202985 -0.9921145 ]], Number:747\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99614966 -0.99715793 -0.9954779 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99614966 -0.99715793 -0.9954779 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99614966 -0.99715793 -0.9954779 ]], Number:748\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938817  -0.998207   -0.99489635]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938817  -0.998207   -0.99489635]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938817  -0.998207   -0.99489635]], Number:749\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99728113 -0.9969748  -0.999457  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99728113 -0.9969748  -0.999457  ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99728113 -0.9969748  -0.999457  ]], Number:750\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942003 -0.9972938 -0.9955311]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942003 -0.9972938 -0.9955311]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9942003 -0.9972938 -0.9955311]], Number:751\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99574155 -0.9930636  -0.994154  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99574155 -0.9930636  -0.994154  ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99574155 -0.9930636  -0.994154  ]], Number:752\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99407375 -0.99253446 -0.9932584 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99407375 -0.99253446 -0.9932584 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99407375 -0.99253446 -0.9932584 ]], Number:753\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988158 -0.9930549 -0.9929772]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988158 -0.9930549 -0.9929772]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9988158 -0.9930549 -0.9929772]], Number:754\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984916  -0.99776405 -0.99388766]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984916  -0.99776405 -0.99388766]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9984916  -0.99776405 -0.99388766]], Number:755\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929127  -0.99948573 -0.993834  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929127  -0.99948573 -0.993834  ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9929127  -0.99948573 -0.993834  ]], Number:756\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930275 -0.9964121 -0.9981768]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930275 -0.9964121 -0.9981768]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930275 -0.9964121 -0.9981768]], Number:757\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99800706 -0.99809545 -0.99318576]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99800706 -0.99809545 -0.99318576]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99800706 -0.99809545 -0.99318576]], Number:758\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99394274 -0.9979327  -0.99404573]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99394274 -0.9979327  -0.99404573]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99394274 -0.9979327  -0.99404573]], Number:759\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99345773 -0.99789447 -0.9928233 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99345773 -0.99789447 -0.9928233 ]], Number:760\n",
      "-------------------------\n",
      "Episode 38\tFrame 760 \tAverage100 Score: -193.59tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950844  -0.99948394 -0.9968167 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950844  -0.99948394 -0.9968167 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950844  -0.99948394 -0.9968167 ]], Number:761\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99728155 -0.9984334  -0.9949851 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99728155 -0.9984334  -0.9949851 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99728155 -0.9984334  -0.9949851 ]], Number:762\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965874 -0.9924087 -0.9964007]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965874 -0.9924087 -0.9964007]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9965874 -0.9924087 -0.9964007]], Number:763\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954494 -0.9927925 -0.9999503]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954494 -0.9927925 -0.9999503]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954494 -0.9927925 -0.9999503]], Number:764\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974962  -0.9927607  -0.99476415]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974962  -0.9927607  -0.99476415]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974962  -0.9927607  -0.99476415]], Number:765\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99329364 -0.999466   -0.9936689 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99329364 -0.999466   -0.9936689 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99329364 -0.999466   -0.9936689 ]], Number:766\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99792516 -0.99747384 -0.9971599 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99792516 -0.99747384 -0.9971599 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99792516 -0.99747384 -0.9971599 ]], Number:767\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957841  -0.99323064 -0.9946926 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957841  -0.99323064 -0.9946926 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9957841  -0.99323064 -0.9946926 ]], Number:768\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99445146 -0.99816996 -0.99439895]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99445146 -0.99816996 -0.99439895]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99445146 -0.99816996 -0.99439895]], Number:769\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99698645 -0.9933413  -0.99512184]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99698645 -0.9933413  -0.99512184]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99698645 -0.9933413  -0.99512184]], Number:770\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99861753 -0.99686176 -0.99468464]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99861753 -0.99686176 -0.99468464]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99861753 -0.99686176 -0.99468464]], Number:771\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99482155 -0.99820507 -0.998347  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99482155 -0.99820507 -0.998347  ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99482155 -0.99820507 -0.998347  ]], Number:772\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99407864 -0.9992716  -0.99582726]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99407864 -0.9992716  -0.99582726]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99407864 -0.9992716  -0.99582726]], Number:773\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99376297 -0.99837047 -0.99980253]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99376297 -0.99837047 -0.99980253]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99376297 -0.99837047 -0.99980253]], Number:774\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922711  -0.99343747 -0.9942884 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922711  -0.99343747 -0.9942884 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922711  -0.99343747 -0.9942884 ]], Number:775\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994033   -0.99353766 -0.9961934 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994033   -0.99353766 -0.9961934 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.994033   -0.99353766 -0.9961934 ]], Number:776\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927    -0.9923417 -0.9943561]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927    -0.9923417 -0.9943561]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927    -0.9923417 -0.9943561]], Number:777\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941389  -0.99958223 -0.9985699 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9941389  -0.99958223 -0.9985699 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9941389  -0.99958223 -0.9985699 ]], Number:778\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955897  -0.99373174 -0.99972314]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955897  -0.99373174 -0.99972314]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9955897  -0.99373174 -0.99972314]], Number:779\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928575  -0.99273896 -0.9996728 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9928575  -0.99273896 -0.9996728 ]], Number:780\n",
      "-------------------------\n",
      "Episode 39\tFrame 780 \tAverage100 Score: -193.75tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942216  -0.9943105  -0.99761796]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942216  -0.9943105  -0.99761796]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9942216  -0.9943105  -0.99761796]], Number:781\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954786  -0.99856436 -0.9965832 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954786  -0.99856436 -0.9965832 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954786  -0.99856436 -0.9965832 ]], Number:782\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99278474 -0.99665713 -0.99606156]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99278474 -0.99665713 -0.99606156]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99278474 -0.99665713 -0.99606156]], Number:783\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973821 -0.9945324 -0.9959679]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973821 -0.9945324 -0.9959679]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9973821 -0.9945324 -0.9959679]], Number:784\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99691904 -0.9957516  -0.99500126]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99691904 -0.9957516  -0.99500126]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99691904 -0.9957516  -0.99500126]], Number:785\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99255407 -0.99758595 -0.9984948 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99255407 -0.99758595 -0.9984948 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99255407 -0.99758595 -0.9984948 ]], Number:786\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99807245 -0.9979805  -0.9957281 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99807245 -0.9979805  -0.9957281 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99807245 -0.9979805  -0.9957281 ]], Number:787\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99903685 -0.99853915 -0.99639237]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99903685 -0.99853915 -0.99639237]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99903685 -0.99853915 -0.99639237]], Number:788\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999377  -0.99690664 -0.992843  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999377  -0.99690664 -0.992843  ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9999377  -0.99690664 -0.992843  ]], Number:789\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99776655 -0.99530464 -0.9945988 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99776655 -0.99530464 -0.9945988 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99776655 -0.99530464 -0.9945988 ]], Number:790\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99442554 -0.99229735 -0.9959053 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99442554 -0.99229735 -0.9959053 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99442554 -0.99229735 -0.9959053 ]], Number:791\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99841505 -0.99413836 -0.99886817]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99841505 -0.99413836 -0.99886817]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99841505 -0.99413836 -0.99886817]], Number:792\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99656546 -0.999563   -0.99830455]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99656546 -0.999563   -0.99830455]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99656546 -0.999563   -0.99830455]], Number:793\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977561  -0.99516696 -0.99288875]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977561  -0.99516696 -0.99288875]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9977561  -0.99516696 -0.99288875]], Number:794\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994071  -0.99237245 -0.9974617 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9994071  -0.99237245 -0.9974617 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994071  -0.99237245 -0.9974617 ]], Number:795\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977065  -0.9997495  -0.99760514]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977065  -0.9997495  -0.99760514]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9977065  -0.9997495  -0.99760514]], Number:796\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99999017 -0.99706125 -0.9942618 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99999017 -0.99706125 -0.9942618 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99999017 -0.99706125 -0.9942618 ]], Number:797\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957416  -0.9980915  -0.99229044]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957416  -0.9980915  -0.99229044]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9957416  -0.9980915  -0.99229044]], Number:798\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929093 -0.9950691 -0.9978468]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929093 -0.9950691 -0.9978468]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9929093 -0.9950691 -0.9978468]], Number:799\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969381  -0.9994075  -0.99403983]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9969381  -0.9994075  -0.99403983]], Number:800\n",
      "-------------------------\n",
      "Episode 40\tFrame 800 \tAverage100 Score: -193.91tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99492896 -0.99447954 -0.9947125 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99492896 -0.99447954 -0.9947125 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99492896 -0.99447954 -0.9947125 ]], Number:801\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962249  -0.99600303 -0.9970583 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962249  -0.99600303 -0.9970583 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962249  -0.99600303 -0.9970583 ]], Number:802\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977002  -0.9993883  -0.99524355]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977002  -0.9993883  -0.99524355]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9977002  -0.9993883  -0.99524355]], Number:803\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954789  -0.99267393 -0.99654704]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954789  -0.99267393 -0.99654704]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954789  -0.99267393 -0.99654704]], Number:804\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99209374 -0.9994118  -0.9961336 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99209374 -0.9994118  -0.9961336 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99209374 -0.9994118  -0.9961336 ]], Number:805\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9960217 -0.9952923 -0.993009 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9960217 -0.9952923 -0.993009 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9960217 -0.9952923 -0.993009 ]], Number:806\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99550265 -0.99464095 -0.9932017 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99550265 -0.99464095 -0.9932017 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99550265 -0.99464095 -0.9932017 ]], Number:807\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99218345 -0.99904406 -0.99340045]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99218345 -0.99904406 -0.99340045]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99218345 -0.99904406 -0.99340045]], Number:808\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968561  -0.99428046 -0.993556  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968561  -0.99428046 -0.993556  ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968561  -0.99428046 -0.993556  ]], Number:809\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99924487 -0.99958    -0.992861  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99924487 -0.99958    -0.992861  ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99924487 -0.99958    -0.992861  ]], Number:810\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99574447 -0.99341613 -0.9989729 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99574447 -0.99341613 -0.9989729 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99574447 -0.99341613 -0.9989729 ]], Number:811\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957773  -0.99550647 -0.99921304]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957773  -0.99550647 -0.99921304]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9957773  -0.99550647 -0.99921304]], Number:812\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99482864 -0.9979264  -0.9950229 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99482864 -0.9979264  -0.9950229 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99482864 -0.9979264  -0.9950229 ]], Number:813\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987547  -0.9944771  -0.99542224]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987547  -0.9944771  -0.99542224]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9987547  -0.9944771  -0.99542224]], Number:814\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99409133 -0.9948558  -0.99369967]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99409133 -0.9948558  -0.99369967]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99409133 -0.9948558  -0.99369967]], Number:815\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999703 -0.9992418 -0.996779 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999703 -0.9992418 -0.996779 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9999703 -0.9992418 -0.996779 ]], Number:816\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9967327  -0.99607205 -0.9951453 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9967327  -0.99607205 -0.9951453 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9967327  -0.99607205 -0.9951453 ]], Number:817\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99729586 -0.9976856  -0.9977215 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99729586 -0.9976856  -0.9977215 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99729586 -0.9976856  -0.9977215 ]], Number:818\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973781  -0.99599266 -0.9926946 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973781  -0.99599266 -0.9926946 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9973781  -0.99599266 -0.9926946 ]], Number:819\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998371  -0.993626  -0.9944738]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.998371  -0.993626  -0.9944738]], Number:820\n",
      "-------------------------\n",
      "Episode 41\tFrame 820 \tAverage100 Score: -194.06tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99660516 -0.9952234  -0.99471277]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99660516 -0.9952234  -0.99471277]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99660516 -0.9952234  -0.99471277]], Number:821\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993222   -0.9995351  -0.99543864]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993222   -0.9995351  -0.99543864]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993222   -0.9995351  -0.99543864]], Number:822\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964581  -0.99775845 -0.99299943]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964581  -0.99775845 -0.99299943]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9964581  -0.99775845 -0.99299943]], Number:823\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9978016  -0.99468654 -0.993883  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978016  -0.99468654 -0.993883  ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9978016  -0.99468654 -0.993883  ]], Number:824\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933212 -0.998875  -0.9951586]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933212 -0.998875  -0.9951586]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9933212 -0.998875  -0.9951586]], Number:825\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99833494 -0.9936628  -0.9998955 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99833494 -0.9936628  -0.9998955 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99833494 -0.9936628  -0.9998955 ]], Number:826\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99672276 -0.99296296 -0.9997182 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99672276 -0.99296296 -0.9997182 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99672276 -0.99296296 -0.9997182 ]], Number:827\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99778867 -0.9946844  -0.9982894 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99778867 -0.9946844  -0.9982894 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99778867 -0.9946844  -0.9982894 ]], Number:828\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99227434 -0.9976219  -0.99608624]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99227434 -0.9976219  -0.99608624]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99227434 -0.9976219  -0.99608624]], Number:829\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9967337  -0.99525416 -0.99920577]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9967337  -0.99525416 -0.99920577]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9967337  -0.99525416 -0.99920577]], Number:830\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954931  -0.9956498  -0.99899846]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954931  -0.9956498  -0.99899846]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9954931  -0.9956498  -0.99899846]], Number:831\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99823964 -0.99963576 -0.9961328 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99823964 -0.99963576 -0.9961328 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99823964 -0.99963576 -0.9961328 ]], Number:832\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99808556 -0.99585944 -0.99514335]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99808556 -0.99585944 -0.99514335]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99808556 -0.99585944 -0.99514335]], Number:833\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9952956  -0.99810994 -0.99540967]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952956  -0.99810994 -0.99540967]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9952956  -0.99810994 -0.99540967]], Number:834\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99478376 -0.9955342  -0.999894  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99478376 -0.9955342  -0.999894  ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99478376 -0.9955342  -0.999894  ]], Number:835\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999603 -0.9976844 -0.9951841]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999603 -0.9976844 -0.9951841]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9999603 -0.9976844 -0.9951841]], Number:836\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9978123  -0.9940067  -0.99291354]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978123  -0.9940067  -0.99291354]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9978123  -0.9940067  -0.99291354]], Number:837\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99424237 -0.9993285  -0.99519384]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99424237 -0.9993285  -0.99519384]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99424237 -0.9993285  -0.99519384]], Number:838\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99331516 -0.99276936 -0.9994933 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99331516 -0.99276936 -0.9994933 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99331516 -0.99276936 -0.9994933 ]], Number:839\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954111  -0.99806684 -0.99588674]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954111  -0.99806684 -0.99588674]], Number:840\n",
      "-------------------------\n",
      "Episode 42\tFrame 840 \tAverage100 Score: -194.20tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99677134 -0.9929906  -0.9991589 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99677134 -0.9929906  -0.9991589 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99677134 -0.9929906  -0.9991589 ]], Number:841\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985677  -0.994238   -0.99355316]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985677  -0.994238   -0.99355316]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985677  -0.994238   -0.99355316]], Number:842\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977264 -0.9920994 -0.9949109]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9977264 -0.9920994 -0.9949109]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9977264 -0.9920994 -0.9949109]], Number:843\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941098 -0.999674  -0.9942095]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941098 -0.999674  -0.9942095]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941098 -0.999674  -0.9942095]], Number:844\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99558693 -0.9993393  -0.99870116]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99558693 -0.9993393  -0.99870116]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99558693 -0.9993393  -0.99870116]], Number:845\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997893  -0.9920589 -0.997612 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997893  -0.9920589 -0.997612 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.997893  -0.9920589 -0.997612 ]], Number:846\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954096 -0.9979463 -0.9966393]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954096 -0.9979463 -0.9966393]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954096 -0.9979463 -0.9966393]], Number:847\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962308  -0.99986136 -0.99958605]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962308  -0.99986136 -0.99958605]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962308  -0.99986136 -0.99958605]], Number:848\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955772  -0.99859685 -0.9967081 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955772  -0.99859685 -0.9967081 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955772  -0.99859685 -0.9967081 ]], Number:849\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99982923 -0.9957411  -0.9979671 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99982923 -0.9957411  -0.9979671 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99982923 -0.9957411  -0.9979671 ]], Number:850\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948257  -0.9940003  -0.99232775]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948257  -0.9940003  -0.99232775]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9948257  -0.9940003  -0.99232775]], Number:851\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99294126 -0.9950853  -0.99487704]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99294126 -0.9950853  -0.99487704]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99294126 -0.9950853  -0.99487704]], Number:852\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921788 -0.9960019 -0.9960892]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921788 -0.9960019 -0.9960892]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921788 -0.9960019 -0.9960892]], Number:853\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972054  -0.99738324 -0.99941015]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972054  -0.99738324 -0.99941015]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9972054  -0.99738324 -0.99941015]], Number:854\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993916   -0.99521136 -0.9921036 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993916   -0.99521136 -0.9921036 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993916   -0.99521136 -0.9921036 ]], Number:855\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99813056 -0.9980233  -0.9977622 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99813056 -0.9980233  -0.9977622 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99813056 -0.9980233  -0.9977622 ]], Number:856\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974667  -0.9987374  -0.99478006]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974667  -0.9987374  -0.99478006]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974667  -0.9987374  -0.99478006]], Number:857\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969868  -0.99949247 -0.997298  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9969868  -0.99949247 -0.997298  ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9969868  -0.99949247 -0.997298  ]], Number:858\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99557793 -0.9995618  -0.9949913 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99557793 -0.9995618  -0.9949913 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99557793 -0.9995618  -0.9949913 ]], Number:859\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99281955 -0.9953007  -0.99401724]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99281955 -0.9953007  -0.99401724]], Number:860\n",
      "-------------------------\n",
      "Episode 43\tFrame 860 \tAverage100 Score: -194.33tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995591 -0.9995969 -0.993921 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995591 -0.9995969 -0.993921 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9995591 -0.9995969 -0.993921 ]], Number:861\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99368477 -0.9986342  -0.9981267 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99368477 -0.9986342  -0.9981267 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99368477 -0.9986342  -0.9981267 ]], Number:862\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99799633 -0.99473155 -0.9980433 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99799633 -0.99473155 -0.9980433 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99799633 -0.99473155 -0.9980433 ]], Number:863\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99301004 -0.9939071  -0.9975333 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99301004 -0.9939071  -0.9975333 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99301004 -0.9939071  -0.9975333 ]], Number:864\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955529  -0.993371   -0.99798954]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955529  -0.993371   -0.99798954]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955529  -0.993371   -0.99798954]], Number:865\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9983778 -0.9952417 -0.9920254]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983778 -0.9952417 -0.9920254]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9983778 -0.9952417 -0.9920254]], Number:866\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99866164 -0.9962052  -0.9998554 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99866164 -0.9962052  -0.9998554 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99866164 -0.9962052  -0.9998554 ]], Number:867\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.998508   -0.9934897  -0.99249214]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998508   -0.9934897  -0.99249214]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.998508   -0.9934897  -0.99249214]], Number:868\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99787235 -0.9976805  -0.9992115 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99787235 -0.9976805  -0.9992115 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99787235 -0.9976805  -0.9992115 ]], Number:869\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949168  -0.99670845 -0.9936939 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949168  -0.99670845 -0.9936939 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9949168  -0.99670845 -0.9936939 ]], Number:870\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975616  -0.99610335 -0.9936911 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975616  -0.99610335 -0.9936911 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9975616  -0.99610335 -0.9936911 ]], Number:871\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99929047 -0.99803644 -0.9933375 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99929047 -0.99803644 -0.9933375 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99929047 -0.99803644 -0.9933375 ]], Number:872\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992711   -0.99248755 -0.99292016]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.992711   -0.99248755 -0.99292016]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.992711   -0.99248755 -0.99292016]], Number:873\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99844617 -0.9979141  -0.99752337]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99844617 -0.9979141  -0.99752337]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99844617 -0.9979141  -0.99752337]], Number:874\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99914294 -0.9954195  -0.99715245]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99914294 -0.9954195  -0.99715245]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99914294 -0.9954195  -0.99715245]], Number:875\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99926054 -0.99406415 -0.9979129 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99926054 -0.99406415 -0.9979129 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99926054 -0.99406415 -0.9979129 ]], Number:876\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970124  -0.99646795 -0.9946756 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970124  -0.99646795 -0.9946756 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9970124  -0.99646795 -0.9946756 ]], Number:877\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99306315 -0.99245894 -0.9968955 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99306315 -0.99245894 -0.9968955 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99306315 -0.99245894 -0.9968955 ]], Number:878\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99327797 -0.99376005 -0.9951995 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99327797 -0.99376005 -0.9951995 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99327797 -0.99376005 -0.9951995 ]], Number:879\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99668497 -0.99979866 -0.996826  ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99668497 -0.99979866 -0.996826  ]], Number:880\n",
      "-------------------------\n",
      "Episode 44\tFrame 880 \tAverage100 Score: -194.46tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921328  -0.99775755 -0.9921985 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921328  -0.99775755 -0.9921985 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921328  -0.99775755 -0.9921985 ]], Number:881\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99340934 -0.99369603 -0.99266684]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99340934 -0.99369603 -0.99266684]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99340934 -0.99369603 -0.99266684]], Number:882\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99590826 -0.9931118  -0.99533933]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99590826 -0.9931118  -0.99533933]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99590826 -0.9931118  -0.99533933]], Number:883\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938939  -0.9994329  -0.99938905]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938939  -0.9994329  -0.99938905]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938939  -0.9994329  -0.99938905]], Number:884\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99411887 -0.9990075  -0.99707305]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99411887 -0.9990075  -0.99707305]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99411887 -0.9990075  -0.99707305]], Number:885\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996614  -0.9926432  -0.99324715]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996614  -0.9926432  -0.99324715]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996614  -0.9926432  -0.99324715]], Number:886\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923316  -0.99237436 -0.99596953]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923316  -0.99237436 -0.99596953]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923316  -0.99237436 -0.99596953]], Number:887\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99707556 -0.9977327  -0.996295  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99707556 -0.9977327  -0.996295  ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99707556 -0.9977327  -0.996295  ]], Number:888\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974292  -0.9942207  -0.99801195]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974292  -0.9942207  -0.99801195]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974292  -0.9942207  -0.99801195]], Number:889\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99258566 -0.9957777  -0.9990402 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99258566 -0.9957777  -0.9990402 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99258566 -0.9957777  -0.9990402 ]], Number:890\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975019  -0.99859023 -0.9952299 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975019  -0.99859023 -0.9952299 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9975019  -0.99859023 -0.9952299 ]], Number:891\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942552 -0.9973354 -0.9935886]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942552 -0.9973354 -0.9935886]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9942552 -0.9973354 -0.9935886]], Number:892\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989191 -0.9952726 -0.9995926]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989191 -0.9952726 -0.9995926]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9989191 -0.9952726 -0.9995926]], Number:893\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975892 -0.9949895 -0.9946214]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975892 -0.9949895 -0.9946214]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9975892 -0.9949895 -0.9946214]], Number:894\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99634707 -0.9938011  -0.99673   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99634707 -0.9938011  -0.99673   ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99634707 -0.9938011  -0.99673   ]], Number:895\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99533886 -0.9990705  -0.9943695 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99533886 -0.9990705  -0.9943695 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99533886 -0.9990705  -0.9943695 ]], Number:896\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969442  -0.9953535  -0.99610615]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969442  -0.9953535  -0.99610615]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9969442  -0.9953535  -0.99610615]], Number:897\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982307 -0.9934604 -0.995211 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982307 -0.9934604 -0.995211 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9982307 -0.9934604 -0.995211 ]], Number:898\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962921  -0.99979097 -0.99936587]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962921  -0.99979097 -0.99936587]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9962921  -0.99979097 -0.99936587]], Number:899\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937046  -0.99643946 -0.9980584 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9937046  -0.99643946 -0.9980584 ]], Number:900\n",
      "-------------------------\n",
      "Episode 45\tFrame 900 \tAverage100 Score: -194.58tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934313  -0.9995525  -0.99698836]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934313  -0.9995525  -0.99698836]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934313  -0.9995525  -0.99698836]], Number:901\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9983458 -0.9923334 -0.9968741]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983458 -0.9923334 -0.9968741]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9983458 -0.9923334 -0.9968741]], Number:902\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99440855 -0.99930704 -0.99637115]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99440855 -0.99930704 -0.99637115]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99440855 -0.99930704 -0.99637115]], Number:903\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99936104 -0.9954156  -0.9935308 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99936104 -0.9954156  -0.9935308 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99936104 -0.9954156  -0.9935308 ]], Number:904\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948899 -0.9957886 -0.9938771]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948899 -0.9957886 -0.9938771]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948899 -0.9957886 -0.9938771]], Number:905\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99531305 -0.9995758  -0.9947556 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99531305 -0.9995758  -0.9947556 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99531305 -0.9995758  -0.9947556 ]], Number:906\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99394953 -0.9950348  -0.9956044 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99394953 -0.9950348  -0.9956044 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99394953 -0.9950348  -0.9956044 ]], Number:907\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921669  -0.99739677 -0.99353415]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921669  -0.99739677 -0.99353415]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921669  -0.99739677 -0.99353415]], Number:908\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9958087 -0.996314  -0.9968023]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9958087 -0.996314  -0.9968023]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9958087 -0.996314  -0.9968023]], Number:909\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993495 -0.9945216 -0.9954835]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993495 -0.9945216 -0.9954835]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993495 -0.9945216 -0.9954835]], Number:910\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993806   -0.9924539  -0.99206585]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993806   -0.9924539  -0.99206585]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.993806   -0.9924539  -0.99206585]], Number:911\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99639356 -0.99762464 -0.99929214]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99639356 -0.99762464 -0.99929214]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99639356 -0.99762464 -0.99929214]], Number:912\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99411154 -0.9951606  -0.9939606 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99411154 -0.9951606  -0.9939606 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99411154 -0.9951606  -0.9939606 ]], Number:913\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923     -0.9980248  -0.99871534]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923     -0.9980248  -0.99871534]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9923     -0.9980248  -0.99871534]], Number:914\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99910563 -0.99882114 -0.9928607 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99910563 -0.99882114 -0.9928607 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99910563 -0.99882114 -0.9928607 ]], Number:915\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99846214 -0.9974842  -0.99557555]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99846214 -0.9974842  -0.99557555]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99846214 -0.9974842  -0.99557555]], Number:916\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998376 -0.9949545 -0.9950277]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9998376 -0.9949545 -0.9950277]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9998376 -0.9949545 -0.9950277]], Number:917\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999369  -0.99910027 -0.9958769 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999369  -0.99910027 -0.9958769 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9999369  -0.99910027 -0.9958769 ]], Number:918\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992652  -0.9962425 -0.998716 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992652  -0.9962425 -0.998716 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.992652  -0.9962425 -0.998716 ]], Number:919\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9990976  -0.99883693 -0.9983729 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9990976  -0.99883693 -0.9983729 ]], Number:920\n",
      "-------------------------\n",
      "Episode 46\tFrame 920 \tAverage100 Score: -194.70tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99963504 -0.9941838  -0.9940911 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99963504 -0.9941838  -0.9940911 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99963504 -0.9941838  -0.9940911 ]], Number:921\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99529237 -0.99926543 -0.9948118 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99529237 -0.99926543 -0.9948118 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99529237 -0.99926543 -0.9948118 ]], Number:922\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968885  -0.99714136 -0.9946968 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968885  -0.99714136 -0.9946968 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9968885  -0.99714136 -0.9946968 ]], Number:923\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99685955 -0.9997754  -0.9995754 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99685955 -0.9997754  -0.9995754 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99685955 -0.9997754  -0.9995754 ]], Number:924\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922154 -0.9927417 -0.9943896]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922154 -0.9927417 -0.9943896]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922154 -0.9927417 -0.9943896]], Number:925\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932521  -0.99476725 -0.99741757]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932521  -0.99476725 -0.99741757]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932521  -0.99476725 -0.99741757]], Number:926\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99544406 -0.9966183  -0.9988545 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99544406 -0.9966183  -0.9988545 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99544406 -0.9966183  -0.9988545 ]], Number:927\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988696 -0.9929654 -0.9959839]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988696 -0.9929654 -0.9959839]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988696 -0.9929654 -0.9959839]], Number:928\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946067  -0.9979897  -0.99793136]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946067  -0.9979897  -0.99793136]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9946067  -0.9979897  -0.99793136]], Number:929\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994256  -0.9962792  -0.99259645]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994256  -0.9962792  -0.99259645]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994256  -0.9962792  -0.99259645]], Number:930\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927831  -0.9951178  -0.99331254]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927831  -0.9951178  -0.99331254]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9927831  -0.9951178  -0.99331254]], Number:931\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961502 -0.9942571 -0.9951761]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961502 -0.9942571 -0.9951761]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9961502 -0.9942571 -0.9951761]], Number:932\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9960338  -0.9987691  -0.99566823]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9960338  -0.9987691  -0.99566823]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9960338  -0.9987691  -0.99566823]], Number:933\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976197  -0.99347705 -0.9935512 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976197  -0.99347705 -0.9935512 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9976197  -0.99347705 -0.9935512 ]], Number:934\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941522 -0.996955  -0.9984655]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941522 -0.996955  -0.9984655]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941522 -0.996955  -0.9984655]], Number:935\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998762 -0.9986738 -0.9983769]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9998762 -0.9986738 -0.9983769]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9998762 -0.9986738 -0.9983769]], Number:936\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966082  -0.9999947  -0.99946624]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966082  -0.9999947  -0.99946624]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966082  -0.9999947  -0.99946624]], Number:937\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995936  -0.99614626 -0.9947149 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995936  -0.99614626 -0.9947149 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9995936  -0.99614626 -0.9947149 ]], Number:938\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99383014 -0.99305207 -0.999767  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99383014 -0.99305207 -0.999767  ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99383014 -0.99305207 -0.999767  ]], Number:939\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99603564 -0.9959658  -0.9960425 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99603564 -0.9959658  -0.9960425 ]], Number:940\n",
      "-------------------------\n",
      "Episode 47\tFrame 940 \tAverage100 Score: -194.81tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992286 -0.9986121 -0.9936337]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992286 -0.9986121 -0.9936337]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992286 -0.9986121 -0.9936337]], Number:941\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99527156 -0.99793214 -0.9959569 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99527156 -0.99793214 -0.9959569 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99527156 -0.99793214 -0.9959569 ]], Number:942\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934045 -0.9957981 -0.9984788]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934045 -0.9957981 -0.9984788]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9934045 -0.9957981 -0.9984788]], Number:943\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99691576 -0.99269205 -0.99307716]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99691576 -0.99269205 -0.99307716]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99691576 -0.99269205 -0.99307716]], Number:944\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99510056 -0.99567306 -0.9990082 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99510056 -0.99567306 -0.9990082 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99510056 -0.99567306 -0.9990082 ]], Number:945\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936458 -0.9980851 -0.9938715]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936458 -0.9980851 -0.9938715]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9936458 -0.9980851 -0.9938715]], Number:946\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99587166 -0.9935221  -0.99705863]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99587166 -0.9935221  -0.99705863]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99587166 -0.9935221  -0.99705863]], Number:947\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99662775 -0.9981105  -0.99501735]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99662775 -0.9981105  -0.99501735]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99662775 -0.9981105  -0.99501735]], Number:948\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995468  -0.9957862 -0.9984256]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995468  -0.9957862 -0.9984256]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995468  -0.9957862 -0.9984256]], Number:949\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99258006 -0.9981482  -0.997651  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99258006 -0.9981482  -0.997651  ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99258006 -0.9981482  -0.997651  ]], Number:950\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992888  -0.99587905 -0.99947184]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992888  -0.99587905 -0.99947184]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9992888  -0.99587905 -0.99947184]], Number:951\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938411 -0.999     -0.9930312]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938411 -0.999     -0.9930312]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9938411 -0.999     -0.9930312]], Number:952\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9952505 -0.9963289 -0.9931906]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952505 -0.9963289 -0.9931906]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9952505 -0.9963289 -0.9931906]], Number:953\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99306446 -0.9984085  -0.99670494]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99306446 -0.9984085  -0.99670494]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99306446 -0.9984085  -0.99670494]], Number:954\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99897677 -0.99918705 -0.9928647 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99897677 -0.99918705 -0.9928647 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99897677 -0.99918705 -0.9928647 ]], Number:955\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935966 -0.9975075 -0.9928008]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9935966 -0.9975075 -0.9928008]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9935966 -0.9975075 -0.9928008]], Number:956\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955104 -0.9950443 -0.9928671]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955104 -0.9950443 -0.9928671]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955104 -0.9950443 -0.9928671]], Number:957\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99302244 -0.99995893 -0.99430865]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99302244 -0.99995893 -0.99430865]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99302244 -0.99995893 -0.99430865]], Number:958\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997982  -0.9976928  -0.99880743]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997982  -0.9976928  -0.99880743]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9997982  -0.9976928  -0.99880743]], Number:959\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99326    -0.99906576 -0.9925258 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99326    -0.99906576 -0.9925258 ]], Number:960\n",
      "-------------------------\n",
      "Episode 48\tFrame 960 \tAverage100 Score: -194.92tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99947166 -0.9993096  -0.99377   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99947166 -0.9993096  -0.99377   ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99947166 -0.9993096  -0.99377   ]], Number:961\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970568 -0.9968923 -0.9962449]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970568 -0.9968923 -0.9962449]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9970568 -0.9968923 -0.9962449]], Number:962\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992948   -0.99739534 -0.9936244 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992948   -0.99739534 -0.9936244 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.992948   -0.99739534 -0.9936244 ]], Number:963\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949148  -0.9962357  -0.99323094]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949148  -0.9962357  -0.99323094]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9949148  -0.9962357  -0.99323094]], Number:964\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99311686 -0.99515486 -0.99878013]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99311686 -0.99515486 -0.99878013]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99311686 -0.99515486 -0.99878013]], Number:965\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99259096 -0.99818426 -0.99234587]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99259096 -0.99818426 -0.99234587]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99259096 -0.99818426 -0.99234587]], Number:966\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995533   -0.99297994 -0.9947934 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995533   -0.99297994 -0.9947934 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995533   -0.99297994 -0.9947934 ]], Number:967\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99437875 -0.99348736 -0.99577516]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99437875 -0.99348736 -0.99577516]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99437875 -0.99348736 -0.99577516]], Number:968\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99659926 -0.9932004  -0.9997129 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99659926 -0.9932004  -0.9997129 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99659926 -0.9932004  -0.9997129 ]], Number:969\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921563  -0.9974576  -0.99532205]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921563  -0.9974576  -0.99532205]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921563  -0.9974576  -0.99532205]], Number:970\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99995154 -0.9997683  -0.9985276 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99995154 -0.9997683  -0.9985276 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99995154 -0.9997683  -0.9985276 ]], Number:971\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99979335 -0.9947305  -0.999839  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99979335 -0.9947305  -0.999839  ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99979335 -0.9947305  -0.999839  ]], Number:972\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99448127 -0.995597   -0.9958046 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99448127 -0.995597   -0.9958046 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99448127 -0.995597   -0.9958046 ]], Number:973\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988982  -0.9994917  -0.99621946]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988982  -0.9994917  -0.99621946]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9988982  -0.9994917  -0.99621946]], Number:974\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99943775 -0.9959591  -0.9934957 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99943775 -0.9959591  -0.9934957 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99943775 -0.9959591  -0.9934957 ]], Number:975\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945867  -0.99927574 -0.9926184 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945867  -0.99927574 -0.9926184 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9945867  -0.99927574 -0.9926184 ]], Number:976\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99608964 -0.99908763 -0.9920298 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99608964 -0.99908763 -0.9920298 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99608964 -0.99908763 -0.9920298 ]], Number:977\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934649  -0.99327284 -0.99675405]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934649  -0.99327284 -0.99675405]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9934649  -0.99327284 -0.99675405]], Number:978\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927254  -0.9995632  -0.99394137]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927254  -0.9995632  -0.99394137]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9927254  -0.9995632  -0.99394137]], Number:979\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99626786 -0.9933892  -0.9946629 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99626786 -0.9933892  -0.9946629 ]], Number:980\n",
      "-------------------------\n",
      "Episode 49\tFrame 980 \tAverage100 Score: -195.03tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934162  -0.9951854  -0.99846226]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934162  -0.9951854  -0.99846226]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934162  -0.9951854  -0.99846226]], Number:981\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921859 -0.9984807 -0.993424 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921859 -0.9984807 -0.993424 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921859 -0.9984807 -0.993424 ]], Number:982\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99638426 -0.9927842  -0.99625593]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99638426 -0.9927842  -0.99625593]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99638426 -0.9927842  -0.99625593]], Number:983\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99557483 -0.9932116  -0.99388194]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99557483 -0.9932116  -0.99388194]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99557483 -0.9932116  -0.99388194]], Number:984\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99377686 -0.995732   -0.9944518 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99377686 -0.995732   -0.9944518 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99377686 -0.995732   -0.9944518 ]], Number:985\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964768 -0.9924705 -0.9980728]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964768 -0.9924705 -0.9980728]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9964768 -0.9924705 -0.9980728]], Number:986\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964977 -0.9988126 -0.9985545]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964977 -0.9988126 -0.9985545]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9964977 -0.9988126 -0.9985545]], Number:987\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99962544 -0.9967879  -0.99916816]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99962544 -0.9967879  -0.99916816]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99962544 -0.9967879  -0.99916816]], Number:988\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962697  -0.9996024  -0.99284714]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962697  -0.9996024  -0.99284714]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962697  -0.9996024  -0.99284714]], Number:989\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997629  -0.99809986 -0.9994851 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997629  -0.99809986 -0.9994851 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997629  -0.99809986 -0.9994851 ]], Number:990\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99509376 -0.9963889  -0.9933634 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99509376 -0.9963889  -0.9933634 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99509376 -0.9963889  -0.9933634 ]], Number:991\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995893  -0.9969836  -0.99636537]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995893  -0.9969836  -0.99636537]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9995893  -0.9969836  -0.99636537]], Number:992\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9958723  -0.99664956 -0.99244976]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9958723  -0.99664956 -0.99244976]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9958723  -0.99664956 -0.99244976]], Number:993\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936977  -0.9932687  -0.99942845]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936977  -0.9932687  -0.99942845]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9936977  -0.9932687  -0.99942845]], Number:994\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985133  -0.9946259  -0.99916303]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985133  -0.9946259  -0.99916303]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985133  -0.9946259  -0.99916303]], Number:995\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99424285 -0.99771637 -0.99404293]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99424285 -0.99771637 -0.99404293]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99424285 -0.99771637 -0.99404293]], Number:996\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975114  -0.99612826 -0.9962529 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975114  -0.99612826 -0.9962529 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975114  -0.99612826 -0.9962529 ]], Number:997\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99968123 -0.9966509  -0.9946445 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99968123 -0.9966509  -0.9946445 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99968123 -0.9966509  -0.9946445 ]], Number:998\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990232  -0.99208283 -0.99886715]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990232  -0.99208283 -0.99886715]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9990232  -0.99208283 -0.99886715]], Number:999\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99435896 -0.9945687  -0.9966593 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99435896 -0.9945687  -0.9966593 ]], Number:1000\n",
      "-------------------------\n",
      "Episode 50\tFrame 1000 \tAverage100 Score: -195.13tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99727505 -0.9990382  -0.99854064]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99727505 -0.9990382  -0.99854064]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99727505 -0.9990382  -0.99854064]], Number:1001\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99525183 -0.99663275 -0.9980395 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99525183 -0.99663275 -0.9980395 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99525183 -0.99663275 -0.9980395 ]], Number:1002\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940692 -0.9998031 -0.9969244]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9940692 -0.9998031 -0.9969244]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9940692 -0.9998031 -0.9969244]], Number:1003\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999566   -0.9987758  -0.99976796]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999566   -0.9987758  -0.99976796]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999566   -0.9987758  -0.99976796]], Number:1004\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995779   -0.9948786  -0.99708253]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995779   -0.9948786  -0.99708253]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995779   -0.9948786  -0.99708253]], Number:1005\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99414134 -0.99323845 -0.9959323 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99414134 -0.99323845 -0.9959323 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99414134 -0.99323845 -0.9959323 ]], Number:1006\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974523  -0.99366254 -0.9992597 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974523  -0.99366254 -0.9992597 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974523  -0.99366254 -0.9992597 ]], Number:1007\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99233556 -0.99374443 -0.995334  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99233556 -0.99374443 -0.995334  ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99233556 -0.99374443 -0.995334  ]], Number:1008\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977619  -0.99966323 -0.99326897]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977619  -0.99966323 -0.99326897]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9977619  -0.99966323 -0.99326897]], Number:1009\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99470866 -0.99495214 -0.99660754]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99470866 -0.99495214 -0.99660754]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99470866 -0.99495214 -0.99660754]], Number:1010\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939111  -0.99511224 -0.9952138 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939111  -0.99511224 -0.9952138 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9939111  -0.99511224 -0.9952138 ]], Number:1011\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99547356 -0.9941962  -0.99737394]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99547356 -0.9941962  -0.99737394]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99547356 -0.9941962  -0.99737394]], Number:1012\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997437  -0.99350035 -0.9995667 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997437  -0.99350035 -0.9995667 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997437  -0.99350035 -0.9995667 ]], Number:1013\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997933   -0.99264187 -0.99279624]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997933   -0.99264187 -0.99279624]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.997933   -0.99264187 -0.99279624]], Number:1014\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99685264 -0.99630094 -0.9987025 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99685264 -0.99630094 -0.9987025 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99685264 -0.99630094 -0.9987025 ]], Number:1015\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999062   -0.9935064  -0.99378365]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999062   -0.9935064  -0.99378365]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.999062   -0.9935064  -0.99378365]], Number:1016\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9931226 -0.9940789 -0.9977236]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9931226 -0.9940789 -0.9977236]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9931226 -0.9940789 -0.9977236]], Number:1017\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99862945 -0.99623424 -0.99375325]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99862945 -0.99623424 -0.99375325]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99862945 -0.99623424 -0.99375325]], Number:1018\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99638695 -0.9992343  -0.99245006]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99638695 -0.9992343  -0.99245006]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99638695 -0.9992343  -0.99245006]], Number:1019\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99303275 -0.9949237  -0.9931265 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99303275 -0.9949237  -0.9931265 ]], Number:1020\n",
      "-------------------------\n",
      "Episode 51\tFrame 1020 \tAverage100 Score: -195.22tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985445 -0.9981967 -0.9962712]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985445 -0.9981967 -0.9962712]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985445 -0.9981967 -0.9962712]], Number:1021\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99422425 -0.9982185  -0.99622035]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99422425 -0.9982185  -0.99622035]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99422425 -0.9982185  -0.99622035]], Number:1022\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955406 -0.9929949 -0.9987869]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955406 -0.9929949 -0.9987869]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9955406 -0.9929949 -0.9987869]], Number:1023\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99775636 -0.9967746  -0.9991014 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99775636 -0.9967746  -0.9991014 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99775636 -0.9967746  -0.9991014 ]], Number:1024\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993816   -0.99859875 -0.9950548 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993816   -0.99859875 -0.9950548 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993816   -0.99859875 -0.9950548 ]], Number:1025\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963023  -0.9979379  -0.99836206]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963023  -0.9979379  -0.99836206]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9963023  -0.9979379  -0.99836206]], Number:1026\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99931395 -0.9923807  -0.9939216 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99931395 -0.9923807  -0.9939216 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99931395 -0.9923807  -0.9939216 ]], Number:1027\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99789035 -0.9927159  -0.9975484 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99789035 -0.9927159  -0.9975484 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99789035 -0.9927159  -0.9975484 ]], Number:1028\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974509 -0.9952231 -0.9997319]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974509 -0.9952231 -0.9997319]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974509 -0.9952231 -0.9997319]], Number:1029\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948583 -0.9969956 -0.9928344]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948583 -0.9969956 -0.9928344]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948583 -0.9969956 -0.9928344]], Number:1030\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99976736 -0.99342847 -0.9974547 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99976736 -0.99342847 -0.9974547 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99976736 -0.99342847 -0.9974547 ]], Number:1031\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99574345 -0.99930084 -0.9996673 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99574345 -0.99930084 -0.9996673 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99574345 -0.99930084 -0.9996673 ]], Number:1032\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923105  -0.99319595 -0.99571097]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9923105  -0.99319595 -0.99571097]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923105  -0.99319595 -0.99571097]], Number:1033\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999849  -0.99702185 -0.9985016 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999849  -0.99702185 -0.9985016 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9999849  -0.99702185 -0.9985016 ]], Number:1034\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922634  -0.99265707 -0.9979864 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922634  -0.99265707 -0.9979864 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922634  -0.99265707 -0.9979864 ]], Number:1035\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99280685 -0.99449414 -0.99836445]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99280685 -0.99449414 -0.99836445]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99280685 -0.99449414 -0.99836445]], Number:1036\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962461 -0.9930289 -0.9993498]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962461 -0.9930289 -0.9993498]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962461 -0.9930289 -0.9993498]], Number:1037\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965346 -0.9937247 -0.9964674]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965346 -0.9937247 -0.9964674]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9965346 -0.9937247 -0.9964674]], Number:1038\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973008 -0.9973622 -0.9920092]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973008 -0.9973622 -0.9920092]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9973008 -0.9973622 -0.9920092]], Number:1039\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987683 -0.9947714 -0.9938467]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9987683 -0.9947714 -0.9938467]], Number:1040\n",
      "-------------------------\n",
      "Episode 52\tFrame 1040 \tAverage100 Score: -195.31tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961744  -0.99885803 -0.99692535]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961744  -0.99885803 -0.99692535]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9961744  -0.99885803 -0.99692535]], Number:1041\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993521  -0.99983084 -0.99656874]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993521  -0.99983084 -0.99656874]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993521  -0.99983084 -0.99656874]], Number:1042\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928433  -0.99910486 -0.99370104]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928433  -0.99910486 -0.99370104]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9928433  -0.99910486 -0.99370104]], Number:1043\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921698 -0.995986  -0.9934669]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921698 -0.995986  -0.9934669]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921698 -0.995986  -0.9934669]], Number:1044\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99520665 -0.99911076 -0.99615055]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99520665 -0.99911076 -0.99615055]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99520665 -0.99911076 -0.99615055]], Number:1045\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99302644 -0.99606955 -0.99732417]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99302644 -0.99606955 -0.99732417]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99302644 -0.99606955 -0.99732417]], Number:1046\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985818 -0.9948887 -0.9926865]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985818 -0.9948887 -0.9926865]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985818 -0.9948887 -0.9926865]], Number:1047\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927486  -0.99610585 -0.999483  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9927486  -0.99610585 -0.999483  ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927486  -0.99610585 -0.999483  ]], Number:1048\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970482  -0.9942079  -0.99498063]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970482  -0.9942079  -0.99498063]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9970482  -0.9942079  -0.99498063]], Number:1049\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994775  -0.99628407 -0.99318576]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994775  -0.99628407 -0.99318576]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994775  -0.99628407 -0.99318576]], Number:1050\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99839616 -0.992775   -0.9936311 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99839616 -0.992775   -0.9936311 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99839616 -0.992775   -0.9936311 ]], Number:1051\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940871 -0.9927454 -0.9964031]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940871 -0.9927454 -0.9964031]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9940871 -0.9927454 -0.9964031]], Number:1052\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99860317 -0.993463   -0.99682045]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99860317 -0.993463   -0.99682045]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99860317 -0.993463   -0.99682045]], Number:1053\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99944293 -0.9935533  -0.99915963]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99944293 -0.9935533  -0.99915963]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99944293 -0.9935533  -0.99915963]], Number:1054\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924443 -0.9921474 -0.9978402]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924443 -0.9921474 -0.9978402]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9924443 -0.9921474 -0.9978402]], Number:1055\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975711  -0.99242324 -0.9957244 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975711  -0.99242324 -0.9957244 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9975711  -0.99242324 -0.9957244 ]], Number:1056\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994082   -0.9984629  -0.99505067]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994082   -0.9984629  -0.99505067]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.994082   -0.9984629  -0.99505067]], Number:1057\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9978672  -0.99490595 -0.99757457]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978672  -0.99490595 -0.99757457]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9978672  -0.99490595 -0.99757457]], Number:1058\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977776 -0.9967933 -0.9965296]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977776 -0.9967933 -0.9965296]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9977776 -0.9967933 -0.9965296]], Number:1059\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964059 -0.9963372 -0.993373 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9964059 -0.9963372 -0.993373 ]], Number:1060\n",
      "-------------------------\n",
      "Episode 53\tFrame 1060 \tAverage100 Score: -195.40tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997412  -0.99453825 -0.9972981 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997412  -0.99453825 -0.9972981 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997412  -0.99453825 -0.9972981 ]], Number:1061\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985373  -0.99912477 -0.99824464]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985373  -0.99912477 -0.99824464]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985373  -0.99912477 -0.99824464]], Number:1062\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935338 -0.9931454 -0.9945101]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9935338 -0.9931454 -0.9945101]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9935338 -0.9931454 -0.9945101]], Number:1063\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926628 -0.9945018 -0.9958683]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926628 -0.9945018 -0.9958683]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9926628 -0.9945018 -0.9958683]], Number:1064\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932498  -0.99370307 -0.9998852 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9932498  -0.99370307 -0.9998852 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932498  -0.99370307 -0.9998852 ]], Number:1065\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929427  -0.99684876 -0.9952538 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929427  -0.99684876 -0.9952538 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929427  -0.99684876 -0.9952538 ]], Number:1066\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945875 -0.9945706 -0.9959126]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945875 -0.9945706 -0.9959126]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945875 -0.9945706 -0.9959126]], Number:1067\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99796075 -0.9995527  -0.9958829 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99796075 -0.9995527  -0.9958829 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99796075 -0.9995527  -0.9958829 ]], Number:1068\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99646723 -0.9954885  -0.9966975 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99646723 -0.9954885  -0.9966975 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99646723 -0.9954885  -0.9966975 ]], Number:1069\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950373  -0.99358946 -0.99771506]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950373  -0.99358946 -0.99771506]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950373  -0.99358946 -0.99771506]], Number:1070\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99812776 -0.9974524  -0.9962927 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99812776 -0.9974524  -0.9962927 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99812776 -0.9974524  -0.9962927 ]], Number:1071\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949676  -0.99302316 -0.9996582 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949676  -0.99302316 -0.9996582 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9949676  -0.99302316 -0.9996582 ]], Number:1072\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940807 -0.9947773 -0.9929275]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940807 -0.9947773 -0.9929275]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9940807 -0.9947773 -0.9929275]], Number:1073\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9920731  -0.99887985 -0.9920252 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920731  -0.99887985 -0.9920252 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9920731  -0.99887985 -0.9920252 ]], Number:1074\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963562  -0.99784184 -0.996931  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963562  -0.99784184 -0.996931  ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9963562  -0.99784184 -0.996931  ]], Number:1075\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99796915 -0.9940403  -0.99822927]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99796915 -0.9940403  -0.99822927]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99796915 -0.9940403  -0.99822927]], Number:1076\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991913  -0.99969596 -0.9930983 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991913  -0.99969596 -0.9930983 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991913  -0.99969596 -0.9930983 ]], Number:1077\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99272746 -0.9955547  -0.9997947 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99272746 -0.9955547  -0.9997947 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99272746 -0.9955547  -0.9997947 ]], Number:1078\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99729455 -0.99920446 -0.9937303 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99729455 -0.99920446 -0.9937303 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99729455 -0.99920446 -0.9937303 ]], Number:1079\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989115  -0.99766034 -0.99381065]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9989115  -0.99766034 -0.99381065]], Number:1080\n",
      "-------------------------\n",
      "Episode 54\tFrame 1080 \tAverage100 Score: -195.49tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99669385 -0.99835575 -0.9932665 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99669385 -0.99835575 -0.9932665 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99669385 -0.99835575 -0.9932665 ]], Number:1081\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99243975 -0.9967423  -0.9951486 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99243975 -0.9967423  -0.9951486 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99243975 -0.9967423  -0.9951486 ]], Number:1082\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99924475 -0.9971129  -0.9994169 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99924475 -0.9971129  -0.9994169 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99924475 -0.9971129  -0.9994169 ]], Number:1083\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99470377 -0.9927462  -0.99467486]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99470377 -0.9927462  -0.99467486]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99470377 -0.9927462  -0.99467486]], Number:1084\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946408 -0.999852  -0.9946055]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946408 -0.999852  -0.9946055]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9946408 -0.999852  -0.9946055]], Number:1085\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99700284 -0.99695665 -0.99432224]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99700284 -0.99695665 -0.99432224]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99700284 -0.99695665 -0.99432224]], Number:1086\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982497 -0.9930022 -0.9985944]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9982497 -0.9930022 -0.9985944]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9982497 -0.9930022 -0.9985944]], Number:1087\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945793  -0.99505377 -0.99624467]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945793  -0.99505377 -0.99624467]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945793  -0.99505377 -0.99624467]], Number:1088\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99672836 -0.99937946 -0.99608284]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99672836 -0.99937946 -0.99608284]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99672836 -0.99937946 -0.99608284]], Number:1089\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99689376 -0.9985613  -0.9998483 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99689376 -0.9985613  -0.9998483 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99689376 -0.9985613  -0.9998483 ]], Number:1090\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99681425 -0.99530536 -0.99727815]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99681425 -0.99530536 -0.99727815]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99681425 -0.99530536 -0.99727815]], Number:1091\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930841  -0.9955474  -0.99419004]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930841  -0.9955474  -0.99419004]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9930841  -0.9955474  -0.99419004]], Number:1092\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948437  -0.99672043 -0.9940681 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948437  -0.99672043 -0.9940681 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948437  -0.99672043 -0.9940681 ]], Number:1093\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975679  -0.99824685 -0.99738884]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975679  -0.99824685 -0.99738884]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9975679  -0.99824685 -0.99738884]], Number:1094\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99464166 -0.9990345  -0.99897116]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99464166 -0.9990345  -0.99897116]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99464166 -0.9990345  -0.99897116]], Number:1095\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940154  -0.99621475 -0.9979049 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940154  -0.99621475 -0.9979049 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9940154  -0.99621475 -0.9979049 ]], Number:1096\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930449  -0.99286956 -0.9989502 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930449  -0.99286956 -0.9989502 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930449  -0.99286956 -0.9989502 ]], Number:1097\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99639124 -0.9937576  -0.99355197]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99639124 -0.9937576  -0.99355197]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99639124 -0.9937576  -0.99355197]], Number:1098\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991629  -0.99732447 -0.99601823]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991629  -0.99732447 -0.99601823]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9991629  -0.99732447 -0.99601823]], Number:1099\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99971735 -0.9928401  -0.9978767 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99971735 -0.9928401  -0.9978767 ]], Number:1100\n",
      "-------------------------\n",
      "Episode 55\tFrame 1100 \tAverage100 Score: -195.57tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972111 -0.9977768 -0.9973714]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972111 -0.9977768 -0.9973714]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9972111 -0.9977768 -0.9973714]], Number:1101\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99456304 -0.9936465  -0.9958075 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99456304 -0.9936465  -0.9958075 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99456304 -0.9936465  -0.9958075 ]], Number:1102\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9981352 -0.9921946 -0.9981307]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9981352 -0.9921946 -0.9981307]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9981352 -0.9921946 -0.9981307]], Number:1103\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953577  -0.99303395 -0.9956458 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953577  -0.99303395 -0.9956458 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9953577  -0.99303395 -0.9956458 ]], Number:1104\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966101 -0.9938715 -0.993447 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966101 -0.9938715 -0.993447 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966101 -0.9938715 -0.993447 ]], Number:1105\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993945 -0.9982676 -0.9970084]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993945 -0.9982676 -0.9970084]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993945 -0.9982676 -0.9970084]], Number:1106\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99226826 -0.99814445 -0.99700904]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99226826 -0.99814445 -0.99700904]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99226826 -0.99814445 -0.99700904]], Number:1107\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936065  -0.99483544 -0.99960524]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936065  -0.99483544 -0.99960524]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9936065  -0.99483544 -0.99960524]], Number:1108\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999362   -0.99327856 -0.9986694 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999362   -0.99327856 -0.9986694 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999362   -0.99327856 -0.9986694 ]], Number:1109\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992316 -0.9985646 -0.9994087]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992316 -0.9985646 -0.9994087]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992316 -0.9985646 -0.9994087]], Number:1110\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99402916 -0.999221   -0.99269146]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99402916 -0.999221   -0.99269146]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99402916 -0.999221   -0.99269146]], Number:1111\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99488133 -0.9933571  -0.99759924]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99488133 -0.9933571  -0.99759924]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99488133 -0.9933571  -0.99759924]], Number:1112\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928776 -0.9997188 -0.9955889]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928776 -0.9997188 -0.9955889]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9928776 -0.9997188 -0.9955889]], Number:1113\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99968255 -0.9921471  -0.9939775 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99968255 -0.9921471  -0.9939775 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99968255 -0.9921471  -0.9939775 ]], Number:1114\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99538606 -0.99352473 -0.9949805 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99538606 -0.99352473 -0.9949805 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99538606 -0.99352473 -0.9949805 ]], Number:1115\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950925 -0.9974479 -0.9986622]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950925 -0.9974479 -0.9986622]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9950925 -0.9974479 -0.9986622]], Number:1116\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9952554  -0.99758804 -0.99896616]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952554  -0.99758804 -0.99896616]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9952554  -0.99758804 -0.99896616]], Number:1117\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99822956 -0.99851173 -0.99405175]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99822956 -0.99851173 -0.99405175]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99822956 -0.99851173 -0.99405175]], Number:1118\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930945 -0.9969809 -0.9992247]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9930945 -0.9969809 -0.9992247]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9930945 -0.9969809 -0.9992247]], Number:1119\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99668294 -0.9998899  -0.99460554]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99668294 -0.9998899  -0.99460554]], Number:1120\n",
      "-------------------------\n",
      "Episode 56\tFrame 1120 \tAverage100 Score: -195.65tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99777967 -0.9924457  -0.99750537]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99777967 -0.9924457  -0.99750537]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99777967 -0.9924457  -0.99750537]], Number:1121\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922841  -0.99719507 -0.9982748 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922841  -0.99719507 -0.9982748 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922841  -0.99719507 -0.9982748 ]], Number:1122\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99702567 -0.99506354 -0.9970332 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99702567 -0.99506354 -0.9970332 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99702567 -0.99506354 -0.9970332 ]], Number:1123\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997002  -0.99889797 -0.9979521 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997002  -0.99889797 -0.9979521 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997002  -0.99889797 -0.9979521 ]], Number:1124\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975137  -0.99263984 -0.99471045]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975137  -0.99263984 -0.99471045]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975137  -0.99263984 -0.99471045]], Number:1125\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99472666 -0.9963887  -0.9990246 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99472666 -0.9963887  -0.9990246 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99472666 -0.9963887  -0.9990246 ]], Number:1126\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957812 -0.9957207 -0.9932273]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957812 -0.9957207 -0.9932273]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9957812 -0.9957207 -0.9932273]], Number:1127\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991184  -0.9938537  -0.99497235]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991184  -0.9938537  -0.99497235]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991184  -0.9938537  -0.99497235]], Number:1128\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99720174 -0.9955869  -0.99408907]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99720174 -0.9955869  -0.99408907]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99720174 -0.9955869  -0.99408907]], Number:1129\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99354506 -0.99255174 -0.99298525]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99354506 -0.99255174 -0.99298525]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99354506 -0.99255174 -0.99298525]], Number:1130\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956901  -0.9979014  -0.99787945]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956901  -0.9979014  -0.99787945]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9956901  -0.9979014  -0.99787945]], Number:1131\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964313 -0.9940823 -0.9978418]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964313 -0.9940823 -0.9978418]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9964313 -0.9940823 -0.9978418]], Number:1132\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996052  -0.99502033 -0.993895  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996052  -0.99502033 -0.993895  ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996052  -0.99502033 -0.993895  ]], Number:1133\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984887  -0.993882   -0.99774504]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984887  -0.993882   -0.99774504]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9984887  -0.993882   -0.99774504]], Number:1134\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9986492  -0.9957441  -0.99407285]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9986492  -0.9957441  -0.99407285]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9986492  -0.9957441  -0.99407285]], Number:1135\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984017 -0.9975614 -0.9933078]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984017 -0.9975614 -0.9933078]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9984017 -0.9975614 -0.9933078]], Number:1136\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99537927 -0.9971053  -0.9929705 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99537927 -0.9971053  -0.9929705 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99537927 -0.9971053  -0.9929705 ]], Number:1137\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957191  -0.9992614  -0.99247885]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957191  -0.9992614  -0.99247885]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9957191  -0.9992614  -0.99247885]], Number:1138\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940639  -0.99791884 -0.9978269 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940639  -0.99791884 -0.9978269 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9940639  -0.99791884 -0.9978269 ]], Number:1139\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968804  -0.9924856  -0.99375355]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968804  -0.9924856  -0.99375355]], Number:1140\n",
      "-------------------------\n",
      "Episode 57\tFrame 1140 \tAverage100 Score: -195.72tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99852866 -0.9931698  -0.99320644]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99852866 -0.9931698  -0.99320644]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99852866 -0.9931698  -0.99320644]], Number:1141\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99811214 -0.99741256 -0.9985309 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99811214 -0.99741256 -0.9985309 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99811214 -0.99741256 -0.9985309 ]], Number:1142\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99450773 -0.99875027 -0.992956  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99450773 -0.99875027 -0.992956  ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99450773 -0.99875027 -0.992956  ]], Number:1143\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99746364 -0.996184   -0.9973478 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99746364 -0.996184   -0.9973478 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99746364 -0.996184   -0.9973478 ]], Number:1144\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939344  -0.99226594 -0.99713516]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939344  -0.99226594 -0.99713516]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939344  -0.99226594 -0.99713516]], Number:1145\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99349135 -0.99232155 -0.99239224]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99349135 -0.99232155 -0.99239224]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99349135 -0.99232155 -0.99239224]], Number:1146\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99866056 -0.9925053  -0.9948792 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99866056 -0.9925053  -0.9948792 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99866056 -0.9925053  -0.9948792 ]], Number:1147\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99929774 -0.99804735 -0.9968841 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99929774 -0.99804735 -0.9968841 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99929774 -0.99804735 -0.9968841 ]], Number:1148\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999931 -0.9945814 -0.9983848]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999931 -0.9945814 -0.9983848]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9999931 -0.9945814 -0.9983848]], Number:1149\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972821 -0.9988003 -0.9947294]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9972821 -0.9988003 -0.9947294]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9972821 -0.9988003 -0.9947294]], Number:1150\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963818  -0.9946412  -0.99756646]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963818  -0.9946412  -0.99756646]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9963818  -0.9946412  -0.99756646]], Number:1151\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99646133 -0.993826   -0.9966199 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99646133 -0.993826   -0.9966199 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99646133 -0.993826   -0.9966199 ]], Number:1152\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956615 -0.995536  -0.998858 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956615 -0.995536  -0.998858 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9956615 -0.995536  -0.998858 ]], Number:1153\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963334  -0.9960885  -0.99522203]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963334  -0.9960885  -0.99522203]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9963334  -0.9960885  -0.99522203]], Number:1154\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99643236 -0.99715066 -0.9941811 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99643236 -0.99715066 -0.9941811 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99643236 -0.99715066 -0.9941811 ]], Number:1155\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992348   -0.99758714 -0.9995293 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992348   -0.99758714 -0.9995293 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.992348   -0.99758714 -0.9995293 ]], Number:1156\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99567044 -0.9968785  -0.9927516 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99567044 -0.9968785  -0.9927516 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99567044 -0.9968785  -0.9927516 ]], Number:1157\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940194 -0.9932284 -0.9986626]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940194 -0.9932284 -0.9986626]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9940194 -0.9932284 -0.9986626]], Number:1158\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934    -0.9999247 -0.9961104]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934    -0.9999247 -0.9961104]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9934    -0.9999247 -0.9961104]], Number:1159\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997743  -0.9981351  -0.99976027]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997743  -0.9981351  -0.99976027]], Number:1160\n",
      "-------------------------\n",
      "Episode 58\tFrame 1160 \tAverage100 Score: -195.80tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943588  -0.99255085 -0.9976273 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943588  -0.99255085 -0.9976273 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9943588  -0.99255085 -0.9976273 ]], Number:1161\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99571717 -0.9997003  -0.9970799 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99571717 -0.9997003  -0.9970799 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99571717 -0.9997003  -0.9970799 ]], Number:1162\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99391454 -0.9979538  -0.9995002 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99391454 -0.9979538  -0.9995002 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99391454 -0.9979538  -0.9995002 ]], Number:1163\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991592 -0.9925206 -0.9975196]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991592 -0.9925206 -0.9975196]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991592 -0.9925206 -0.9975196]], Number:1164\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99795055 -0.9980023  -0.9971801 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99795055 -0.9980023  -0.9971801 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99795055 -0.9980023  -0.9971801 ]], Number:1165\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99475825 -0.99980795 -0.998954  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99475825 -0.99980795 -0.998954  ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99475825 -0.99980795 -0.998954  ]], Number:1166\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945724  -0.99684256 -0.9939941 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9945724  -0.99684256 -0.9939941 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945724  -0.99684256 -0.9939941 ]], Number:1167\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99521244 -0.99636537 -0.9946814 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99521244 -0.99636537 -0.9946814 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99521244 -0.99636537 -0.9946814 ]], Number:1168\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938655  -0.99940157 -0.99356407]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938655  -0.99940157 -0.99356407]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938655  -0.99940157 -0.99356407]], Number:1169\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933096  -0.9944857  -0.99721694]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933096  -0.9944857  -0.99721694]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9933096  -0.9944857  -0.99721694]], Number:1170\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944849  -0.9965167  -0.99847615]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944849  -0.9965167  -0.99847615]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9944849  -0.9965167  -0.99847615]], Number:1171\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972521  -0.99854773 -0.99975526]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972521  -0.99854773 -0.99975526]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9972521  -0.99854773 -0.99975526]], Number:1172\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99950147 -0.9969948  -0.9964708 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99950147 -0.9969948  -0.9964708 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99950147 -0.9969948  -0.9964708 ]], Number:1173\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961282 -0.9988725 -0.9954733]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961282 -0.9988725 -0.9954733]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9961282 -0.9988725 -0.9954733]], Number:1174\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926645 -0.9979008 -0.995661 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926645 -0.9979008 -0.995661 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9926645 -0.9979008 -0.995661 ]], Number:1175\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99248147 -0.9982582  -0.99636465]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99248147 -0.9982582  -0.99636465]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99248147 -0.9982582  -0.99636465]], Number:1176\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999177  -0.99623674 -0.9927707 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999177  -0.99623674 -0.9927707 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9999177  -0.99623674 -0.9927707 ]], Number:1177\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929309  -0.99721915 -0.99343044]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929309  -0.99721915 -0.99343044]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9929309  -0.99721915 -0.99343044]], Number:1178\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9920586  -0.99406576 -0.9981525 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920586  -0.99406576 -0.9981525 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9920586  -0.99406576 -0.9981525 ]], Number:1179\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955014  -0.99700403 -0.99945766]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955014  -0.99700403 -0.99945766]], Number:1180\n",
      "-------------------------\n",
      "Episode 59\tFrame 1180 \tAverage100 Score: -195.87tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926871 -0.9928373 -0.9960609]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926871 -0.9928373 -0.9960609]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9926871 -0.9928373 -0.9960609]], Number:1181\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99205893 -0.9992184  -0.99583584]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99205893 -0.9992184  -0.99583584]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99205893 -0.9992184  -0.99583584]], Number:1182\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987447 -0.9929795 -0.9986311]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987447 -0.9929795 -0.9986311]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9987447 -0.9929795 -0.9986311]], Number:1183\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957925 -0.994288  -0.9944315]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9957925 -0.994288  -0.9944315]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9957925 -0.994288  -0.9944315]], Number:1184\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922174 -0.9934941 -0.9968381]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922174 -0.9934941 -0.9968381]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922174 -0.9934941 -0.9968381]], Number:1185\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99249345 -0.9939276  -0.9989729 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99249345 -0.9939276  -0.9989729 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99249345 -0.9939276  -0.9989729 ]], Number:1186\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99800336 -0.9974356  -0.9930455 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99800336 -0.9974356  -0.9930455 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99800336 -0.9974356  -0.9930455 ]], Number:1187\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987743  -0.9938258  -0.99867225]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987743  -0.9938258  -0.99867225]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9987743  -0.9938258  -0.99867225]], Number:1188\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923122  -0.99204344 -0.9928937 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923122  -0.99204344 -0.9928937 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923122  -0.99204344 -0.9928937 ]], Number:1189\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966575  -0.99553335 -0.9956508 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966575  -0.99553335 -0.9956508 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966575  -0.99553335 -0.9956508 ]], Number:1190\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9981039  -0.9971025  -0.99880904]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9981039  -0.9971025  -0.99880904]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9981039  -0.9971025  -0.99880904]], Number:1191\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99602836 -0.9963369  -0.998857  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99602836 -0.9963369  -0.998857  ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99602836 -0.9963369  -0.998857  ]], Number:1192\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99907804 -0.9931141  -0.99281555]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99907804 -0.9931141  -0.99281555]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99907804 -0.9931141  -0.99281555]], Number:1193\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99350536 -0.9956511  -0.9976129 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99350536 -0.9956511  -0.9976129 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99350536 -0.9956511  -0.9976129 ]], Number:1194\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976486  -0.99947613 -0.99512047]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976486  -0.99947613 -0.99512047]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976486  -0.99947613 -0.99512047]], Number:1195\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9952941  -0.99716455 -0.9937554 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952941  -0.99716455 -0.9937554 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9952941  -0.99716455 -0.9937554 ]], Number:1196\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938804 -0.9944821 -0.999239 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938804 -0.9944821 -0.999239 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938804 -0.9944821 -0.999239 ]], Number:1197\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99730724 -0.9932214  -0.99542344]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99730724 -0.9932214  -0.99542344]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99730724 -0.9932214  -0.99542344]], Number:1198\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956119 -0.9950718 -0.9946147]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956119 -0.9950718 -0.9946147]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9956119 -0.9950718 -0.9946147]], Number:1199\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9930728 -0.9926364 -0.9987247]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930728 -0.9926364 -0.9987247]], Number:1200\n",
      "-------------------------\n",
      "Episode 60\tFrame 1200 \tAverage100 Score: -195.94tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99682295 -0.99450094 -0.9982955 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99682295 -0.99450094 -0.9982955 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99682295 -0.99450094 -0.9982955 ]], Number:1201\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982706  -0.99214613 -0.99731815]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982706  -0.99214613 -0.99731815]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9982706  -0.99214613 -0.99731815]], Number:1202\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984107  -0.99982715 -0.9968132 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984107  -0.99982715 -0.9968132 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9984107  -0.99982715 -0.9968132 ]], Number:1203\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99994785 -0.9957752  -0.9953606 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99994785 -0.9957752  -0.9953606 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99994785 -0.9957752  -0.9953606 ]], Number:1204\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99496204 -0.9950057  -0.9967329 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99496204 -0.9950057  -0.9967329 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99496204 -0.9950057  -0.9967329 ]], Number:1205\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99966234 -0.9961926  -0.99623823]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99966234 -0.9961926  -0.99623823]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99966234 -0.9961926  -0.99623823]], Number:1206\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99354184 -0.994276   -0.99773955]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99354184 -0.994276   -0.99773955]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99354184 -0.994276   -0.99773955]], Number:1207\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991666 -0.9923291 -0.9922706]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991666 -0.9923291 -0.9922706]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991666 -0.9923291 -0.9922706]], Number:1208\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99976   -0.9992771 -0.9970498]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99976   -0.9992771 -0.9970498]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99976   -0.9992771 -0.9970498]], Number:1209\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99883336 -0.9974558  -0.99934447]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99883336 -0.9974558  -0.99934447]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99883336 -0.9974558  -0.99934447]], Number:1210\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992726  -0.999653   -0.99558276]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992726  -0.999653   -0.99558276]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9992726  -0.999653   -0.99558276]], Number:1211\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939872  -0.99206424 -0.99479   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939872  -0.99206424 -0.99479   ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9939872  -0.99206424 -0.99479   ]], Number:1212\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9952422 -0.9960891 -0.9926521]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952422 -0.9960891 -0.9926521]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9952422 -0.9960891 -0.9926521]], Number:1213\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934871 -0.9941605 -0.995444 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934871 -0.9941605 -0.995444 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9934871 -0.9941605 -0.995444 ]], Number:1214\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99631757 -0.99785    -0.9974756 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99631757 -0.99785    -0.9974756 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99631757 -0.99785    -0.9974756 ]], Number:1215\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9960022  -0.99201304 -0.9959515 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9960022  -0.99201304 -0.9959515 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9960022  -0.99201304 -0.9959515 ]], Number:1216\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99334323 -0.9937973  -0.9944445 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99334323 -0.9937973  -0.9944445 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99334323 -0.9937973  -0.9944445 ]], Number:1217\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957129 -0.9972201 -0.9991013]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9957129 -0.9972201 -0.9991013]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9957129 -0.9972201 -0.9991013]], Number:1218\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990468  -0.99845976 -0.9974134 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990468  -0.99845976 -0.9974134 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9990468  -0.99845976 -0.9974134 ]], Number:1219\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983995  -0.9995533  -0.99325466]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9983995  -0.9995533  -0.99325466]], Number:1220\n",
      "-------------------------\n",
      "Episode 61\tFrame 1220 \tAverage100 Score: -196.00tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955803  -0.9978953  -0.99849147]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955803  -0.9978953  -0.99849147]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955803  -0.9978953  -0.99849147]], Number:1221\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941721  -0.9942423  -0.99528146]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941721  -0.9942423  -0.99528146]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941721  -0.9942423  -0.99528146]], Number:1222\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989624  -0.99273837 -0.99324846]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989624  -0.99273837 -0.99324846]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9989624  -0.99273837 -0.99324846]], Number:1223\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99809504 -0.9935454  -0.99396616]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99809504 -0.9935454  -0.99396616]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99809504 -0.9935454  -0.99396616]], Number:1224\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99800396 -0.9949582  -0.9945203 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99800396 -0.9949582  -0.9945203 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99800396 -0.9949582  -0.9945203 ]], Number:1225\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99529725 -0.9980924  -0.99610126]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99529725 -0.9980924  -0.99610126]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99529725 -0.9980924  -0.99610126]], Number:1226\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99480003 -0.99597067 -0.9982975 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99480003 -0.99597067 -0.9982975 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99480003 -0.99597067 -0.9982975 ]], Number:1227\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99494463 -0.99725455 -0.99578965]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99494463 -0.99725455 -0.99578965]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99494463 -0.99725455 -0.99578965]], Number:1228\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942658  -0.99527454 -0.99440074]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942658  -0.99527454 -0.99440074]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9942658  -0.99527454 -0.99440074]], Number:1229\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99400115 -0.9968934  -0.9926342 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99400115 -0.9968934  -0.9926342 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99400115 -0.9968934  -0.9926342 ]], Number:1230\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9920704  -0.9995959  -0.99722886]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920704  -0.9995959  -0.99722886]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9920704  -0.9995959  -0.99722886]], Number:1231\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955297 -0.995171  -0.9926973]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955297 -0.995171  -0.9926973]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9955297 -0.995171  -0.9926973]], Number:1232\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9967983  -0.9961622  -0.99201685]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9967983  -0.9961622  -0.99201685]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9967983  -0.9961622  -0.99201685]], Number:1233\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9971572 -0.9987317 -0.9942135]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9971572 -0.9987317 -0.9942135]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9971572 -0.9987317 -0.9942135]], Number:1234\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999404   -0.9971107  -0.99746233]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999404   -0.9971107  -0.99746233]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999404   -0.9971107  -0.99746233]], Number:1235\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9978463 -0.998025  -0.9923355]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978463 -0.998025  -0.9923355]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9978463 -0.998025  -0.9923355]], Number:1236\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9971698  -0.9977449  -0.99279505]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9971698  -0.9977449  -0.99279505]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9971698  -0.9977449  -0.99279505]], Number:1237\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930997 -0.9983791 -0.9986391]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930997 -0.9983791 -0.9986391]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9930997 -0.9983791 -0.9986391]], Number:1238\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995254   -0.99466443 -0.99598134]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995254   -0.99466443 -0.99598134]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.995254   -0.99466443 -0.99598134]], Number:1239\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939452  -0.9971151  -0.99744517]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939452  -0.9971151  -0.99744517]], Number:1240\n",
      "-------------------------\n",
      "Episode 62\tFrame 1240 \tAverage100 Score: -196.07tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966208 -0.9982923 -0.9960723]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966208 -0.9982923 -0.9960723]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966208 -0.9982923 -0.9960723]], Number:1241\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962037  -0.99611753 -0.99333495]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962037  -0.99611753 -0.99333495]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962037  -0.99611753 -0.99333495]], Number:1242\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99470687 -0.9982937  -0.9941517 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99470687 -0.9982937  -0.9941517 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99470687 -0.9982937  -0.9941517 ]], Number:1243\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943683  -0.99471426 -0.9956168 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943683  -0.99471426 -0.9956168 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9943683  -0.99471426 -0.9956168 ]], Number:1244\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99491316 -0.99933976 -0.995304  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99491316 -0.99933976 -0.995304  ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99491316 -0.99933976 -0.995304  ]], Number:1245\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991154  -0.99648345 -0.9939693 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991154  -0.99648345 -0.9939693 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991154  -0.99648345 -0.9939693 ]], Number:1246\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99917996 -0.99372536 -0.9971782 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99917996 -0.99372536 -0.9971782 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99917996 -0.99372536 -0.9971782 ]], Number:1247\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963985  -0.992054   -0.99376273]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963985  -0.992054   -0.99376273]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9963985  -0.992054   -0.99376273]], Number:1248\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975853  -0.9988634  -0.99334794]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975853  -0.9988634  -0.99334794]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975853  -0.9988634  -0.99334794]], Number:1249\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929195  -0.99212205 -0.9965888 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929195  -0.99212205 -0.9965888 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929195  -0.99212205 -0.9965888 ]], Number:1250\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998351 -0.9999133 -0.995363 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9998351 -0.9999133 -0.995363 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9998351 -0.9999133 -0.995363 ]], Number:1251\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9937631  -0.99458504 -0.9948499 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937631  -0.99458504 -0.9948499 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9937631  -0.99458504 -0.9948499 ]], Number:1252\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961078  -0.9937968  -0.99245036]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961078  -0.9937968  -0.99245036]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9961078  -0.9937968  -0.99245036]], Number:1253\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929506  -0.99548596 -0.99704367]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929506  -0.99548596 -0.99704367]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9929506  -0.99548596 -0.99704367]], Number:1254\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921455  -0.9923282  -0.99393034]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921455  -0.9923282  -0.99393034]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921455  -0.9923282  -0.99393034]], Number:1255\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973758  -0.99286443 -0.99486524]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973758  -0.99286443 -0.99486524]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9973758  -0.99286443 -0.99486524]], Number:1256\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953708 -0.9976929 -0.9949791]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953708 -0.9976929 -0.9949791]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9953708 -0.9976929 -0.9949791]], Number:1257\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943032  -0.99750966 -0.9955253 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943032  -0.99750966 -0.9955253 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9943032  -0.99750966 -0.9955253 ]], Number:1258\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924072  -0.99271417 -0.9991124 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924072  -0.99271417 -0.9991124 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9924072  -0.99271417 -0.9991124 ]], Number:1259\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99824464 -0.9949412  -0.99599206]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99824464 -0.9949412  -0.99599206]], Number:1260\n",
      "-------------------------\n",
      "Episode 63\tFrame 1260 \tAverage100 Score: -196.13tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99310535 -0.9982016  -0.99650687]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99310535 -0.9982016  -0.99650687]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99310535 -0.9982016  -0.99650687]], Number:1261\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968404 -0.9966404 -0.9958776]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968404 -0.9966404 -0.9958776]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968404 -0.9966404 -0.9958776]], Number:1262\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99714786 -0.9931395  -0.99443406]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99714786 -0.9931395  -0.99443406]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99714786 -0.9931395  -0.99443406]], Number:1263\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953667 -0.9967538 -0.9978987]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953667 -0.9967538 -0.9978987]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9953667 -0.9967538 -0.9978987]], Number:1264\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945425 -0.9961682 -0.9966621]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945425 -0.9961682 -0.9966621]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9945425 -0.9961682 -0.9966621]], Number:1265\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997039 -0.998266  -0.9953609]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997039 -0.998266  -0.9953609]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997039 -0.998266  -0.9953609]], Number:1266\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99584895 -0.99527246 -0.992348  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99584895 -0.99527246 -0.992348  ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99584895 -0.99527246 -0.992348  ]], Number:1267\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974167  -0.99291193 -0.9920233 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974167  -0.99291193 -0.9920233 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9974167  -0.99291193 -0.9920233 ]], Number:1268\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927374  -0.99992967 -0.992407  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927374  -0.99992967 -0.992407  ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927374  -0.99992967 -0.992407  ]], Number:1269\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99325174 -0.992638   -0.9983816 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99325174 -0.992638   -0.9983816 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99325174 -0.992638   -0.9983816 ]], Number:1270\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930797 -0.9964436 -0.9952476]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930797 -0.9964436 -0.9952476]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9930797 -0.9964436 -0.9952476]], Number:1271\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995738   -0.99578553 -0.9954142 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995738   -0.99578553 -0.9954142 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.995738   -0.99578553 -0.9954142 ]], Number:1272\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99517506 -0.9949715  -0.99320996]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99517506 -0.9949715  -0.99320996]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99517506 -0.9949715  -0.99320996]], Number:1273\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939669 -0.9973361 -0.9979985]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939669 -0.9973361 -0.9979985]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9939669 -0.9973361 -0.9979985]], Number:1274\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9958959 -0.9995711 -0.9966507]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9958959 -0.9995711 -0.9966507]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9958959 -0.9995711 -0.9966507]], Number:1275\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9986074 -0.99964   -0.9961479]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9986074 -0.99964   -0.9961479]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9986074 -0.99964   -0.9961479]], Number:1276\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988618  -0.99301434 -0.9960779 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988618  -0.99301434 -0.9960779 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988618  -0.99301434 -0.9960779 ]], Number:1277\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99746317 -0.9944704  -0.99880403]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99746317 -0.9944704  -0.99880403]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99746317 -0.9944704  -0.99880403]], Number:1278\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932244  -0.99271125 -0.9935582 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932244  -0.99271125 -0.9935582 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9932244  -0.99271125 -0.9935582 ]], Number:1279\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99245375 -0.99695045 -0.99919564]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99245375 -0.99695045 -0.99919564]], Number:1280\n",
      "-------------------------\n",
      "Episode 64\tFrame 1280 \tAverage100 Score: -196.19tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99791914 -0.99923986 -0.99299425]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99791914 -0.99923986 -0.99299425]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99791914 -0.99923986 -0.99299425]], Number:1281\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99225897 -0.99692476 -0.99625266]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99225897 -0.99692476 -0.99625266]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99225897 -0.99692476 -0.99625266]], Number:1282\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99782383 -0.99568576 -0.99443775]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99782383 -0.99568576 -0.99443775]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99782383 -0.99568576 -0.99443775]], Number:1283\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996294 -0.9960632 -0.9963002]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996294 -0.9960632 -0.9963002]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996294 -0.9960632 -0.9963002]], Number:1284\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936303  -0.99393404 -0.9965637 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936303  -0.99393404 -0.9965637 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9936303  -0.99393404 -0.9965637 ]], Number:1285\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99623054 -0.9958847  -0.99469286]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99623054 -0.9958847  -0.99469286]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99623054 -0.9958847  -0.99469286]], Number:1286\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994295  -0.9998621 -0.9991822]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994295  -0.9998621 -0.9991822]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.994295  -0.9998621 -0.9991822]], Number:1287\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99984324 -0.9976597  -0.99278927]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99984324 -0.9976597  -0.99278927]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99984324 -0.9976597  -0.99278927]], Number:1288\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99964845 -0.9923828  -0.99949026]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99964845 -0.9923828  -0.99949026]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99964845 -0.9923828  -0.99949026]], Number:1289\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9967903  -0.99752915 -0.9920767 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9967903  -0.99752915 -0.9920767 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9967903  -0.99752915 -0.9920767 ]], Number:1290\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99350524 -0.99867266 -0.9976655 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99350524 -0.99867266 -0.9976655 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99350524 -0.99867266 -0.9976655 ]], Number:1291\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99623275 -0.99873805 -0.9925647 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99623275 -0.99873805 -0.9925647 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99623275 -0.99873805 -0.9925647 ]], Number:1292\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99696505 -0.99923736 -0.9961677 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99696505 -0.99923736 -0.9961677 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99696505 -0.99923736 -0.9961677 ]], Number:1293\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99482816 -0.99812645 -0.997181  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99482816 -0.99812645 -0.997181  ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99482816 -0.99812645 -0.997181  ]], Number:1294\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9965827  -0.9983531  -0.99787056]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965827  -0.9983531  -0.99787056]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9965827  -0.9983531  -0.99787056]], Number:1295\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99661964 -0.9956136  -0.99258757]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99661964 -0.9956136  -0.99258757]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99661964 -0.9956136  -0.99258757]], Number:1296\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943528  -0.9971678  -0.99848074]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943528  -0.9971678  -0.99848074]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9943528  -0.9971678  -0.99848074]], Number:1297\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926274  -0.99715203 -0.9974841 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926274  -0.99715203 -0.9974841 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9926274  -0.99715203 -0.9974841 ]], Number:1298\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99280417 -0.9974103  -0.9959757 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99280417 -0.9974103  -0.9959757 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99280417 -0.9974103  -0.9959757 ]], Number:1299\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938945  -0.99689054 -0.9994273 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938945  -0.99689054 -0.9994273 ]], Number:1300\n",
      "-------------------------\n",
      "Episode 65\tFrame 1300 \tAverage100 Score: -196.25tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99968445 -0.9972818  -0.994069  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99968445 -0.9972818  -0.994069  ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99968445 -0.9972818  -0.994069  ]], Number:1301\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99702823 -0.9926327  -0.9949418 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99702823 -0.9926327  -0.9949418 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99702823 -0.9926327  -0.9949418 ]], Number:1302\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928132  -0.99952066 -0.99504274]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928132  -0.99952066 -0.99504274]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9928132  -0.99952066 -0.99504274]], Number:1303\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99287796 -0.99872947 -0.9992369 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99287796 -0.99872947 -0.9992369 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99287796 -0.99872947 -0.9992369 ]], Number:1304\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922525  -0.99266714 -0.99774575]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922525  -0.99266714 -0.99774575]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922525  -0.99266714 -0.99774575]], Number:1305\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99207157 -0.9986229  -0.99996006]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99207157 -0.9986229  -0.99996006]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99207157 -0.9986229  -0.99996006]], Number:1306\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969199 -0.9926836 -0.992845 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969199 -0.9926836 -0.992845 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9969199 -0.9926836 -0.992845 ]], Number:1307\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99498796 -0.99313915 -0.99895   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99498796 -0.99313915 -0.99895   ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99498796 -0.99313915 -0.99895   ]], Number:1308\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9937417 -0.9925409 -0.9998172]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937417 -0.9925409 -0.9998172]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9937417 -0.9925409 -0.9998172]], Number:1309\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99683523 -0.9957983  -0.9995761 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99683523 -0.9957983  -0.9995761 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99683523 -0.9957983  -0.9995761 ]], Number:1310\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99483776 -0.9951265  -0.99347085]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99483776 -0.9951265  -0.99347085]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99483776 -0.9951265  -0.99347085]], Number:1311\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948116  -0.99613714 -0.99482095]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9948116  -0.99613714 -0.99482095]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9948116  -0.99613714 -0.99482095]], Number:1312\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99864674 -0.9944013  -0.9989807 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99864674 -0.9944013  -0.9989807 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99864674 -0.9944013  -0.9989807 ]], Number:1313\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929011  -0.99948514 -0.999827  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929011  -0.99948514 -0.999827  ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9929011  -0.99948514 -0.999827  ]], Number:1314\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948246 -0.9982484 -0.9926854]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948246 -0.9982484 -0.9926854]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948246 -0.9982484 -0.9926854]], Number:1315\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940621  -0.9972809  -0.99580914]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940621  -0.9972809  -0.99580914]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9940621  -0.9972809  -0.99580914]], Number:1316\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975916  -0.99842566 -0.99743634]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975916  -0.99842566 -0.99743634]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9975916  -0.99842566 -0.99743634]], Number:1317\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.998404   -0.99876904 -0.99923843]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998404   -0.99876904 -0.99923843]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.998404   -0.99876904 -0.99923843]], Number:1318\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99355346 -0.99513024 -0.99800944]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99355346 -0.99513024 -0.99800944]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99355346 -0.99513024 -0.99800944]], Number:1319\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9986279  -0.99653834 -0.9927753 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9986279  -0.99653834 -0.9927753 ]], Number:1320\n",
      "-------------------------\n",
      "Episode 66\tFrame 1320 \tAverage100 Score: -196.31tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968591  -0.99536693 -0.9992666 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968591  -0.99536693 -0.9992666 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968591  -0.99536693 -0.9992666 ]], Number:1321\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9971478  -0.99243754 -0.992784  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9971478  -0.99243754 -0.992784  ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9971478  -0.99243754 -0.992784  ]], Number:1322\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99901146 -0.9989446  -0.9930231 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99901146 -0.9989446  -0.9930231 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99901146 -0.9989446  -0.9930231 ]], Number:1323\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984036 -0.9972125 -0.9971193]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984036 -0.9972125 -0.9971193]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9984036 -0.9972125 -0.9971193]], Number:1324\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925301  -0.9986958  -0.99321175]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925301  -0.9986958  -0.99321175]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9925301  -0.9986958  -0.99321175]], Number:1325\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99411947 -0.9963311  -0.9987282 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99411947 -0.9963311  -0.9987282 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99411947 -0.9963311  -0.9987282 ]], Number:1326\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99492186 -0.9932998  -0.9983937 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99492186 -0.9932998  -0.9983937 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99492186 -0.9932998  -0.9983937 ]], Number:1327\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9979385 -0.9992668 -0.9972897]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9979385 -0.9992668 -0.9972897]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9979385 -0.9992668 -0.9972897]], Number:1328\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921086 -0.9957619 -0.9966765]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9921086 -0.9957619 -0.9966765]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921086 -0.9957619 -0.9966765]], Number:1329\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933728  -0.99994665 -0.99354815]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933728  -0.99994665 -0.99354815]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9933728  -0.99994665 -0.99354815]], Number:1330\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99396676 -0.999175   -0.99884   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99396676 -0.999175   -0.99884   ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99396676 -0.999175   -0.99884   ]], Number:1331\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996346 -0.9987452 -0.9939761]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996346 -0.9987452 -0.9939761]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9996346 -0.9987452 -0.9939761]], Number:1332\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922657 -0.9936701 -0.9954346]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922657 -0.9936701 -0.9954346]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922657 -0.9936701 -0.9954346]], Number:1333\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99415475 -0.99399537 -0.9933599 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99415475 -0.99399537 -0.9933599 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99415475 -0.99399537 -0.9933599 ]], Number:1334\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99790704 -0.9967265  -0.9963366 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99790704 -0.9967265  -0.9963366 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99790704 -0.9967265  -0.9963366 ]], Number:1335\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995482  -0.99873024 -0.9968166 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995482  -0.99873024 -0.9968166 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9995482  -0.99873024 -0.9968166 ]], Number:1336\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966513  -0.99868417 -0.9949654 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966513  -0.99868417 -0.9949654 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966513  -0.99868417 -0.9949654 ]], Number:1337\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964451  -0.99541914 -0.9943326 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964451  -0.99541914 -0.9943326 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9964451  -0.99541914 -0.9943326 ]], Number:1338\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99814683 -0.99282473 -0.9924887 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99814683 -0.99282473 -0.9924887 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99814683 -0.99282473 -0.9924887 ]], Number:1339\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961833  -0.99622893 -0.99772495]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9961833  -0.99622893 -0.99772495]], Number:1340\n",
      "-------------------------\n",
      "Episode 67\tFrame 1340 \tAverage100 Score: -196.36tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932841  -0.9974401  -0.99724567]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932841  -0.9974401  -0.99724567]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932841  -0.9974401  -0.99724567]], Number:1341\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990227 -0.9955185 -0.9939356]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990227 -0.9955185 -0.9939356]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9990227 -0.9955185 -0.9939356]], Number:1342\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994245 -0.9971297 -0.9978218]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994245 -0.9971297 -0.9978218]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9994245 -0.9971297 -0.9978218]], Number:1343\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994241   -0.9967417  -0.99230087]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994241   -0.9967417  -0.99230087]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.994241   -0.9967417  -0.99230087]], Number:1344\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943592  -0.99889827 -0.99877745]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9943592  -0.99889827 -0.99877745]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9943592  -0.99889827 -0.99877745]], Number:1345\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99405193 -0.9971309  -0.9990258 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99405193 -0.9971309  -0.9990258 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99405193 -0.9971309  -0.9990258 ]], Number:1346\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99499923 -0.9927732  -0.9967032 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99499923 -0.9927732  -0.9967032 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99499923 -0.9927732  -0.9967032 ]], Number:1347\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99246395 -0.9946123  -0.99261886]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99246395 -0.9946123  -0.99261886]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99246395 -0.9946123  -0.99261886]], Number:1348\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99570054 -0.99810505 -0.9971518 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99570054 -0.99810505 -0.9971518 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99570054 -0.99810505 -0.9971518 ]], Number:1349\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964698  -0.9967101  -0.99242395]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964698  -0.9967101  -0.99242395]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9964698  -0.9967101  -0.99242395]], Number:1350\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995186  -0.9935486  -0.99632865]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995186  -0.9935486  -0.99632865]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9995186  -0.9935486  -0.99632865]], Number:1351\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99215275 -0.9940685  -0.9986765 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99215275 -0.9940685  -0.9986765 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99215275 -0.9940685  -0.9986765 ]], Number:1352\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99304396 -0.99594176 -0.99795675]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99304396 -0.99594176 -0.99795675]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99304396 -0.99594176 -0.99795675]], Number:1353\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968583  -0.9939665  -0.99838364]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968583  -0.9939665  -0.99838364]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9968583  -0.9939665  -0.99838364]], Number:1354\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.996869   -0.99958223 -0.99980164]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.996869   -0.99958223 -0.99980164]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.996869   -0.99958223 -0.99980164]], Number:1355\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99450564 -0.99367404 -0.992439  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99450564 -0.99367404 -0.992439  ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99450564 -0.99367404 -0.992439  ]], Number:1356\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99690354 -0.9933595  -0.99359727]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99690354 -0.9933595  -0.99359727]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99690354 -0.9933595  -0.99359727]], Number:1357\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995337 -0.9938879 -0.9963448]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995337 -0.9938879 -0.9963448]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9995337 -0.9938879 -0.9963448]], Number:1358\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99911016 -0.9920576  -0.9992409 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99911016 -0.9920576  -0.9992409 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99911016 -0.9920576  -0.9992409 ]], Number:1359\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934397 -0.9989106 -0.9947561]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934397 -0.9989106 -0.9947561]], Number:1360\n",
      "-------------------------\n",
      "Episode 68\tFrame 1360 \tAverage100 Score: -196.42tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927515  -0.99727017 -0.99787194]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927515  -0.99727017 -0.99787194]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927515  -0.99727017 -0.99787194]], Number:1361\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956577  -0.99679154 -0.996651  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956577  -0.99679154 -0.996651  ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9956577  -0.99679154 -0.996651  ]], Number:1362\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.998544   -0.9961769  -0.99777067]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.998544   -0.9961769  -0.99777067]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.998544   -0.9961769  -0.99777067]], Number:1363\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948403 -0.9968727 -0.9930277]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948403 -0.9968727 -0.9930277]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948403 -0.9968727 -0.9930277]], Number:1364\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99247116 -0.9985399  -0.99829614]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99247116 -0.9985399  -0.99829614]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99247116 -0.9985399  -0.99829614]], Number:1365\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963241  -0.99278015 -0.99771154]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963241  -0.99278015 -0.99771154]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9963241  -0.99278015 -0.99771154]], Number:1366\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99695617 -0.9962896  -0.992767  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99695617 -0.9962896  -0.992767  ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99695617 -0.9962896  -0.992767  ]], Number:1367\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999653  -0.996194  -0.9980874]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999653  -0.996194  -0.9980874]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999653  -0.996194  -0.9980874]], Number:1368\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941653  -0.99326664 -0.999434  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941653  -0.99326664 -0.999434  ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941653  -0.99326664 -0.999434  ]], Number:1369\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976683  -0.99483067 -0.99941224]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976683  -0.99483067 -0.99941224]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976683  -0.99483067 -0.99941224]], Number:1370\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948756  -0.99740523 -0.9977632 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948756  -0.99740523 -0.9977632 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9948756  -0.99740523 -0.9977632 ]], Number:1371\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957698  -0.99762166 -0.9925496 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957698  -0.99762166 -0.9925496 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9957698  -0.99762166 -0.9925496 ]], Number:1372\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992118  -0.9929771  -0.99848336]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992118  -0.9929771  -0.99848336]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992118  -0.9929771  -0.99848336]], Number:1373\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957287  -0.99865055 -0.99969417]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957287  -0.99865055 -0.99969417]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9957287  -0.99865055 -0.99969417]], Number:1374\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989564 -0.994029  -0.9966309]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989564 -0.994029  -0.9966309]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9989564 -0.994029  -0.9966309]], Number:1375\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928338 -0.9953581 -0.9979243]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928338 -0.9953581 -0.9979243]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9928338 -0.9953581 -0.9979243]], Number:1376\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99742615 -0.99359715 -0.9948    ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99742615 -0.99359715 -0.9948    ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99742615 -0.99359715 -0.9948    ]], Number:1377\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99957126 -0.99861544 -0.9968303 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99957126 -0.99861544 -0.9968303 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99957126 -0.99861544 -0.9968303 ]], Number:1378\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925663 -0.9921841 -0.9939096]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925663 -0.9921841 -0.9939096]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9925663 -0.9921841 -0.9939096]], Number:1379\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952795  -0.99842334 -0.9945209 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9952795  -0.99842334 -0.9945209 ]], Number:1380\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 69\tFrame 1380 \tAverage100 Score: -196.47tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972276 -0.9926994 -0.9956087]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972276 -0.9926994 -0.9956087]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9972276 -0.9926994 -0.9956087]], Number:1381\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99270606 -0.99398255 -0.99446756]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99270606 -0.99398255 -0.99446756]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99270606 -0.99398255 -0.99446756]], Number:1382\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9982309  -0.99986875 -0.9930269 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9982309  -0.99986875 -0.9930269 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9982309  -0.99986875 -0.9930269 ]], Number:1383\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99616915 -0.9958707  -0.99555314]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99616915 -0.9958707  -0.99555314]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99616915 -0.9958707  -0.99555314]], Number:1384\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99893016 -0.99456346 -0.9965374 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99893016 -0.99456346 -0.9965374 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99893016 -0.99456346 -0.9965374 ]], Number:1385\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99433374 -0.9956423  -0.9962272 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99433374 -0.9956423  -0.9962272 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99433374 -0.9956423  -0.9962272 ]], Number:1386\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924185  -0.99876255 -0.99883133]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924185  -0.99876255 -0.99883133]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9924185  -0.99876255 -0.99883133]], Number:1387\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955398  -0.99939454 -0.9922804 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955398  -0.99939454 -0.9922804 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955398  -0.99939454 -0.9922804 ]], Number:1388\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99387616 -0.99727213 -0.9930739 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99387616 -0.99727213 -0.9930739 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99387616 -0.99727213 -0.9930739 ]], Number:1389\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99640006 -0.9946806  -0.99551564]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99640006 -0.9946806  -0.99551564]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99640006 -0.9946806  -0.99551564]], Number:1390\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99564934 -0.99555725 -0.9945073 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99564934 -0.99555725 -0.9945073 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99564934 -0.99555725 -0.9945073 ]], Number:1391\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951243  -0.99641705 -0.999198  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951243  -0.99641705 -0.999198  ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9951243  -0.99641705 -0.999198  ]], Number:1392\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964786  -0.9968805  -0.99552494]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964786  -0.9968805  -0.99552494]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9964786  -0.9968805  -0.99552494]], Number:1393\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974401  -0.99402225 -0.9949515 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974401  -0.99402225 -0.9949515 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9974401  -0.99402225 -0.9949515 ]], Number:1394\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959681 -0.9948349 -0.99826  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959681 -0.9948349 -0.99826  ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9959681 -0.9948349 -0.99826  ]], Number:1395\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989498 -0.9932536 -0.9926249]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9989498 -0.9932536 -0.9926249]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9989498 -0.9932536 -0.9926249]], Number:1396\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993309  -0.9960951 -0.9946222]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993309  -0.9960951 -0.9946222]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993309  -0.9960951 -0.9946222]], Number:1397\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99759316 -0.99527663 -0.9998734 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99759316 -0.99527663 -0.9998734 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99759316 -0.99527663 -0.9998734 ]], Number:1398\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933312 -0.9980947 -0.9982391]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933312 -0.9980947 -0.9982391]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9933312 -0.9980947 -0.9982391]], Number:1399\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984509 -0.9965438 -0.9962864]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9984509 -0.9965438 -0.9962864]], Number:1400\n",
      "-------------------------\n",
      "Episode 70\tFrame 1400 \tAverage100 Score: -196.52tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99327993 -0.9956156  -0.9971124 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99327993 -0.9956156  -0.9971124 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99327993 -0.9956156  -0.9971124 ]], Number:1401\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929957  -0.99678767 -0.9952944 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929957  -0.99678767 -0.9952944 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929957  -0.99678767 -0.9952944 ]], Number:1402\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99773955 -0.9962117  -0.9931511 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99773955 -0.9962117  -0.9931511 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99773955 -0.9962117  -0.9931511 ]], Number:1403\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99313277 -0.9930234  -0.9978446 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99313277 -0.9930234  -0.9978446 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99313277 -0.9930234  -0.9978446 ]], Number:1404\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950401  -0.9957605  -0.99822074]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950401  -0.9957605  -0.99822074]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950401  -0.9957605  -0.99822074]], Number:1405\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930973  -0.9943156  -0.99711156]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930973  -0.9943156  -0.99711156]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930973  -0.9943156  -0.99711156]], Number:1406\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99319106 -0.992001   -0.99745625]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99319106 -0.992001   -0.99745625]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99319106 -0.992001   -0.99745625]], Number:1407\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99932677 -0.9969885  -0.9926106 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99932677 -0.9969885  -0.9926106 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99932677 -0.9969885  -0.9926106 ]], Number:1408\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99962574 -0.9977467  -0.99888235]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99962574 -0.9977467  -0.99888235]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99962574 -0.9977467  -0.99888235]], Number:1409\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951438  -0.99865824 -0.9928075 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951438  -0.99865824 -0.9928075 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9951438  -0.99865824 -0.9928075 ]], Number:1410\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9931161  -0.99402475 -0.9960244 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9931161  -0.99402475 -0.9960244 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9931161  -0.99402475 -0.9960244 ]], Number:1411\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992246   -0.9945065  -0.99471676]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992246   -0.9945065  -0.99471676]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.992246   -0.9945065  -0.99471676]], Number:1412\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99876493 -0.99649125 -0.9935784 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99876493 -0.99649125 -0.9935784 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99876493 -0.99649125 -0.9935784 ]], Number:1413\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972765 -0.998087  -0.9959392]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972765 -0.998087  -0.9959392]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9972765 -0.998087  -0.9959392]], Number:1414\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926497 -0.9941948 -0.9950849]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926497 -0.9941948 -0.9950849]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9926497 -0.9941948 -0.9950849]], Number:1415\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9980511  -0.99399096 -0.993835  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9980511  -0.99399096 -0.993835  ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9980511  -0.99399096 -0.993835  ]], Number:1416\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99965423 -0.995042   -0.99244106]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99965423 -0.995042   -0.99244106]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99965423 -0.995042   -0.99244106]], Number:1417\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9958097  -0.9969248  -0.99582785]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9958097  -0.9969248  -0.99582785]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9958097  -0.9969248  -0.99582785]], Number:1418\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99846685 -0.99789464 -0.9935806 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99846685 -0.99789464 -0.9935806 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99846685 -0.99789464 -0.9935806 ]], Number:1419\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99510705 -0.9993748  -0.9950001 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99510705 -0.9993748  -0.9950001 ]], Number:1420\n",
      "-------------------------\n",
      "Episode 71\tFrame 1420 \tAverage100 Score: -196.57tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992326  -0.99692553 -0.9980441 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992326  -0.99692553 -0.9980441 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992326  -0.99692553 -0.9980441 ]], Number:1421\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936062 -0.9994858 -0.9958764]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936062 -0.9994858 -0.9958764]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9936062 -0.9994858 -0.9958764]], Number:1422\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977759 -0.9999987 -0.992647 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977759 -0.9999987 -0.992647 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9977759 -0.9999987 -0.992647 ]], Number:1423\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99639755 -0.99211806 -0.99417675]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99639755 -0.99211806 -0.99417675]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99639755 -0.99211806 -0.99417675]], Number:1424\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99532604 -0.9972538  -0.9922491 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99532604 -0.9972538  -0.9922491 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99532604 -0.9972538  -0.9922491 ]], Number:1425\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939145  -0.99985415 -0.9970231 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939145  -0.99985415 -0.9970231 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939145  -0.99985415 -0.9970231 ]], Number:1426\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here: [-0.9937013  -0.99958503 -0.9981938 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937013  -0.99958503 -0.9981938 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9937013  -0.99958503 -0.9981938 ]], Number:1427\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99362075 -0.9948123  -0.9983511 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99362075 -0.9948123  -0.9983511 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99362075 -0.9948123  -0.9983511 ]], Number:1428\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977103  -0.9921314  -0.99268234]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977103  -0.9921314  -0.99268234]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9977103  -0.9921314  -0.99268234]], Number:1429\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99776804 -0.9939152  -0.99630773]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99776804 -0.9939152  -0.99630773]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99776804 -0.9939152  -0.99630773]], Number:1430\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99423015 -0.99208564 -0.9923805 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99423015 -0.99208564 -0.9923805 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99423015 -0.99208564 -0.9923805 ]], Number:1431\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99298185 -0.997002   -0.9946567 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99298185 -0.997002   -0.9946567 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99298185 -0.997002   -0.9946567 ]], Number:1432\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935396 -0.9981252 -0.9979935]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9935396 -0.9981252 -0.9979935]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9935396 -0.9981252 -0.9979935]], Number:1433\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925358  -0.998296   -0.99510825]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925358  -0.998296   -0.99510825]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9925358  -0.998296   -0.99510825]], Number:1434\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9937297  -0.99641263 -0.995832  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937297  -0.99641263 -0.995832  ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9937297  -0.99641263 -0.995832  ]], Number:1435\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99286836 -0.9963275  -0.99700016]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99286836 -0.9963275  -0.99700016]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99286836 -0.9963275  -0.99700016]], Number:1436\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992449  -0.9936024  -0.99760777]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992449  -0.9936024  -0.99760777]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992449  -0.9936024  -0.99760777]], Number:1437\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9974506  -0.99270064 -0.9970959 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9974506  -0.99270064 -0.9970959 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9974506  -0.99270064 -0.9970959 ]], Number:1438\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923615  -0.99894553 -0.9968959 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923615  -0.99894553 -0.9968959 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9923615  -0.99894553 -0.9968959 ]], Number:1439\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976784  -0.99489874 -0.9932366 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976784  -0.99489874 -0.9932366 ]], Number:1440\n",
      "-------------------------\n",
      "Episode 72\tFrame 1440 \tAverage100 Score: -196.62tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99925905 -0.9941426  -0.9975951 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99925905 -0.9941426  -0.9975951 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99925905 -0.9941426  -0.9975951 ]], Number:1441\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99757797 -0.9991401  -0.994103  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99757797 -0.9991401  -0.994103  ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99757797 -0.9991401  -0.994103  ]], Number:1442\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956481  -0.99677515 -0.9994563 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956481  -0.99677515 -0.9994563 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9956481  -0.99677515 -0.9994563 ]], Number:1443\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99731934 -0.9987136  -0.99918133]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99731934 -0.9987136  -0.99918133]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99731934 -0.9987136  -0.99918133]], Number:1444\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926597  -0.99996823 -0.99474776]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926597  -0.99996823 -0.99474776]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9926597  -0.99996823 -0.99474776]], Number:1445\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950033  -0.99763876 -0.999836  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9950033  -0.99763876 -0.999836  ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9950033  -0.99763876 -0.999836  ]], Number:1446\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923016  -0.9966598  -0.99469376]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923016  -0.9966598  -0.99469376]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923016  -0.9966598  -0.99469376]], Number:1447\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997349 -0.9963216 -0.9948885]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997349 -0.9963216 -0.9948885]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997349 -0.9963216 -0.9948885]], Number:1448\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988583 -0.9972657 -0.9929294]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988583 -0.9972657 -0.9929294]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988583 -0.9972657 -0.9929294]], Number:1449\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99393207 -0.9935451  -0.9963098 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99393207 -0.9935451  -0.9963098 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99393207 -0.9935451  -0.9963098 ]], Number:1450\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926676  -0.99818534 -0.99581903]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926676  -0.99818534 -0.99581903]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9926676  -0.99818534 -0.99581903]], Number:1451\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99961156 -0.9961073  -0.9975637 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99961156 -0.9961073  -0.9975637 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99961156 -0.9961073  -0.9975637 ]], Number:1452\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996209 -0.9946615 -0.9981103]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996209 -0.9946615 -0.9981103]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996209 -0.9946615 -0.9981103]], Number:1453\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99364465 -0.99200505 -0.99343365]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99364465 -0.99200505 -0.99343365]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99364465 -0.99200505 -0.99343365]], Number:1454\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988749  -0.9999438  -0.99848586]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988749  -0.9999438  -0.99848586]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988749  -0.9999438  -0.99848586]], Number:1455\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973708  -0.9988428  -0.99246824]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973708  -0.9988428  -0.99246824]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9973708  -0.9988428  -0.99246824]], Number:1456\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988177 -0.9970103 -0.992323 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988177 -0.9970103 -0.992323 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988177 -0.9970103 -0.992323 ]], Number:1457\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975814 -0.9944268 -0.9996225]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975814 -0.9944268 -0.9996225]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9975814 -0.9944268 -0.9996225]], Number:1458\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993374  -0.9932668  -0.99227214]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993374  -0.9932668  -0.99227214]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9993374  -0.9932668  -0.99227214]], Number:1459\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962261 -0.9957111 -0.9994373]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962261 -0.9957111 -0.9994373]], Number:1460\n",
      "-------------------------\n",
      "Episode 73\tFrame 1460 \tAverage100 Score: -196.66tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99744475 -0.9949252  -0.99620783]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99744475 -0.9949252  -0.99620783]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99744475 -0.9949252  -0.99620783]], Number:1461\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99654025 -0.9936055  -0.9920254 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99654025 -0.9936055  -0.9920254 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99654025 -0.9936055  -0.9920254 ]], Number:1462\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943028  -0.9977911  -0.99886674]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943028  -0.9977911  -0.99886674]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9943028  -0.9977911  -0.99886674]], Number:1463\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941582  -0.99410367 -0.99402577]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9941582  -0.99410367 -0.99402577]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941582  -0.99410367 -0.99402577]], Number:1464\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99869585 -0.9974763  -0.99410284]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99869585 -0.9974763  -0.99410284]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99869585 -0.9974763  -0.99410284]], Number:1465\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99599105 -0.99628437 -0.9979355 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99599105 -0.99628437 -0.9979355 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99599105 -0.99628437 -0.9979355 ]], Number:1466\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990136  -0.99823356 -0.9974083 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990136  -0.99823356 -0.9974083 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9990136  -0.99823356 -0.9974083 ]], Number:1467\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964069  -0.9929697  -0.99417335]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964069  -0.9929697  -0.99417335]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9964069  -0.9929697  -0.99417335]], Number:1468\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944084  -0.9948884  -0.99474424]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944084  -0.9948884  -0.99474424]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9944084  -0.9948884  -0.99474424]], Number:1469\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99536705 -0.99918073 -0.9963629 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99536705 -0.99918073 -0.9963629 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99536705 -0.99918073 -0.9963629 ]], Number:1470\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9980244  -0.9969343  -0.99419445]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9980244  -0.9969343  -0.99419445]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9980244  -0.9969343  -0.99419445]], Number:1471\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969728  -0.99559355 -0.99623525]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969728  -0.99559355 -0.99623525]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9969728  -0.99559355 -0.99623525]], Number:1472\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959234  -0.99368316 -0.99717003]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959234  -0.99368316 -0.99717003]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9959234  -0.99368316 -0.99717003]], Number:1473\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946749 -0.9937077 -0.9965715]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946749 -0.9937077 -0.9965715]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9946749 -0.9937077 -0.9965715]], Number:1474\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99354005 -0.9946009  -0.9935232 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99354005 -0.9946009  -0.9935232 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99354005 -0.9946009  -0.9935232 ]], Number:1475\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99806064 -0.9978274  -0.9949966 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99806064 -0.9978274  -0.9949966 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99806064 -0.9978274  -0.9949966 ]], Number:1476\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934035 -0.9980719 -0.9989101]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934035 -0.9980719 -0.9989101]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934035 -0.9980719 -0.9989101]], Number:1477\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99247336 -0.99932754 -0.9944554 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99247336 -0.99932754 -0.9944554 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99247336 -0.99932754 -0.9944554 ]], Number:1478\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99866796 -0.9931396  -0.99667066]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99866796 -0.9931396  -0.99667066]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99866796 -0.9931396  -0.99667066]], Number:1479\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99849737 -0.9962556  -0.9956719 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99849737 -0.9962556  -0.9956719 ]], Number:1480\n",
      "-------------------------\n",
      "Episode 74\tFrame 1480 \tAverage100 Score: -196.71tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992066  -0.99557793 -0.9924628 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9992066  -0.99557793 -0.9924628 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9992066  -0.99557793 -0.9924628 ]], Number:1481\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99998    -0.99627125 -0.9946937 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99998    -0.99627125 -0.9946937 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99998    -0.99627125 -0.9946937 ]], Number:1482\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99852496 -0.9944654  -0.9948834 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99852496 -0.9944654  -0.9948834 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99852496 -0.9944654  -0.9948834 ]], Number:1483\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972092  -0.99641514 -0.99967027]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972092  -0.99641514 -0.99967027]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9972092  -0.99641514 -0.99967027]], Number:1484\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934989  -0.99759257 -0.9996356 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934989  -0.99759257 -0.9996356 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934989  -0.99759257 -0.9996356 ]], Number:1485\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9981813  -0.99753463 -0.9957416 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9981813  -0.99753463 -0.9957416 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9981813  -0.99753463 -0.9957416 ]], Number:1486\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99692297 -0.9975862  -0.99682707]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99692297 -0.9975862  -0.99682707]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99692297 -0.9975862  -0.99682707]], Number:1487\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923394 -0.9924327 -0.9990016]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923394 -0.9924327 -0.9990016]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923394 -0.9924327 -0.9990016]], Number:1488\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943944 -0.9948021 -0.9927776]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943944 -0.9948021 -0.9927776]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9943944 -0.9948021 -0.9927776]], Number:1489\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99447364 -0.9964523  -0.9938078 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99447364 -0.9964523  -0.9938078 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99447364 -0.9964523  -0.9938078 ]], Number:1490\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99705845 -0.99245226 -0.99271554]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99705845 -0.99245226 -0.99271554]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99705845 -0.99245226 -0.99271554]], Number:1491\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948679 -0.9981622 -0.9944686]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948679 -0.9981622 -0.9944686]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9948679 -0.9981622 -0.9944686]], Number:1492\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99626577 -0.9950257  -0.9928278 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99626577 -0.9950257  -0.9928278 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99626577 -0.9950257  -0.9928278 ]], Number:1493\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99475724 -0.995993   -0.9931355 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99475724 -0.995993   -0.9931355 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99475724 -0.995993   -0.9931355 ]], Number:1494\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995926   -0.9931319  -0.99993277]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995926   -0.9931319  -0.99993277]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995926   -0.9931319  -0.99993277]], Number:1495\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970994 -0.9968691 -0.9973323]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9970994 -0.9968691 -0.9973323]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9970994 -0.9968691 -0.9973323]], Number:1496\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990538  -0.99718875 -0.9964038 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990538  -0.99718875 -0.9964038 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9990538  -0.99718875 -0.9964038 ]], Number:1497\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9983808  -0.99853283 -0.9982477 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9983808  -0.99853283 -0.9982477 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9983808  -0.99853283 -0.9982477 ]], Number:1498\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933457  -0.99314374 -0.99419427]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933457  -0.99314374 -0.99419427]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9933457  -0.99314374 -0.99419427]], Number:1499\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977753  -0.99601793 -0.9990522 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9977753  -0.99601793 -0.9990522 ]], Number:1500\n",
      "-------------------------\n",
      "Episode 75\tFrame 1500 \tAverage100 Score: -196.75tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922536  -0.99671257 -0.9958241 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922536  -0.99671257 -0.9958241 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922536  -0.99671257 -0.9958241 ]], Number:1501\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99656534 -0.9998328  -0.99634296]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99656534 -0.9998328  -0.99634296]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99656534 -0.9998328  -0.99634296]], Number:1502\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995106  -0.9924975 -0.995122 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995106  -0.9924975 -0.995122 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.995106  -0.9924975 -0.995122 ]], Number:1503\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947151  -0.99743843 -0.9942387 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947151  -0.99743843 -0.9942387 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947151  -0.99743843 -0.9942387 ]], Number:1504\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922292  -0.9963225  -0.99949676]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922292  -0.9963225  -0.99949676]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922292  -0.9963225  -0.99949676]], Number:1505\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973725  -0.9998704  -0.99560696]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973725  -0.9998704  -0.99560696]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9973725  -0.9998704  -0.99560696]], Number:1506\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99905336 -0.99991095 -0.99360454]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99905336 -0.99991095 -0.99360454]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99905336 -0.99991095 -0.99360454]], Number:1507\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966068  -0.99822795 -0.99929965]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966068  -0.99822795 -0.99929965]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966068  -0.99822795 -0.99929965]], Number:1508\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99690515 -0.99220324 -0.9982393 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99690515 -0.99220324 -0.9982393 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99690515 -0.99220324 -0.9982393 ]], Number:1509\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956363 -0.9928614 -0.9927834]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956363 -0.9928614 -0.9927834]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9956363 -0.9928614 -0.9927834]], Number:1510\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9975665  -0.99211264 -0.9978308 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9975665  -0.99211264 -0.9978308 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9975665  -0.99211264 -0.9978308 ]], Number:1511\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9952285  -0.99895924 -0.9942396 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952285  -0.99895924 -0.9942396 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9952285  -0.99895924 -0.9942396 ]], Number:1512\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99702656 -0.9959573  -0.9968592 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99702656 -0.9959573  -0.9968592 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99702656 -0.9959573  -0.9968592 ]], Number:1513\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948838 -0.9929702 -0.9952627]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948838 -0.9929702 -0.9952627]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9948838 -0.9929702 -0.9952627]], Number:1514\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998584  -0.9942275  -0.99921495]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9998584  -0.9942275  -0.99921495]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9998584  -0.9942275  -0.99921495]], Number:1515\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973794  -0.9990291  -0.99320614]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973794  -0.9990291  -0.99320614]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9973794  -0.9990291  -0.99320614]], Number:1516\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969286  -0.99722433 -0.99639666]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969286  -0.99722433 -0.99639666]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9969286  -0.99722433 -0.99639666]], Number:1517\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99818313 -0.99354863 -0.99542785]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99818313 -0.99354863 -0.99542785]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99818313 -0.99354863 -0.99542785]], Number:1518\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957371 -0.9983892 -0.9938329]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957371 -0.9983892 -0.9938329]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9957371 -0.9983892 -0.9938329]], Number:1519\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99906254 -0.9946003  -0.99309117]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99906254 -0.9946003  -0.99309117]], Number:1520\n",
      "-------------------------\n",
      "Episode 76\tFrame 1520 \tAverage100 Score: -196.79tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99798065 -0.9982854  -0.99383837]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99798065 -0.9982854  -0.99383837]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99798065 -0.9982854  -0.99383837]], Number:1521\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941739  -0.99332035 -0.99797857]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941739  -0.99332035 -0.99797857]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941739  -0.99332035 -0.99797857]], Number:1522\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968818  -0.9922722  -0.99727803]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968818  -0.9922722  -0.99727803]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9968818  -0.9922722  -0.99727803]], Number:1523\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99784946 -0.99263126 -0.9984803 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99784946 -0.99263126 -0.9984803 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99784946 -0.99263126 -0.9984803 ]], Number:1524\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9956722  -0.99418485 -0.9925931 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9956722  -0.99418485 -0.9925931 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9956722  -0.99418485 -0.9925931 ]], Number:1525\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99328893 -0.99282116 -0.99742734]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99328893 -0.99282116 -0.99742734]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99328893 -0.99282116 -0.99742734]], Number:1526\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934514  -0.99244326 -0.997307  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934514  -0.99244326 -0.997307  ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934514  -0.99244326 -0.997307  ]], Number:1527\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942863  -0.9983141  -0.99788094]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942863  -0.9983141  -0.99788094]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9942863  -0.9983141  -0.99788094]], Number:1528\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99517727 -0.9956759  -0.9946265 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99517727 -0.9956759  -0.9946265 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99517727 -0.9956759  -0.9946265 ]], Number:1529\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999023  -0.9975291 -0.9925486]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999023  -0.9975291 -0.9925486]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999023  -0.9975291 -0.9925486]], Number:1530\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977027  -0.9977147  -0.99856216]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9977027  -0.9977147  -0.99856216]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9977027  -0.9977147  -0.99856216]], Number:1531\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933584 -0.9947566 -0.9928499]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933584 -0.9947566 -0.9928499]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9933584 -0.9947566 -0.9928499]], Number:1532\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943386  -0.992777   -0.99749464]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943386  -0.992777   -0.99749464]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9943386  -0.992777   -0.99749464]], Number:1533\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99442565 -0.9994041  -0.99876034]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99442565 -0.9994041  -0.99876034]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99442565 -0.9994041  -0.99876034]], Number:1534\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939615 -0.9956863 -0.9955339]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939615 -0.9956863 -0.9955339]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939615 -0.9956863 -0.9955339]], Number:1535\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934879 -0.9932802 -0.9957978]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934879 -0.9932802 -0.9957978]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9934879 -0.9932802 -0.9957978]], Number:1536\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99622744 -0.99606526 -0.9935081 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99622744 -0.99606526 -0.9935081 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99622744 -0.99606526 -0.9935081 ]], Number:1537\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99941754 -0.99806744 -0.9992654 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99941754 -0.99806744 -0.9992654 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99941754 -0.99806744 -0.9992654 ]], Number:1538\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948774  -0.99329793 -0.9972109 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948774  -0.99329793 -0.9972109 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9948774  -0.99329793 -0.9972109 ]], Number:1539\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9965753  -0.99665844 -0.9937107 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9965753  -0.99665844 -0.9937107 ]], Number:1540\n",
      "-------------------------\n",
      "Episode 77\tFrame 1540 \tAverage100 Score: -196.83tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9981413 -0.9936524 -0.9976561]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9981413 -0.9936524 -0.9976561]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9981413 -0.9936524 -0.9976561]], Number:1541\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99595594 -0.99361986 -0.99963415]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99595594 -0.99361986 -0.99963415]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99595594 -0.99361986 -0.99963415]], Number:1542\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99673843 -0.99269015 -0.99507254]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99673843 -0.99269015 -0.99507254]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99673843 -0.99269015 -0.99507254]], Number:1543\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9920668  -0.99738425 -0.9956246 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9920668  -0.99738425 -0.9956246 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9920668  -0.99738425 -0.9956246 ]], Number:1544\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99290097 -0.99246407 -0.99778134]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99290097 -0.99246407 -0.99778134]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99290097 -0.99246407 -0.99778134]], Number:1545\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99875385 -0.9987585  -0.9999275 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99875385 -0.9987585  -0.9999275 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99875385 -0.9987585  -0.9999275 ]], Number:1546\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993314  -0.99690557 -0.9958383 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993314  -0.99690557 -0.9958383 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993314  -0.99690557 -0.9958383 ]], Number:1547\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99457586 -0.9983502  -0.9932707 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99457586 -0.9983502  -0.9932707 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99457586 -0.9983502  -0.9932707 ]], Number:1548\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99319804 -0.99518347 -0.99431646]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99319804 -0.99518347 -0.99431646]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99319804 -0.99518347 -0.99431646]], Number:1549\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9979726  -0.99494517 -0.99493957]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9979726  -0.99494517 -0.99493957]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9979726  -0.99494517 -0.99493957]], Number:1550\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99739456 -0.99334055 -0.99952894]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99739456 -0.99334055 -0.99952894]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99739456 -0.99334055 -0.99952894]], Number:1551\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924461 -0.9956769 -0.9988819]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924461 -0.9956769 -0.9988819]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9924461 -0.9956769 -0.9988819]], Number:1552\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.997089  -0.9958601 -0.9985642]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.997089  -0.9958601 -0.9985642]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.997089  -0.9958601 -0.9985642]], Number:1553\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99935865 -0.993371   -0.9988544 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99935865 -0.993371   -0.9988544 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99935865 -0.993371   -0.9988544 ]], Number:1554\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936537  -0.99619216 -0.99278355]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936537  -0.99619216 -0.99278355]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9936537  -0.99619216 -0.99278355]], Number:1555\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99226046 -0.9921076  -0.99404997]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99226046 -0.9921076  -0.99404997]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99226046 -0.9921076  -0.99404997]], Number:1556\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99713445 -0.9929065  -0.9976012 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99713445 -0.9929065  -0.9976012 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99713445 -0.9929065  -0.9976012 ]], Number:1557\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994279 -0.9921533 -0.9985   ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994279 -0.9921533 -0.9985   ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9994279 -0.9921533 -0.9985   ]], Number:1558\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933208  -0.99270356 -0.9934445 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933208  -0.99270356 -0.9934445 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9933208  -0.99270356 -0.9934445 ]], Number:1559\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9929085  -0.9975042  -0.99607426]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929085  -0.9975042  -0.99607426]], Number:1560\n",
      "-------------------------\n",
      "Episode 78\tFrame 1560 \tAverage100 Score: -196.88tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962443  -0.99492306 -0.9990306 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962443  -0.99492306 -0.9990306 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962443  -0.99492306 -0.9990306 ]], Number:1561\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99511653 -0.993731   -0.99981195]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99511653 -0.993731   -0.99981195]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99511653 -0.993731   -0.99981195]], Number:1562\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959986 -0.9976729 -0.9945348]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9959986 -0.9976729 -0.9945348]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9959986 -0.9976729 -0.9945348]], Number:1563\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99909776 -0.9940272  -0.9960726 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99909776 -0.9940272  -0.9960726 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99909776 -0.9940272  -0.9960726 ]], Number:1564\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99209225 -0.996541   -0.9986741 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99209225 -0.996541   -0.9986741 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99209225 -0.996541   -0.9986741 ]], Number:1565\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947809  -0.9924154  -0.99331725]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947809  -0.9924154  -0.99331725]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947809  -0.9924154  -0.99331725]], Number:1566\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99781334 -0.9961683  -0.9967496 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99781334 -0.9961683  -0.9967496 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99781334 -0.9961683  -0.9967496 ]], Number:1567\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996967 -0.9972498 -0.993165 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996967 -0.9972498 -0.993165 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996967 -0.9972498 -0.993165 ]], Number:1568\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99491143 -0.99313396 -0.9999333 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99491143 -0.99313396 -0.9999333 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99491143 -0.99313396 -0.9999333 ]], Number:1569\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99615365 -0.9937833  -0.9991213 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99615365 -0.9937833  -0.9991213 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99615365 -0.9937833  -0.9991213 ]], Number:1570\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933546 -0.9924954 -0.9960731]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933546 -0.9924954 -0.9960731]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9933546 -0.9924954 -0.9960731]], Number:1571\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973668  -0.9977377  -0.99472433]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973668  -0.9977377  -0.99472433]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9973668  -0.9977377  -0.99472433]], Number:1572\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.996809   -0.9957385  -0.99290866]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.996809   -0.9957385  -0.99290866]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.996809   -0.9957385  -0.99290866]], Number:1573\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99913985 -0.99617624 -0.99573165]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99913985 -0.99617624 -0.99573165]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99913985 -0.99617624 -0.99573165]], Number:1574\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957567 -0.9965827 -0.9977143]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957567 -0.9965827 -0.9977143]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9957567 -0.9965827 -0.9977143]], Number:1575\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9980392  -0.99301153 -0.9946548 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9980392  -0.99301153 -0.9946548 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9980392  -0.99301153 -0.9946548 ]], Number:1576\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99290854 -0.9937731  -0.9961478 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99290854 -0.9937731  -0.9961478 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99290854 -0.9937731  -0.9961478 ]], Number:1577\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99344814 -0.9920941  -0.9923014 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99344814 -0.9920941  -0.9923014 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99344814 -0.9920941  -0.9923014 ]], Number:1578\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998221  -0.99406433 -0.99658763]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9998221  -0.99406433 -0.99658763]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9998221  -0.99406433 -0.99658763]], Number:1579\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99756867 -0.99496895 -0.99741966]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99756867 -0.99496895 -0.99741966]], Number:1580\n",
      "-------------------------\n",
      "Episode 79\tFrame 1580 \tAverage100 Score: -196.92tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99414355 -0.9950166  -0.99437934]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99414355 -0.9950166  -0.99437934]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99414355 -0.9950166  -0.99437934]], Number:1581\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9959805 -0.9945486 -0.9963936]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9959805 -0.9945486 -0.9963936]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9959805 -0.9945486 -0.9963936]], Number:1582\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9987691  -0.9926416  -0.99232894]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9987691  -0.9926416  -0.99232894]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9987691  -0.9926416  -0.99232894]], Number:1583\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99639654 -0.99633557 -0.9940915 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99639654 -0.99633557 -0.9940915 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99639654 -0.99633557 -0.9940915 ]], Number:1584\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99321645 -0.99289095 -0.9921118 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99321645 -0.99289095 -0.9921118 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99321645 -0.99289095 -0.9921118 ]], Number:1585\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955455 -0.9946157 -0.9926583]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955455 -0.9946157 -0.9926583]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955455 -0.9946157 -0.9926583]], Number:1586\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999464   -0.99869704 -0.99951404]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999464   -0.99869704 -0.99951404]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999464   -0.99869704 -0.99951404]], Number:1587\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998291  -0.99265623 -0.99644184]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9998291  -0.99265623 -0.99644184]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9998291  -0.99265623 -0.99644184]], Number:1588\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99631506 -0.9974405  -0.99792796]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99631506 -0.9974405  -0.99792796]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99631506 -0.9974405  -0.99792796]], Number:1589\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9952099  -0.9953264  -0.99433386]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9952099  -0.9953264  -0.99433386]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9952099  -0.9953264  -0.99433386]], Number:1590\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955711 -0.9972115 -0.9989748]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955711 -0.9972115 -0.9989748]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9955711 -0.9972115 -0.9989748]], Number:1591\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99890506 -0.99210936 -0.9986937 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99890506 -0.99210936 -0.9986937 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99890506 -0.99210936 -0.9986937 ]], Number:1592\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99523383 -0.99997413 -0.9921836 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99523383 -0.99997413 -0.9921836 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99523383 -0.99997413 -0.9921836 ]], Number:1593\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99750817 -0.99759746 -0.9931843 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99750817 -0.99759746 -0.9931843 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99750817 -0.99759746 -0.9931843 ]], Number:1594\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935763 -0.9931487 -0.993795 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9935763 -0.9931487 -0.993795 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9935763 -0.9931487 -0.993795 ]], Number:1595\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942829 -0.9932017 -0.9954655]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942829 -0.9932017 -0.9954655]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9942829 -0.9932017 -0.9954655]], Number:1596\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985073  -0.99447215 -0.99790156]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985073  -0.99447215 -0.99790156]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985073  -0.99447215 -0.99790156]], Number:1597\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928043  -0.99606824 -0.99921256]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928043  -0.99606824 -0.99921256]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9928043  -0.99606824 -0.99921256]], Number:1598\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99846405 -0.9960759  -0.99977493]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99846405 -0.9960759  -0.99977493]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99846405 -0.9960759  -0.99977493]], Number:1599\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99619704 -0.99666435 -0.99675953]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99619704 -0.99666435 -0.99675953]], Number:1600\n",
      "-------------------------\n",
      "Episode 80\tFrame 1600 \tAverage100 Score: -196.95tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994699 -0.9939338 -0.9947109]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994699 -0.9939338 -0.9947109]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994699 -0.9939338 -0.9947109]], Number:1601\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99615   -0.9923575 -0.9928465]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99615   -0.9923575 -0.9928465]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99615   -0.9923575 -0.9928465]], Number:1602\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99386173 -0.9995661  -0.9971811 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99386173 -0.9995661  -0.9971811 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99386173 -0.9995661  -0.9971811 ]], Number:1603\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99516267 -0.9968417  -0.9952654 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99516267 -0.9968417  -0.9952654 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99516267 -0.9968417  -0.9952654 ]], Number:1604\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99818236 -0.9932013  -0.9929312 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99818236 -0.9932013  -0.9929312 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99818236 -0.9932013  -0.9929312 ]], Number:1605\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989714  -0.9965086  -0.99876976]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989714  -0.9965086  -0.99876976]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9989714  -0.9965086  -0.99876976]], Number:1606\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940938  -0.99216133 -0.99524105]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940938  -0.99216133 -0.99524105]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9940938  -0.99216133 -0.99524105]], Number:1607\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9961755  -0.99375015 -0.99690616]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9961755  -0.99375015 -0.99690616]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9961755  -0.99375015 -0.99690616]], Number:1608\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921073 -0.9933035 -0.9972095]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921073 -0.9933035 -0.9972095]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921073 -0.9933035 -0.9972095]], Number:1609\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99635    -0.99349546 -0.99619514]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99635    -0.99349546 -0.99619514]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99635    -0.99349546 -0.99619514]], Number:1610\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99989784 -0.9955192  -0.9997072 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99989784 -0.9955192  -0.9997072 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99989784 -0.9955192  -0.9997072 ]], Number:1611\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99931604 -0.9922579  -0.9978975 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99931604 -0.9922579  -0.9978975 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99931604 -0.9922579  -0.9978975 ]], Number:1612\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99714375 -0.992774   -0.9930879 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99714375 -0.992774   -0.9930879 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99714375 -0.992774   -0.9930879 ]], Number:1613\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963086  -0.9929439  -0.99845225]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963086  -0.9929439  -0.99845225]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9963086  -0.9929439  -0.99845225]], Number:1614\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996259  -0.99450797 -0.996428  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996259  -0.99450797 -0.996428  ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996259  -0.99450797 -0.996428  ]], Number:1615\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943929  -0.9969716  -0.99788874]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943929  -0.9969716  -0.99788874]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9943929  -0.9969716  -0.99788874]], Number:1616\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99648356 -0.99217355 -0.9996149 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99648356 -0.99217355 -0.9996149 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99648356 -0.99217355 -0.9996149 ]], Number:1617\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9937874 -0.9969965 -0.9958942]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937874 -0.9969965 -0.9958942]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9937874 -0.9969965 -0.9958942]], Number:1618\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9963656  -0.9924289  -0.99420696]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9963656  -0.9924289  -0.99420696]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9963656  -0.9924289  -0.99420696]], Number:1619\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99702895 -0.99965656 -0.99827826]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99702895 -0.99965656 -0.99827826]], Number:1620\n",
      "-------------------------\n",
      "Episode 81\tFrame 1620 \tAverage100 Score: -196.99tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99302596 -0.99470943 -0.99472815]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99302596 -0.99470943 -0.99472815]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99302596 -0.99470943 -0.99472815]], Number:1621\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99271995 -0.99298507 -0.9993209 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99271995 -0.99298507 -0.9993209 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99271995 -0.99298507 -0.9993209 ]], Number:1622\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9943724  -0.99879205 -0.99971837]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9943724  -0.99879205 -0.99971837]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9943724  -0.99879205 -0.99971837]], Number:1623\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994999 -0.9992255 -0.9942946]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994999 -0.9992255 -0.9942946]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9994999 -0.9992255 -0.9942946]], Number:1624\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99989945 -0.99595535 -0.9934161 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99989945 -0.99595535 -0.9934161 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99989945 -0.99595535 -0.9934161 ]], Number:1625\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99769104 -0.99312675 -0.99510205]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99769104 -0.99312675 -0.99510205]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99769104 -0.99312675 -0.99510205]], Number:1626\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99560934 -0.99816835 -0.9998021 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99560934 -0.99816835 -0.9998021 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99560934 -0.99816835 -0.9998021 ]], Number:1627\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936536 -0.9924793 -0.9972415]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936536 -0.9924793 -0.9972415]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9936536 -0.9924793 -0.9972415]], Number:1628\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9946255  -0.99658054 -0.9993075 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9946255  -0.99658054 -0.9993075 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9946255  -0.99658054 -0.9993075 ]], Number:1629\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99231863 -0.99494827 -0.9947538 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99231863 -0.99494827 -0.9947538 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99231863 -0.99494827 -0.9947538 ]], Number:1630\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9972999 -0.9952366 -0.9975217]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9972999 -0.9952366 -0.9975217]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9972999 -0.9952366 -0.9975217]], Number:1631\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9945443 -0.9970792 -0.996433 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9945443 -0.9970792 -0.996433 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9945443 -0.9970792 -0.996433 ]], Number:1632\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99303097 -0.9988814  -0.9985795 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99303097 -0.9988814  -0.9985795 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99303097 -0.9988814  -0.9985795 ]], Number:1633\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99397856 -0.9920674  -0.9969705 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99397856 -0.9920674  -0.9969705 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99397856 -0.9920674  -0.9969705 ]], Number:1634\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9951887  -0.9990691  -0.99432737]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9951887  -0.9990691  -0.99432737]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9951887  -0.9990691  -0.99432737]], Number:1635\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99994224 -0.99346536 -0.99639946]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99994224 -0.99346536 -0.99639946]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99994224 -0.99346536 -0.99639946]], Number:1636\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9986133  -0.99784285 -0.99505323]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9986133  -0.99784285 -0.99505323]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9986133  -0.99784285 -0.99505323]], Number:1637\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941574  -0.99570954 -0.9976437 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941574  -0.99570954 -0.9976437 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9941574  -0.99570954 -0.9976437 ]], Number:1638\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992357   -0.9967395  -0.99278677]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992357   -0.9967395  -0.99278677]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.992357   -0.9967395  -0.99278677]], Number:1639\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989032  -0.99906826 -0.9931861 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9989032  -0.99906826 -0.9931861 ]], Number:1640\n",
      "-------------------------\n",
      "Episode 82\tFrame 1640 \tAverage100 Score: -197.03tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999603 -0.9945663 -0.9984196]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999603 -0.9945663 -0.9984196]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9999603 -0.9945663 -0.9984196]], Number:1641\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949254  -0.9972633  -0.99378926]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949254  -0.9972633  -0.99378926]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9949254  -0.9972633  -0.99378926]], Number:1642\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935934 -0.9949743 -0.9958837]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9935934 -0.9949743 -0.9958837]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9935934 -0.9949743 -0.9958837]], Number:1643\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99369955 -0.9960554  -0.9977821 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99369955 -0.9960554  -0.9977821 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99369955 -0.9960554  -0.9977821 ]], Number:1644\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99586946 -0.9991425  -0.9965746 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99586946 -0.9991425  -0.9965746 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99586946 -0.9991425  -0.9965746 ]], Number:1645\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99440366 -0.9926575  -0.9968561 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99440366 -0.9926575  -0.9968561 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99440366 -0.9926575  -0.9968561 ]], Number:1646\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9931036  -0.9960131  -0.99720776]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9931036  -0.9960131  -0.99720776]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9931036  -0.9960131  -0.99720776]], Number:1647\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99788517 -0.99536306 -0.9976196 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99788517 -0.99536306 -0.9976196 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99788517 -0.99536306 -0.9976196 ]], Number:1648\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9955618 -0.9936375 -0.9923091]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9955618 -0.9936375 -0.9923091]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9955618 -0.9936375 -0.9923091]], Number:1649\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99300903 -0.998647   -0.9927577 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99300903 -0.998647   -0.9927577 ]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99300903 -0.998647   -0.9927577 ]], Number:1650\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99667835 -0.99821746 -0.99642557]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99667835 -0.99821746 -0.99642557]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99667835 -0.99821746 -0.99642557]], Number:1651\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99925613 -0.9933486  -0.9941751 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99925613 -0.9933486  -0.9941751 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99925613 -0.9933486  -0.9941751 ]], Number:1652\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928581  -0.99628514 -0.9949082 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928581  -0.99628514 -0.9949082 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9928581  -0.99628514 -0.9949082 ]], Number:1653\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9964711  -0.9932826  -0.99642086]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9964711  -0.9932826  -0.99642086]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9964711  -0.9932826  -0.99642086]], Number:1654\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9940464 -0.9934117 -0.9926471]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9940464 -0.9934117 -0.9926471]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9940464 -0.9934117 -0.9926471]], Number:1655\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9933038  -0.997921   -0.99918044]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9933038  -0.997921   -0.99918044]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9933038  -0.997921   -0.99918044]], Number:1656\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99732274 -0.993339   -0.9967108 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99732274 -0.993339   -0.9967108 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99732274 -0.993339   -0.9967108 ]], Number:1657\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954016  -0.9988326  -0.99446255]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9954016  -0.9988326  -0.99446255]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9954016  -0.9988326  -0.99446255]], Number:1658\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99818355 -0.9930029  -0.9942436 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99818355 -0.9930029  -0.9942436 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99818355 -0.9930029  -0.9942436 ]], Number:1659\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.998258   -0.99968797 -0.992885  ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.998258   -0.99968797 -0.992885  ]], Number:1660\n",
      "-------------------------\n",
      "Episode 83\tFrame 1660 \tAverage100 Score: -197.06tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925093  -0.99682003 -0.995152  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925093  -0.99682003 -0.995152  ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9925093  -0.99682003 -0.995152  ]], Number:1661\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991998  -0.99593467 -0.9991754 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991998  -0.99593467 -0.9991754 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991998  -0.99593467 -0.9991754 ]], Number:1662\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9931458  -0.99442554 -0.9975935 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9931458  -0.99442554 -0.9975935 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9931458  -0.99442554 -0.9975935 ]], Number:1663\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9990904 -0.9987349 -0.998015 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9990904 -0.9987349 -0.998015 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9990904 -0.9987349 -0.998015 ]], Number:1664\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9967538  -0.9939652  -0.99404216]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9967538  -0.9939652  -0.99404216]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9967538  -0.9939652  -0.99404216]], Number:1665\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99692273 -0.9949502  -0.9935899 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99692273 -0.9949502  -0.9935899 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99692273 -0.9949502  -0.9935899 ]], Number:1666\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9938984  -0.99212486 -0.99396944]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9938984  -0.99212486 -0.99396944]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9938984  -0.99212486 -0.99396944]], Number:1667\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9931594 -0.9922948 -0.9937708]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9931594 -0.9922948 -0.9937708]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9931594 -0.9922948 -0.9937708]], Number:1668\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99213845 -0.9990541  -0.9992148 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99213845 -0.9990541  -0.9992148 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99213845 -0.9990541  -0.9992148 ]], Number:1669\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9995035  -0.9923763  -0.99726516]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9995035  -0.9923763  -0.99726516]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9995035  -0.9923763  -0.99726516]], Number:1670\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99700034 -0.99541914 -0.99844784]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99700034 -0.99541914 -0.99844784]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99700034 -0.99541914 -0.99844784]], Number:1671\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999875  -0.9957167 -0.9962809]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999875  -0.9957167 -0.9962809]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.999875  -0.9957167 -0.9962809]], Number:1672\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99507487 -0.99498534 -0.99890536]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99507487 -0.99498534 -0.99890536]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99507487 -0.99498534 -0.99890536]], Number:1673\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9992175 -0.9931486 -0.9939398]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9992175 -0.9931486 -0.9939398]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9992175 -0.9931486 -0.9939398]], Number:1674\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99642646 -0.9939071  -0.99581355]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99642646 -0.9939071  -0.99581355]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99642646 -0.9939071  -0.99581355]], Number:1675\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997817  -0.99523985 -0.9920952 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997817  -0.99523985 -0.9920952 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9997817  -0.99523985 -0.9920952 ]], Number:1676\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9941449  -0.99729884 -0.99963754]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941449  -0.99729884 -0.99963754]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941449  -0.99729884 -0.99963754]], Number:1677\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99209774 -0.99774146 -0.99869084]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99209774 -0.99774146 -0.99869084]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99209774 -0.99774146 -0.99869084]], Number:1678\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953192  -0.9940144  -0.99375236]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953192  -0.9940144  -0.99375236]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9953192  -0.9940144  -0.99375236]], Number:1679\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9941973  -0.99273795 -0.9934658 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9941973  -0.99273795 -0.9934658 ]], Number:1680\n",
      "-------------------------\n",
      "Episode 84\tFrame 1680 \tAverage100 Score: -197.10tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99378586 -0.9935806  -0.9949815 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99378586 -0.9935806  -0.9949815 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99378586 -0.9935806  -0.9949815 ]], Number:1681\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930248  -0.99957997 -0.9994601 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9930248  -0.99957997 -0.9994601 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9930248  -0.99957997 -0.9994601 ]], Number:1682\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99903506 -0.9999503  -0.9962199 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99903506 -0.9999503  -0.9962199 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99903506 -0.9999503  -0.9962199 ]], Number:1683\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966987  -0.99932957 -0.9947273 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966987  -0.99932957 -0.9947273 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9966987  -0.99932957 -0.9947273 ]], Number:1684\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9953092 -0.994464  -0.9941544]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9953092 -0.994464  -0.9941544]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9953092 -0.994464  -0.9941544]], Number:1685\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9954291  -0.9921021  -0.99256593]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954291  -0.9921021  -0.99256593]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954291  -0.9921021  -0.99256593]], Number:1686\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99533725 -0.9931288  -0.99681413]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99533725 -0.9931288  -0.99681413]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99533725 -0.9931288  -0.99681413]], Number:1687\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999025   -0.99919194 -0.9980807 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.999025   -0.99919194 -0.9980807 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999025   -0.99919194 -0.9980807 ]], Number:1688\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99629366 -0.99731755 -0.9926167 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99629366 -0.99731755 -0.9926167 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99629366 -0.99731755 -0.9926167 ]], Number:1689\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992206  -0.9983021 -0.9976513]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992206  -0.9983021 -0.9976513]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.992206  -0.9983021 -0.9976513]], Number:1690\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962632  -0.9927368  -0.99265766]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962632  -0.9927368  -0.99265766]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9962632  -0.9927368  -0.99265766]], Number:1691\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9989199 -0.9961766 -0.9957809]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9989199 -0.9961766 -0.9957809]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9989199 -0.9961766 -0.9957809]], Number:1692\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.993764   -0.9971959  -0.99879444]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.993764   -0.9971959  -0.99879444]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.993764   -0.9971959  -0.99879444]], Number:1693\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995699   -0.99889743 -0.9970256 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995699   -0.99889743 -0.9970256 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.995699   -0.99889743 -0.9970256 ]], Number:1694\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921033  -0.99740213 -0.99563193]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921033  -0.99740213 -0.99563193]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921033  -0.99740213 -0.99563193]], Number:1695\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99272925 -0.99404985 -0.99851567]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99272925 -0.99404985 -0.99851567]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99272925 -0.99404985 -0.99851567]], Number:1696\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99908    -0.99685603 -0.99980605]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99908    -0.99685603 -0.99980605]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99908    -0.99685603 -0.99980605]], Number:1697\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9950668  -0.9966015  -0.99624145]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9950668  -0.9966015  -0.99624145]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9950668  -0.9966015  -0.99624145]], Number:1698\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99990314 -0.9980707  -0.99653083]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99990314 -0.9980707  -0.99653083]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99990314 -0.9980707  -0.99653083]], Number:1699\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9979751  -0.9992598  -0.99336237]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9979751  -0.9992598  -0.99336237]], Number:1700\n",
      "-------------------------\n",
      "Episode 85\tFrame 1700 \tAverage100 Score: -197.13tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99521554 -0.99443394 -0.9933193 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99521554 -0.99443394 -0.9933193 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99521554 -0.99443394 -0.9933193 ]], Number:1701\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9928781  -0.99968034 -0.99466795]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9928781  -0.99968034 -0.99466795]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9928781  -0.99968034 -0.99466795]], Number:1702\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99261755 -0.99871373 -0.99848634]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99261755 -0.99871373 -0.99848634]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99261755 -0.99871373 -0.99848634]], Number:1703\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9939616  -0.997379   -0.99588925]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9939616  -0.997379   -0.99588925]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9939616  -0.997379   -0.99588925]], Number:1704\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99741495 -0.9971197  -0.99705404]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99741495 -0.9971197  -0.99705404]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99741495 -0.9971197  -0.99705404]], Number:1705\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9967557 -0.9943324 -0.9978041]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9967557 -0.9943324 -0.9978041]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9967557 -0.9943324 -0.9978041]], Number:1706\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99955297 -0.9968511  -0.99221104]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99955297 -0.9968511  -0.99221104]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99955297 -0.9968511  -0.99221104]], Number:1707\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9949699  -0.99769247 -0.9979176 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9949699  -0.99769247 -0.9979176 ]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9949699  -0.99769247 -0.9979176 ]], Number:1708\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9957063  -0.9956291  -0.99314815]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9957063  -0.9956291  -0.99314815]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9957063  -0.9956291  -0.99314815]], Number:1709\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.996599  -0.9951958 -0.9950318]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.996599  -0.9951958 -0.9950318]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.996599  -0.9951958 -0.9950318]], Number:1710\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999106  -0.9960291 -0.9968615]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999106  -0.9960291 -0.9968615]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.999106  -0.9960291 -0.9968615]], Number:1711\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99849695 -0.9971454  -0.99572974]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99849695 -0.9971454  -0.99572974]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99849695 -0.9971454  -0.99572974]], Number:1712\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99540025 -0.9927108  -0.999243  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99540025 -0.9927108  -0.999243  ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99540025 -0.9927108  -0.999243  ]], Number:1713\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992473   -0.99618244 -0.9986442 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992473   -0.99618244 -0.9986442 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.992473   -0.99618244 -0.9986442 ]], Number:1714\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99879235 -0.99653846 -0.99785393]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99879235 -0.99653846 -0.99785393]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99879235 -0.99653846 -0.99785393]], Number:1715\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9958482  -0.99417573 -0.99452543]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9958482  -0.99417573 -0.99452543]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9958482  -0.99417573 -0.99452543]], Number:1716\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99915415 -0.99744904 -0.9953473 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99915415 -0.99744904 -0.9953473 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99915415 -0.99744904 -0.9953473 ]], Number:1717\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9930592  -0.9983187  -0.99226356]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9930592  -0.9983187  -0.99226356]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9930592  -0.9983187  -0.99226356]], Number:1718\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991959  -0.9938798  -0.99821657]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991959  -0.9938798  -0.99821657]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9991959  -0.9938798  -0.99821657]], Number:1719\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99252707 -0.9934017  -0.99376154]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99252707 -0.9934017  -0.99376154]], Number:1720\n",
      "-------------------------\n",
      "Episode 86\tFrame 1720 \tAverage100 Score: -197.17tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9976455 -0.998703  -0.9947956]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9976455 -0.998703  -0.9947956]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9976455 -0.998703  -0.9947956]], Number:1721\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9926935  -0.9931873  -0.99701816]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9926935  -0.9931873  -0.99701816]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9926935  -0.9931873  -0.99701816]], Number:1722\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99644697 -0.9920043  -0.9928655 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99644697 -0.9920043  -0.9928655 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99644697 -0.9920043  -0.9928655 ]], Number:1723\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9991614 -0.9925254 -0.9975082]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9991614 -0.9925254 -0.9975082]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9991614 -0.9925254 -0.9975082]], Number:1724\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922848  -0.99910945 -0.9964543 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922848  -0.99910945 -0.9964543 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9922848  -0.99910945 -0.9964543 ]], Number:1725\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9931592  -0.99680716 -0.99374145]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9931592  -0.99680716 -0.99374145]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9931592  -0.99680716 -0.99374145]], Number:1726\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9962085 -0.9935916 -0.996744 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9962085 -0.9935916 -0.996744 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9962085 -0.9935916 -0.996744 ]], Number:1727\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9935865  -0.9997635  -0.99344903]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9935865  -0.9997635  -0.99344903]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9935865  -0.9997635  -0.99344903]], Number:1728\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923453 -0.9932951 -0.9967212]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923453 -0.9932951 -0.9967212]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923453 -0.9932951 -0.9967212]], Number:1729\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99606943 -0.99295676 -0.99749494]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99606943 -0.99295676 -0.99749494]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99606943 -0.99295676 -0.99749494]], Number:1730\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9980236  -0.99928254 -0.99697626]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9980236  -0.99928254 -0.99697626]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9980236  -0.99928254 -0.99697626]], Number:1731\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9925182  -0.9953066  -0.99625707]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9925182  -0.9953066  -0.99625707]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9925182  -0.9953066  -0.99625707]], Number:1732\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998495  -0.99790835 -0.9990514 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9998495  -0.99790835 -0.9990514 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9998495  -0.99790835 -0.9990514 ]], Number:1733\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969527  -0.9990023  -0.99562424]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969527  -0.9990023  -0.99562424]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9969527  -0.9990023  -0.99562424]], Number:1734\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973901 -0.9944562 -0.9934443]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973901 -0.9944562 -0.9934443]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9973901 -0.9944562 -0.9934443]], Number:1735\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99672544 -0.9929632  -0.9956307 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99672544 -0.9929632  -0.9956307 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99672544 -0.9929632  -0.9956307 ]], Number:1736\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994508  -0.9967537 -0.9969687]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994508  -0.9967537 -0.9969687]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.994508  -0.9967537 -0.9969687]], Number:1737\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99393153 -0.99813455 -0.9998351 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99393153 -0.99813455 -0.9998351 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99393153 -0.99813455 -0.9998351 ]], Number:1738\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934867 -0.9980057 -0.9944721]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934867 -0.9980057 -0.9944721]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9934867 -0.9980057 -0.9944721]], Number:1739\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999997  -0.9920719 -0.9936789]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999997  -0.9920719 -0.9936789]], Number:1740\n",
      "-------------------------\n",
      "Episode 87\tFrame 1740 \tAverage100 Score: -197.20tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9937522  -0.99756634 -0.9996961 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9937522  -0.99756634 -0.9996961 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9937522  -0.99756634 -0.9996961 ]], Number:1741\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99876297 -0.99566376 -0.996118  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99876297 -0.99566376 -0.996118  ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99876297 -0.99566376 -0.996118  ]], Number:1742\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99686784 -0.99227774 -0.994947  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99686784 -0.99227774 -0.994947  ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99686784 -0.99227774 -0.994947  ]], Number:1743\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985267 -0.9929148 -0.9923021]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985267 -0.9929148 -0.9923021]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985267 -0.9929148 -0.9923021]], Number:1744\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970054  -0.99482834 -0.99758106]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970054  -0.99482834 -0.99758106]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9970054  -0.99482834 -0.99758106]], Number:1745\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993937  -0.99462956 -0.9932792 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993937  -0.99462956 -0.9932792 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993937  -0.99462956 -0.9932792 ]], Number:1746\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9970412  -0.9969529  -0.99448615]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9970412  -0.9969529  -0.99448615]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9970412  -0.9969529  -0.99448615]], Number:1747\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9929649 -0.9961763 -0.9939821]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9929649 -0.9961763 -0.9939821]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9929649 -0.9961763 -0.9939821]], Number:1748\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99885696 -0.99938995 -0.9950532 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99885696 -0.99938995 -0.9950532 ]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99885696 -0.99938995 -0.9950532 ]], Number:1749\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9978981 -0.9981347 -0.9994138]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9978981 -0.9981347 -0.9994138]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9978981 -0.9981347 -0.9994138]], Number:1750\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99602777 -0.99560934 -0.99614096]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99602777 -0.99560934 -0.99614096]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99602777 -0.99560934 -0.99614096]], Number:1751\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99977255 -0.9957237  -0.9971855 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99977255 -0.9957237  -0.9971855 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99977255 -0.9957237  -0.9971855 ]], Number:1752\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99544454 -0.99817353 -0.9970806 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99544454 -0.99817353 -0.9970806 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99544454 -0.99817353 -0.9970806 ]], Number:1753\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99923563 -0.99802583 -0.9935473 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99923563 -0.99802583 -0.9935473 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.99923563 -0.99802583 -0.9935473 ]], Number:1754\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997393 -0.9973899 -0.998576 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997393 -0.9973899 -0.998576 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997393 -0.9973899 -0.998576 ]], Number:1755\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99339044 -0.99218667 -0.99804926]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99339044 -0.99218667 -0.99804926]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99339044 -0.99218667 -0.99804926]], Number:1756\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99408525 -0.99599546 -0.9969162 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99408525 -0.99599546 -0.9969162 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99408525 -0.99599546 -0.9969162 ]], Number:1757\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99941176 -0.9921444  -0.9948087 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99941176 -0.9921444  -0.9948087 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99941176 -0.9921444  -0.9948087 ]], Number:1758\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924963  -0.99774367 -0.9986579 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924963  -0.99774367 -0.9986579 ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9924963  -0.99774367 -0.9986579 ]], Number:1759\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99295044 -0.99289495 -0.9986747 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99295044 -0.99289495 -0.9986747 ]], Number:1760\n",
      "-------------------------\n",
      "Episode 88\tFrame 1760 \tAverage100 Score: -197.23tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9948852 -0.9931766 -0.9991068]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9948852 -0.9931766 -0.9991068]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9948852 -0.9931766 -0.9991068]], Number:1761\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9988736  -0.9947722  -0.99887896]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9988736  -0.9947722  -0.99887896]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9988736  -0.9947722  -0.99887896]], Number:1762\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99749833 -0.9954057  -0.9942549 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99749833 -0.9954057  -0.9942549 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99749833 -0.9954057  -0.9942549 ]], Number:1763\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99740285 -0.9997537  -0.999113  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99740285 -0.9997537  -0.999113  ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99740285 -0.9997537  -0.999113  ]], Number:1764\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99809444 -0.992164   -0.9978375 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99809444 -0.992164   -0.9978375 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99809444 -0.992164   -0.9978375 ]], Number:1765\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9999414  -0.9966186  -0.99496496]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9999414  -0.9966186  -0.99496496]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9999414  -0.9966186  -0.99496496]], Number:1766\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9921375  -0.99489766 -0.9952122 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9921375  -0.99489766 -0.9952122 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9921375  -0.99489766 -0.9952122 ]], Number:1767\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9936223  -0.99833775 -0.99900264]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9936223  -0.99833775 -0.99900264]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9936223  -0.99833775 -0.99900264]], Number:1768\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9968182 -0.9963324 -0.9957816]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9968182 -0.9963324 -0.9957816]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9968182 -0.9963324 -0.9957816]], Number:1769\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996214  -0.9928684  -0.99596083]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996214  -0.9928684  -0.99596083]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996214  -0.9928684  -0.99596083]], Number:1770\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9984825  -0.99437857 -0.9923878 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9984825  -0.99437857 -0.9923878 ]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.9984825  -0.99437857 -0.9923878 ]], Number:1771\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947464  -0.9938211  -0.99878585]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9947464  -0.9938211  -0.99878585]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9947464  -0.9938211  -0.99878585]], Number:1772\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977561  -0.99326396 -0.99951535]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977561  -0.99326396 -0.99951535]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9977561  -0.99326396 -0.99951535]], Number:1773\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923388  -0.9965219  -0.99467117]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923388  -0.9965219  -0.99467117]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9923388  -0.9965219  -0.99467117]], Number:1774\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99225175 -0.9925557  -0.9922124 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99225175 -0.9925557  -0.9922124 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99225175 -0.9925557  -0.9922124 ]], Number:1775\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932278 -0.9948028 -0.99823  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932278 -0.9948028 -0.99823  ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9932278 -0.9948028 -0.99823  ]], Number:1776\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99356556 -0.9992796  -0.99416405]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99356556 -0.9992796  -0.99416405]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99356556 -0.9992796  -0.99416405]], Number:1777\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99316466 -0.9936826  -0.9994218 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99316466 -0.9936826  -0.9994218 ]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.99316466 -0.9936826  -0.9994218 ]], Number:1778\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99778104 -0.99410397 -0.996156  ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99778104 -0.99410397 -0.996156  ]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99778104 -0.99410397 -0.996156  ]], Number:1779\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9923792  -0.994251   -0.99427575]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923792  -0.994251   -0.99427575]], Number:1780\n",
      "-------------------------\n",
      "Episode 89\tFrame 1780 \tAverage100 Score: -197.26tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.995081  -0.9988615 -0.9979626]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.995081  -0.9988615 -0.9979626]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.995081  -0.9988615 -0.9979626]], Number:1781\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9927567 -0.9952766 -0.9934104]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9927567 -0.9952766 -0.9934104]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9927567 -0.9952766 -0.9934104]], Number:1782\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9922146 -0.9922604 -0.9936659]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9922146 -0.9922604 -0.9936659]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9922146 -0.9922604 -0.9936659]], Number:1783\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99554753 -0.9969704  -0.9983984 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99554753 -0.9969704  -0.9983984 ]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99554753 -0.9969704  -0.9983984 ]], Number:1784\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99773103 -0.9927232  -0.9968314 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99773103 -0.9927232  -0.9968314 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99773103 -0.9927232  -0.9968314 ]], Number:1785\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99979794 -0.9993686  -0.99865305]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99979794 -0.9993686  -0.99865305]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99979794 -0.9993686  -0.99865305]], Number:1786\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9932857 -0.9997288 -0.9952018]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9932857 -0.9997288 -0.9952018]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9932857 -0.9997288 -0.9952018]], Number:1787\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997934 -0.9946197 -0.9958743]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997934 -0.9946197 -0.9958743]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997934 -0.9946197 -0.9958743]], Number:1788\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9997093  -0.99691457 -0.99294287]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9997093  -0.99691457 -0.99294287]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9997093  -0.99691457 -0.99294287]], Number:1789\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9942955  -0.9979677  -0.99964267]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9942955  -0.9979677  -0.99964267]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9942955  -0.9979677  -0.99964267]], Number:1790\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99682415 -0.9930645  -0.99508375]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99682415 -0.9930645  -0.99508375]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99682415 -0.9930645  -0.99508375]], Number:1791\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99383104 -0.9969323  -0.99902093]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99383104 -0.9969323  -0.99902093]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99383104 -0.9969323  -0.99902093]], Number:1792\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9947618  -0.99987113 -0.9959994 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9947618  -0.99987113 -0.9959994 ]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9947618  -0.99987113 -0.9959994 ]], Number:1793\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9966091  -0.99626493 -0.9923007 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9966091  -0.99626493 -0.9923007 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.9966091  -0.99626493 -0.9923007 ]], Number:1794\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9993888 -0.9935708 -0.9977867]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9993888 -0.9935708 -0.9977867]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9993888 -0.9935708 -0.9977867]], Number:1795\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99579835 -0.995353   -0.99845725]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99579835 -0.995353   -0.99845725]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.99579835 -0.995353   -0.99845725]], Number:1796\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9934773  -0.99343735 -0.9929689 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9934773  -0.99343735 -0.9929689 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9934773  -0.99343735 -0.9929689 ]], Number:1797\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9969973  -0.99363405 -0.99217874]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9969973  -0.99363405 -0.99217874]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9969973  -0.99363405 -0.99217874]], Number:1798\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99862254 -0.99270225 -0.99291486]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99862254 -0.99270225 -0.99291486]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99862254 -0.99270225 -0.99291486]], Number:1799\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99899966 -0.99509674 -0.9945431 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99899966 -0.99509674 -0.9945431 ]], Number:1800\n",
      "-------------------------\n",
      "Episode 90\tFrame 1800 \tAverage100 Score: -197.29tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99333435 -0.9962082  -0.9967103 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99333435 -0.9962082  -0.9967103 ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99333435 -0.9962082  -0.9967103 ]], Number:1801\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9981292  -0.99445045 -0.99379885]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9981292  -0.99445045 -0.99379885]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9981292  -0.99445045 -0.99379885]], Number:1802\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99568534 -0.99867207 -0.99862075]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99568534 -0.99867207 -0.99862075]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99568534 -0.99867207 -0.99862075]], Number:1803\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9985528 -0.9956955 -0.9980214]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9985528 -0.9956955 -0.9980214]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9985528 -0.9956955 -0.9980214]], Number:1804\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99956447 -0.9933442  -0.9922766 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99956447 -0.9933442  -0.9922766 ]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99956447 -0.9933442  -0.9922766 ]], Number:1805\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99282223 -0.9976694  -0.9941543 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99282223 -0.9976694  -0.9941543 ]]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "score: [-60.]\n",
      "state: [[1000.         30.015715   41.47       60.87        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99282223 -0.9976694  -0.9941543 ]], Number:1806\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0157,   41.4700,   60.8700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99571145 -0.9945163  -0.9972306 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99571145 -0.9945163  -0.9972306 ]]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "score: [-70.]\n",
      "state: [[1000.         29.674286   42.02       60.43        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99571145 -0.9945163  -0.9972306 ]], Number:1807\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6743,   42.0200,   60.4300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9923577  -0.99470985 -0.99220574]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.9923577  -0.99470985 -0.99220574]]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "score: [-80.]\n",
      "state: [[1000.         30.092857   42.15       61.16        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9923577  -0.99470985 -0.99220574]], Number:1808\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0929,   42.1500,   61.1600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996892  -0.9929511  -0.99254113]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996892  -0.9929511  -0.99254113]]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "score: [-90.]\n",
      "state: [[1000.         29.918571   42.68       61.56        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996892  -0.9929511  -0.99254113]], Number:1809\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.9186,   42.6800,   61.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9998696  -0.99913394 -0.99416023]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9998696  -0.99913394 -0.99416023]]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "score: [-100.]\n",
      "state: [[1000.         29.418571   42.39       60.82        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9998696  -0.99913394 -0.99416023]], Number:1810\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4186,   42.3900,   60.8200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99412686 -0.9929085  -0.99434143]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99412686 -0.9929085  -0.99434143]]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "score: [-110.]\n",
      "state: [[1000.     30.72   42.96   60.65    0.      0.      0.  ]]\n",
      "action:[[-0.99412686 -0.9929085  -0.99434143]], Number:1811\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.7200,   42.9600,   60.6500,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9973242  -0.99674755 -0.9960333 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9973242  -0.99674755 -0.9960333 ]]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "score: [-120.]\n",
      "state: [[1000.        30.24643   42.98      60.2        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.9973242  -0.99674755 -0.9960333 ]], Number:1812\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2464,   42.9800,   60.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9924684 -0.9958556 -0.9920167]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9924684 -0.9958556 -0.9920167]]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-130.]\n",
      "state: [[1000.         29.724571   42.16       59.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9924684 -0.9958556 -0.9920167]], Number:1813\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.7246,   42.1600,   59.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.992721   -0.99658924 -0.9992184 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.992721   -0.99658924 -0.9992184 ]]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "score: [-140.]\n",
      "state: [[1000.     28.25   38.59   57.77    0.      0.      0.  ]]\n",
      "action:[[-0.992721   -0.99658924 -0.9992184 ]], Number:1814\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.2500,   38.5900,   57.7700,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9996053  -0.99338645 -0.9996698 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9996053  -0.99338645 -0.9996698 ]]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "score: [-150.]\n",
      "state: [[1000.         29.010714   37.79       57.78        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9996053  -0.99338645 -0.9996698 ]], Number:1815\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.0107,   37.7900,   57.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9994909  -0.99688643 -0.9945103 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9994909  -0.99688643 -0.9945103 ]]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "score: [-160.]\n",
      "state: [[1000.     29.42   38.1    57.71    0.      0.      0.  ]]\n",
      "action:[[-0.9994909  -0.99688643 -0.9945103 ]], Number:1816\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.4200,   38.1000,   57.7100,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99904543 -0.99776924 -0.9924048 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99904543 -0.99776924 -0.9924048 ]]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "score: [-170.]\n",
      "state: [[1000.         29.697714   38.67       61.93        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99904543 -0.99776924 -0.9924048 ]], Number:1817\n",
      "-------------------------\n",
      "tensor([[1000.0000,   29.6977,   38.6700,   61.9300,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9944387 -0.9978947 -0.9953062]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9944387 -0.9978947 -0.9953062]]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "score: [-180.]\n",
      "state: [[1000.     28.47   37.43   62.56    0.      0.      0.  ]]\n",
      "action:[[-0.9944387 -0.9978947 -0.9953062]], Number:1818\n",
      "-------------------------\n",
      "tensor([[1000.0000,   28.4700,   37.4300,   62.5600,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99259734 -0.9949968  -0.99316365]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99259734 -0.9949968  -0.99316365]]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "score: [-190.]\n",
      "state: [[1000.        27.43757   37.66      60.6        0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99259734 -0.9949968  -0.99316365]], Number:1819\n",
      "-------------------------\n",
      "tensor([[1000.0000,   27.4376,   37.6600,   60.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "Finished\n",
      "[1000, 27.43757143, 37.66, 60.6, 0, 0, 0]\n",
      "end_total_asset:1000.0\n",
      "Sharpe:  nan\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9954439  -0.99882156 -0.992634  ]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-200.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9954439  -0.99882156 -0.992634  ]], Number:1820\n",
      "-------------------------\n",
      "Episode 91\tFrame 1820 \tAverage100 Score: -197.32tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.999906   -0.9985333  -0.99699754]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.999906   -0.9985333  -0.99699754]]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "score: [-10.]\n",
      "state: [[1000.         30.572857   40.92       56.18        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.999906   -0.9985333  -0.99699754]], Number:1821\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.5729,   40.9200,   56.1800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99334925 -0.9987564  -0.9980462 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.99334925 -0.9987564  -0.9980462 ]]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "score: [-20.]\n",
      "state: [[1000.         30.625713   40.83       58.02        0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.99334925 -0.9987564  -0.9980462 ]], Number:1822\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.6257,   40.8300,   58.0200,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99363554 -0.99989325 -0.9988007 ]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent seed None\n",
      "My Action: [[-0.99363554 -0.99989325 -0.9988007 ]]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "score: [-30.]\n",
      "state: [[1000.        30.13857   41.49      59.78       0.         0.\n",
      "     0.     ]]\n",
      "action:[[-0.99363554 -0.99989325 -0.9988007 ]], Number:1823\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.1386,   41.4900,   59.7800,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.994275  -0.9924155 -0.9953004]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.994275  -0.9924155 -0.9953004]]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "score: [-40.]\n",
      "state: [[1000.         30.082857   41.98       62.2         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.994275  -0.9924155 -0.9953004]], Number:1824\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.0829,   41.9800,   62.2000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.9977488  -0.9962984  -0.99613017]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n",
      "agent seed None\n",
      "My Action: [[-0.9977488  -0.9962984  -0.99613017]]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "score: [-50.]\n",
      "state: [[1000.         30.282858   41.95       61.6         0.          0.\n",
      "     0.      ]]\n",
      "action:[[-0.9977488  -0.9962984  -0.99613017]], Number:1825\n",
      "-------------------------\n",
      "tensor([[1000.0000,   30.2829,   41.9500,   61.6000,    0.0000,    0.0000,\n",
      "            0.0000]])\n",
      "here: [-0.99673784 -0.9938229  -0.99943143]\n",
      "trades:0\n",
      "previous:0.0\n",
      "penalty:10\n",
      "step_reward:-10.0\n",
      "step_reward:-10.0\n",
      "agent step reward:-10.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed = 4\n",
    "    frames = 30000\n",
    "    worker = 1\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-3\n",
    "    HIDDEN_SIZE = 128\n",
    "    BUFFER_SIZE = int(1e6)\n",
    "    BATCH_SIZE = 32\n",
    "    LR_ACTOR = 3e-3         # learning rate of the actor \n",
    "    \n",
    "    LR_CRITIC = 3e-3     # learning rate of the critic\n",
    "    saved_model = None #'D4PG.pth'\n",
    "    D2RL = 0\n",
    "\n",
    "    writer = SummaryWriter(\"\")\n",
    "    \n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20100101, end=20100202)\n",
    "    \n",
    "    eval_env = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    eval_env.seed(seed+1)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: {}\".format(device))\n",
    "    \n",
    "    action_high = eval_env.action_space.high[0]\n",
    "    print(action_high)\n",
    "    action_low = eval_env.action_space.low[0]\n",
    "    print(action_low)\n",
    "    state_size = eval_env.observation_space.shape[0]\n",
    "    action_size = eval_env.action_space.shape[0]\n",
    "    print('run seed', seed)\n",
    "    agent = Agent(state_size=state_size, action_size=action_size, n_step=1, per=0, munchausen=0,distributional=1,\n",
    "                 D2RL=D2RL, random_seed=seed, hidden_size=HIDDEN_SIZE, BATCH_SIZE=BATCH_SIZE, BUFFER_SIZE=BUFFER_SIZE, GAMMA=GAMMA,\n",
    "                 LR_ACTOR=LR_ACTOR, LR_CRITIC=LR_CRITIC, TAU=TAU, LEARN_EVERY=1, LEARN_NUMBER=1, device=device, frames=frames, worker=1) \n",
    "    \n",
    "    t0 = time.time()\n",
    "    if saved_model != None:\n",
    "        agent.actor_local.load_state_dict(torch.load(saved_model))\n",
    "        #evaluate(1)\n",
    "        run(eval_env,frames=frames, eps_fixed=False, eps_frames=1000, min_eps=0.025)\n",
    "    else:    \n",
    "        run(eval_env,frames=frames, eps_fixed=False, eps_frames=1000, min_eps=0.025)\n",
    "\n",
    "    eval_env.close()\n",
    "    # save trained model \n",
    "    torch.save(agent.actor_local.state_dict(), 'D4PG_2.pth')\n",
    "    #writer.export_scalars_to_json('./scalars.json')\n",
    "    # save parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation part \n",
    "1) Reading JSON and PTH\n",
    "2) Setting up trading env\n",
    "3) Running test\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(eval_runs=5):\n",
    "    \"\"\"\n",
    "    Makes an evaluation run \n",
    "    \n",
    "    \"\"\"\n",
    "    seed = 4\n",
    "    frames = 10000\n",
    "    worker = 1\n",
    "    GAMMA = 0.99\n",
    "    TAU = 1e-3\n",
    "    HIDDEN_SIZE = 256\n",
    "    BUFFER_SIZE = int(1e6)\n",
    "    BATCH_SIZE = 256\n",
    "    LR_ACTOR = 3e-4         # learning rate of the actor \n",
    "    LR_CRITIC = 3e-4     # learning rate of the critic\n",
    "    saved_model = 'D4PG.pth'\n",
    "    D2RL = 0\n",
    "\n",
    "    writer = SummaryWriter(\"\")\n",
    "    \n",
    "    preprocessed_path = \"done_3stocks.csv\"\n",
    "    if os.path.exists(preprocessed_path):\n",
    "        data = pd.read_csv(preprocessed_path, index_col=0)\n",
    "\n",
    "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
    "    #print(unique_trade_date)\n",
    "\n",
    "    \n",
    "    train = data_split(data, start=20150101, end=20180101)\n",
    "    \n",
    "    env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "    \n",
    "    env_train.seed(seed+1)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: {}\".format(device))\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    output_history = []\n",
    "    frame = 0\n",
    "\n",
    "    eps = 1\n",
    "    eps_start = 1\n",
    "    i_episode = 1\n",
    "    state = env_train.reset()\n",
    "   # state = state[0,:]\n",
    "    #print(\"state space:{}\".format(state[0,:].shape))\n",
    "    score = 0 \n",
    "    \n",
    "    action_high = env_train.action_space.high[0]\n",
    "    action_low = env_train.action_space.low[0]\n",
    "    state_size = env_train.observation_space.shape[0]\n",
    "    action_size = env_train.action_space.shape[0]\n",
    "\n",
    "    for _ in range(eval_runs):\n",
    "        state = env_train.reset()\n",
    "\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            action = agent.act(state) #TODO: getting one dimension back.\n",
    "            print(action)\n",
    "            print(state)\n",
    "            action_v = np.clip(action, action_low, action_high)\n",
    "\n",
    "            next_state, reward, done, info = env_train.step(action_v) #TODO: Wants a list of actions of size a\n",
    "            state = next_state\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                print(\"Episode Rewards: {}\".format(rewards))\n",
    "                break\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal(0, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "a = np.random.seed(4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 9, 6, 1, 1, 2, 8, 7, 3, 5, 6, 3, 5, 3, 5, 8, 8, 2, 8, 1, 7, 8,\n",
       "       7, 2, 1, 2, 9, 9, 4, 9, 8, 4, 7, 6, 2, 4, 5, 9, 2, 5, 1, 4, 3, 1,\n",
       "       5, 3, 8, 8, 9, 7])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
